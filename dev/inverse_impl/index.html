<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Inverse Modeling with ADCME · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../inverse_modeling/">Inverse Modeling</a></li><li><span class="tocitem">Manual</span><ul><li class="is-active"><a class="tocitem" href>Inverse Modeling with ADCME</a><ul class="internal"><li><a class="tocitem" href="#Parameter-Inverse-Problem-1"><span>Parameter Inverse Problem</span></a></li><li><a class="tocitem" href="#Function-Inverse-Problem-1"><span>Function Inverse Problem</span></a></li><li><a class="tocitem" href="#Functional-Inverse-Problem-1"><span>Functional Inverse Problem</span></a></li><li><a class="tocitem" href="#Stochastic-Inverse-Problem-1"><span>Stochastic Inverse Problem</span></a></li></ul></li><li><a class="tocitem" href="../array/">Tensor Operations</a></li><li><a class="tocitem" href="../sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../customop/">Custom Operators</a></li><li><a class="tocitem" href="../while_loop/">While Loops</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../pytorchnn/">Neural Network in C++</a></li><li><a class="tocitem" href="../extra/">Miscellaneous Tools</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Inverse Modeling with ADCME</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Inverse Modeling with ADCME</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/inverse_impl.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Inverse-Modeling-with-ADCME-1"><a class="docs-heading-anchor" href="#Inverse-Modeling-with-ADCME-1">Inverse Modeling with ADCME</a><a class="docs-heading-anchor-permalink" href="#Inverse-Modeling-with-ADCME-1" title="Permalink"></a></h1><p>In this section, we show how to solve the four types of inverse problems identified in <a href="https://kailaix.github.io/ADCME.jl/dev/inverse_modeling/">Inverse Modeling</a>. For simplicity, let the forward model be a 1D Poisson equation</p><div>\[\begin{aligned}-\nabla (X\nabla u(x)) &amp;= \varphi(x) &amp; x\in (0,1)\\ u(0)=u(1) &amp;= 0\end{aligned}\]</div><p>Here <span>$X$</span> is the unknown  which may be one of the four forms: parameter, function, functional or random variable. </p><table><tr><th style="text-align: right"><strong>Inverse problem</strong></th><th style="text-align: right"><strong>Problem type</strong></th><th style="text-align: right"><strong>Approach</strong></th><th style="text-align: center"><strong>Reference</strong></th></tr><tr><td style="text-align: right"><span>$\nabla\cdot(c\nabla u) = 0$</span></td><td style="text-align: right">Parameter</td><td style="text-align: right">Adjoint State Method</td><td style="text-align: center"><a href="http://arxiv.org/abs/1912.07552">1</a> <a href="http://arxiv.org/abs/1912.07547">2</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(f(\mathbf{x})\nabla u) = 0$</span></td><td style="text-align: right">Function</td><td style="text-align: right">DNN</td><td style="text-align: center"><a href="https://arxiv.org/abs/1901.07758">3</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(f(u)\nabla u) = 0$</span></td><td style="text-align: right">Functional</td><td style="text-align: right">DNN Learning from indirect data</td><td style="text-align: center"><a href="https://arxiv.org/abs/1905.12530">4</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(\varpi\nabla u) = 0$</span></td><td style="text-align: right">Stochastic Inversion</td><td style="text-align: right">Adversarial Learning with GAN</td><td style="text-align: center"><a href="https://arxiv.org/abs/1910.06936">5</a></td></tr></table><h2 id="Parameter-Inverse-Problem-1"><a class="docs-heading-anchor" href="#Parameter-Inverse-Problem-1">Parameter Inverse Problem</a><a class="docs-heading-anchor-permalink" href="#Parameter-Inverse-Problem-1" title="Permalink"></a></h2><p>When <span>$X$</span> is just a scalar/vector, we call this type of problem <strong>parameter inverse problem</strong>. We consider a manufactured solution: the exact <span>$X=1$</span> and <span>$u(x)=x(1-x)$</span>, so we have</p><div>\[\varphi(x) = 2\]</div><p>Assume we can observe <span>$u(0.5)=0.25$</span> and the initial guess for <span>$X_0=10$</span>. We use finite difference method to discretize the PDE and the interval <span>$[0,1]$</span> is divided uniformly to <span>$0=x_0&lt;x_1&lt;\ldots&lt;x_n=1$</span>, with <span>$n=100$</span>, <span>$x_{i+1}-x_i = h=\frac{1}{n}$</span>.</p><p>we can solve the problem with the following code snippet</p><pre><code class="language-julia">using ADCME
n = 100
h = 1/n
X0 = Variable(10.0)
A = X0 * diagm(0=&gt;2/h^2*ones(n-1), 1=&gt;-1/h^2*ones(n-2), -1=&gt;-1/h^2*ones(n-2)) # coefficient matrix for the finite difference
φ = 2.0*ones(n-1) # right hand side
u = A\φ
loss = (u[50] - 0.25)^2

sess = Session(); init(sess)
BFGS!(sess, loss)</code></pre><p>After around 7 iterations, the estimated <span>$X_0$</span> converges to 1.0000000016917243. </p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>We can actually solve the linear system <code>A\φ</code> more efficiently by using <a href="../api/#ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}"><code>SparseTensor</code></a>. In this case, simply substitute </p><pre><code class="language-Julia">A = X0 * diagm(0=&gt;2/h^2*ones(n-1), 1=&gt;-1/h^2*ones(n-2), -1=&gt;-1/h^2*ones(n-2))</code></pre><p>with </p><pre><code class="language-julia">A = X0 * SparseTensor(diagm(0=&gt;2/h^2*ones(n-1), 1=&gt;-1/h^2*ones(n-2), -1=&gt;-1/h^2*ones(n-2)))</code></pre></div></div><h2 id="Function-Inverse-Problem-1"><a class="docs-heading-anchor" href="#Function-Inverse-Problem-1">Function Inverse Problem</a><a class="docs-heading-anchor-permalink" href="#Function-Inverse-Problem-1" title="Permalink"></a></h2><p>When <span>$X$</span> is a function that does not depend on <span>$u$</span>, i.e., a function of location <span>$x$</span>, we call this type of problem <strong>function inverse problem</strong>. A common approach to this type of problem is to approximate the unknown function <span>$X$</span> with a parametrized form, such as piecewise linear functions, radial basis functions or Chebyshev polynomials; sometimes we can also discretize <span>$X$</span> and subtitute <span>$X$</span> by a vector of its values at the discrete grid nodes. </p><p>This tutorial is not aimed at comparison of different methods. Instead, we show how we can use neural networks to represent <span>$X$</span> and train the neural network by coupling it with numerical schemes. The gradient calculation can be laborious with the traditional adjoint state methods but is trivial with automatic differentiation. </p><p>Let&#39;s assume the true <span>$X$</span> has the following form</p><div>\[X(x) = \frac{1}{1+x^2}\]</div><p>The exact <span>$\varphi$</span> is given by </p><div>\[\varphi(x) = \frac{2 \left(x^{2} - x \left(2 x - 1\right) + 1\right)}{\left(x^{2} + 1\right)^{2}}\]</div><p>The idea is to use a neural network <span>$\mathcal{N}(x;w)$</span> with weights and biases <span>$w$</span> that maps the location <span>$x\in \mathbb{R}$</span> to a scalar value such that</p><div>\[\mathcal{N}(x; w)\approx X(x)\]</div><p>To find the optional <span>$w$</span>, we solve the Poisson equation with <span>$X(x)=\mathcal{N}(x;w)$</span>, where the numerical scheme is </p><div>\[\left( -\frac{X_i+X_{i+1}}{2} \right) u_{i+1} + \frac{X_{i-1}+2X_i+X_{i+1}}{2} u_i + \left( -\frac{X_i+X_{i-1}}{2} \right) = \varphi(x_i) h^2\]</div><p>Here <span>$X_i = \mathcal{N}(x_i; w)$</span>. </p><p>Assume we can observe the full solution <span>$u(x)$</span>, we can compared it with the solution <span>$u(x;w)$</span>, and minimize the loss function </p><div>\[L(w) = \sum_{i=2}^{n-1} (u(x_i;w)-u(x_i))^2\]</div><pre><code class="language-julia">using ADCME
n = 100
h = 1/n
x = collect(LinRange(0, 1.0, n+1))
X = ae(x, [20,20,20,1])^2  # to ensure that X is positive, we use NN^2 instead of NN
A = spdiag(
  n-1,
  1=&gt;-(X[2:end-2] + X[3:end-1])/2,
  -1=&gt;-(X[3:end-1] + X[2:end-2])/2,
  0=&gt;(2*X[2:end-1]+X[3:end]+X[1:end-2])/2
)/h^2
φ = @. 2*x*(1 - 2*x)/(x^2 + 1)^2 + 2 /(x^2 + 1)
u = Array(A)\φ[2:end-1] # for efficiency, we can use A\φ[2:end-1] (sparse solver)
u_obs = (@. x * (1-x))[2:end-1]
loss = sum((u - u_obs)^2)

sess = Session(); init(sess)
BFGS!(sess, loss)</code></pre><p>We show the exact <span>$X(x)$</span> and the pointwise error in the following plots</p><div>\[\left|\mathcal{N}(x_i;w)-X(x_i)\right|\]</div><table><tr><th style="text-align: right"><img src="../assets/errorX.png" alt="errorX"/></th><th style="text-align: right"><img src="../assets/exactX.png" alt="exactX"/></th></tr><tr><td style="text-align: right">Pointwise Absolute Error</td><td style="text-align: right">Exact <span>$X(u)$</span></td></tr></table><h2 id="Functional-Inverse-Problem-1"><a class="docs-heading-anchor" href="#Functional-Inverse-Problem-1">Functional Inverse Problem</a><a class="docs-heading-anchor-permalink" href="#Functional-Inverse-Problem-1" title="Permalink"></a></h2><p>In the <strong>functional inverse problem</strong>, <span>$X$</span> is a function that <em>depends</em> on <span>$u$</span> (or both <span>$x$</span> and <span>$u$</span>); it must not be confused with the functional inverse problem and it is much harder to solve (since the equation is nonlinear). For example, we may have</p><div>\[X(u) = \frac{1}{1+100u^2}\]</div><p>The corresponding <span>$\varphi$</span> is </p><div>\[\frac{2 \left(100 x^{2} \left(x - 1\right)^{2} - 100 x \left(x - 1\right) \left(2 x - 1\right)^{2} + 1\right)}{\left(100 x^{2} \left(x - 1\right)^{2} + 1\right)^{2}}\]</div><p>To solve the Poisson equation, we use the standard Newton-Raphson scheme (see XXX), in which case, we need to compute the residual</p><div>\[R_i = X&#39;(u_i)\frac{u_{i+1}-u_{i-1}}{2h} + X(u_i)\frac{u_{i+1}+u_{i-1}-2u_i}{h^2} + \varphi(x_i)\]</div><p>and the corresponding Jacobian</p><div>\[\frac{\partial R_i}{\partial u_j} = \left\{ \begin{matrix}  \frac{X&#39;(u_i)}{2h} + \frac{X(u_i)}{h^2} &amp; j=i-1\\ X&#39;&#39;(u_i)\frac{u_{i+1}-u_{i-1}}{2h} + X&#39;(u_i)\frac{u_{i+1}+u_{i-1}-2u_i}{h^2} - \frac{2}{h^2}X(u_i) &amp; j=i \\ -\frac{X&#39;(u_i)}{2h} + \frac{X(u_i)}{h^2} &amp; j=i+1\\ 0 &amp; \mbox{otherwise}  \end{matrix} \right.\]</div><p>Just like the function inverse problem, we also use a neural network to approximate <span>$X(u)$</span>; the difference is that the input of the neural network is <span>$u$</span> instead of <span>$x$</span>. It is convenient to compute <span>$X&#39;(u)$</span> with automatic differentiation. If we had used piecewise linear functions, it is only possible to compute the gradients in the weak sense; but this is not a problem for neural network as long as we use smooth activation function such as <span>$\tanh$</span>. </p><p>ADCME also prepares a built-in Newton-Raphson solver <a href="../newton_raphson/#ADCME.newton_raphson"><code>newton_raphson</code></a> for you. To use this function, you only need to provide the residual and Jacobian </p><pre><code class="language-julia">function residual_and_jacobian(θ, u)
  X = ae(u, [20,20,20,20,1], θ)+1.0 # to avoid X=0 at the first step 
  Xp = tf.gradients(X, u)[1]
  Xpp = tf.gradients(Xp, u)[1]
  up = [u[2:end];constant(zeros(1))]
  un = [constant(zeros(1)); u[1:end-1]]
  R = Xp * (up-un)/2h + X * (up+un-2u)/h^2 + φ
  dRdu = Xpp * (up-un)/2h + Xp*(up+un-2u)/h^2 - 2/h^2*X 
  dRdun = Xp[2:end]/2h + X[2:end]/h^2
  dRdup = -Xp[1:end-1]/2h + X[1:end-1]/h^2
  J = spdiag(n-1, 
	  -1=&gt;dRdun,
  	  0=&gt;dRdu,
      1=&gt;dRdup)
  return R, Array(J)
end</code></pre><p>Then we can solve the Poisson equation with </p><pre><code class="language-julia">newton_raphson(residual_and_jacobian, u0, θ)</code></pre><p>One caveat here is that the Newton-Raphson operator is a <a href="https://kailaix.github.io/ADCME.jl/dev/inverse_modeling/#Forward-Operator-Types-1">nonlinear implicit operator</a> which does not fall into the types of operators where automatic differentiation applies. Instead, a <a href=".">special procedure</a> is needed. Luckily, ADCME provides an API that abstracts away this technical difficulty and users can call <a href="../api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyObject},Union{PyObject, Array{Float64,N} where N}}} where T&lt;:Real"><code>NonlinearConstrainedProblem</code></a> directly to extract the gradients. </p><pre><code class="language-julia">using ADCME 
# definitions of residual_and_jacobian is omited here
n = 100
h = 1/n
x = collect(LinRange(0, 1.0, n+1))

φ = @. 2*(x^2*(x - 1)^2 - x*(x - 1)*(2*x - 1)^2 + (x^2*(x - 1)^2 + 1)^2 + 1)/(x^2*(x - 1)^2 + 1)^2
φ = φ[2:end-1]
θ = Variable(ae_init([1,20,20,20,20,1]))
u0 = constant(zeros(n-1)) #initial guess
function L(u)
  u_obs = (@. x * (1-x))[2:end-1]
	loss = sum((u - u_obs)^2)
end
loss, solution, grad = NonlinearConstrainedProblem(residual_and_jacobian, L, θ, u0)

sess = Session(); init(sess)
BFGS!(sess, loss, grad, θ)</code></pre><p>Note in this case, we only have one set of observations and the inverse problem may be ill-posed, i.e., the solution is not unique. Thus the best we can expect is that we can find one of the solutions <span>$\mathcal{N}(x;w)$</span> that can reproduce the observation we have. This is indeed the case we encounter in this example: the reproduced solution is nearly the same as the observation, but we found a completely different <span>$\mathcal{N}(x;w)$</span> compared with <span>$X(u)$</span>. </p><p><img src="../assets/nn.png" alt="nn"/></p><h2 id="Stochastic-Inverse-Problem-1"><a class="docs-heading-anchor" href="#Stochastic-Inverse-Problem-1">Stochastic Inverse Problem</a><a class="docs-heading-anchor-permalink" href="#Stochastic-Inverse-Problem-1" title="Permalink"></a></h2><p>The final type of inverse problems is called <strong>stochastic inverse problem</strong>. In this problem, <span>$X$</span> is a random variable with unknown distribution. Consequently, the solution <span>$u$</span> will also be a random variable. For example, we may have the following settings in practice</p><ul><li>The measurement of <span>$u(0.5)$</span> may not be accurate. We might assume that <span>$u(0.5) \sim \mathcal{N}(\hat u(0.5), \sigma^2)$</span> where <span>$\hat u(0.5)$</span> is one observation and <span>$\sigma$</span> is the prescribed standard deviation of the measurement. Thus, we want to estimate the distribution of <span>$X$</span> which will produces the same distribution for <span>$u(0.5)$</span>. This type of problem falls under the umbrella of <strong>uncertainty quantification</strong>. </li><li>The quantity <span>$X$</span> itself is subject to randomness in nature, but its distribution may be positively/negatively skewed (e.g., stock price returns). We can measure several samples of <span>$u(0.5)$</span> and want to estimate the distribution of <span>$X$</span> based on the samples. This problem is also called <strong>probablistic inverse problem</strong>. </li></ul><p>We cannot simply minimize the distance between <span>$u(0.5)$</span> and <code>u</code>   (which are random variables) as usual; instead, we need a metric to measure the discrepancy between two distributions–<code>u</code> and <span>$u(0.5)$</span>. The observables <span>$u(0.5)$</span> may be given in multiple forms</p><ul><li>The probability density function. </li><li>The unnormalized log-likelihood function. </li><li>Discrete samples. </li></ul><p>We consider the third type in this tutorial. The idea is to construct a sampler for <span>$X$</span> with a neural network and find the optimal weights and biases by minimizing the discrepancy between actual observed samples  and produced ones. Here is how we train the neural network:</p><p>We first propose a candidate neural network that transforms a sample from <span>$\mathcal{N}(0, I_d)$</span> to a sample from <span>$X$</span>. Then we randomly generate <span>$K$</span> samples <span>$\{z_i\}_{i=1}^K$</span> from <span>$\mathcal{N}(0, I_d)$</span> and transform them to <span>$\{X_i; w\}_{i=1}^K$</span>. We solve the Poisson equation <span>$K$</span> times to obtain <span>$\{u(0.5;z_i, w)\}_{i=1}^K$</span>. Meanwhile, we sample <span>$K$</span> items from the observations (e.g., with the bootstrap method) <span>$\{u_i(0.5)\}_{i=1}^K$</span>. We can use a probability metric <span>$D$</span> to measure the discrepancy between <span>$\{u(0.5;z_i, w)\}_{i=1}^K$</span> and <span>$\{u_i(0.5)\}_{i=1}^K$</span>. There are many choice for <span>$D$</span>, such as (they are not necessarily non-overlapped)</p><ul><li>Wasserstein distance (from optimal transport)</li><li>KL-divergence, JS-divergence, etc. </li><li>Discriminator neural networks (from generative adversarial nets)</li></ul><p>For example, we can consider the first approach, and invoke <a href="@ref"><code>sinkhorn</code></a> provided by ADCME</p><pre><code class="language-julia">using ADCME
using Distributions

# we add a mixture Gaussian noise to the observation
m = MixtureModel(Normal[
   Normal(0.3, 0.1),
   Normal(0.0, 0.1)], [0.5, 0.5])

function solver(a)
  n = 100
  h = 1/n
  A = a[1] * diagm(0=&gt;2/h^2*ones(n-1), 1=&gt;-1/h^2*ones(n-2), -1=&gt;-1/h^2*ones(n-2)) 
  φ = 2.0*ones(n-1) # right hand side
  u = A\φ
  u[50]
end

batch_size = 64
x = placeholder(Float64, shape=[batch_size,10])
z = placeholder(Float64, shape=[batch_size,1])
dat = z + 0.25
fdat  = reshape(map(solver, ae(x, [20,20,20,1])+1.0), batch_size, 1)
loss = empirical_sinkhorn(fdat, dat, dist=(x,y)-&gt;dist(x,y,2), method=&quot;lp&quot;)
opt = AdamOptimizer(0.01, beta1=0.5).minimize(loss)

sess = Session(); init(sess)
for i = 1:100000
  run(sess, opt, feed_dict=Dict(
        x=&gt;randn(batch_size, 10),
        z=&gt;rand(m, batch_size,1)
      ))
end</code></pre><table><tr><th style="text-align: right">Loss Function</th><th style="text-align: right">Iteration 5000</th><th style="text-align: right">Iteration 15000</th><th style="text-align: right">Iteration 25000</th></tr><tr><td style="text-align: right"><img src="../assets/loss.png" alt="loss"/></td><td style="text-align: right"><img src="../assets/test5000.png" alt="test5000"/></td><td style="text-align: right"><img src="../assets/test15000.png" alt="test15000"/></td><td style="text-align: right"><img src="../assets/test25000.png" alt="test25000"/></td></tr></table><p></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../inverse_modeling/">« Inverse Modeling</a><a class="docs-footer-nextpage" href="../array/">Tensor Operations »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 17 December 2019 13:00">Tuesday 17 December 2019</span>. Using Julia version 1.2.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
