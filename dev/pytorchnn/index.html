<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Neural Network in PyTorch C++ · ADCME</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ADCME</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Overview</a></li><li><span class="toctext">Inverse Modeling</span><ul><li><a class="toctext" href="../inverse_modeling/">Overview</a></li></ul></li><li><span class="toctext">Automatic Differentiation</span><ul><li><a class="toctext" href="../four_types/">Forward Operator Types</a></li></ul></li><li><span class="toctext">Resources</span><ul><li><a class="toctext" href="../customop/">Custom Operators</a></li><li><a class="toctext" href="../while_loop/">While Loops</a></li><li><a class="toctext" href="../newton_raphson/">Newton Raphson</a></li><li><a class="toctext" href="../julia_customop/">Julia Custom Operators</a></li><li class="current"><a class="toctext" href>Neural Network in PyTorch C++</a><ul class="internal"></ul></li><li><a class="toctext" href="../extra/">Miscellaneous Tools</a></li><li><a class="toctext" href="../array/">Array Operations</a></li></ul></li><li><span class="toctext">Applications</span><ul><li><a class="toctext" href="../apps/">Adversarial Numerical Analysis</a></li></ul></li><li><a class="toctext" href="../api/">API Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li>Resources</li><li><a href>Neural Network in PyTorch C++</a></li></ul><a class="edit-page" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/pytorchnn.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Neural Network in PyTorch C++</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Neural-Network-in-PyTorch-C-1" href="#Neural-Network-in-PyTorch-C-1">Neural Network in PyTorch C++</a></h1><p>In this section, we describe how we can implement a neural network in C++ with PyTorch APIs. This is useful when we want to create a custom operator in ADCME and a neural network is embedded in the operator (we cannot simply &quot;pass&quot; the neural network to the C++ backend). </p><p>We first need to download <a href="https://pytorch.org/">LibTorch</a> source. Uncompress the library to your working directory. I have created a simple wrapper for some utility functions in ADCME. To use the wrapper, simply add <a href="https://github.com/kailaix/ADCME.jl/blob/master/examples/custom_op/headers/la.h">la.h</a> to your include directories.</p><p>To create a neural network, the following self-explained C++ code can be used</p><pre><code class="language-c">struct Net : torch::nn::Module {
  Net() {
    fc1 = register_module(&quot;fc1&quot;, torch::nn::Linear(3, 64));
    fc2 = register_module(&quot;fc2&quot;, torch::nn::Linear(64, 32));
    fc3 = register_module(&quot;fc3&quot;, torch::nn::Linear(32, 10));
  }

  torch::Tensor forward(torch::Tensor x) {
    x = torch::tanh(fc1-&gt;forward(x));
    x = torch::tanh(fc2-&gt;forward(x));
    x = fc3-&gt;forward(x);
    return x;
  }

  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};</code></pre><div class="admonition info"><div class="admonition-title">Info</div><div class="admonition-text"><p>To create a linear layer with double precison, run <code>fc1-&gt;to(torch::kDouble)</code> after construction. </p></div></div><p><strong>Create a Neural Network</strong></p><pre><code class="language-c">auto nn = std::make_shared&lt;Net&gt;();</code></pre><p><strong>Evaluate an input</strong></p><pre><code class="language-c">auto in = torch::rand({8,3},optf.requires_grad(true));
auto out = nn-&gt;forward(in);</code></pre><p>Here we required gradients with respect to the input <code>in</code> and put <code>optf.requires_grad(true)</code> in the argument.</p><p><strong>Compute Gradients</strong></p><p>To compute gradients, we need to call <code>backward</code> of a <strong>scalar</strong> to populate the gradient entries. For example, assume our neural network model is <span>$y = f_\theta(x)$</span> and we want to compute <span>$\frac{\partial y}{\partial x}$</span>. In our case, <code>x</code> is a <span>$8\times 3$</span> matrix (8 instances of data, each with 3 features). Each output is 10 dimensional. For each input <span>$x_i\in\mathbb{R}^3$</span> and each output feature <span>$y_j\in\mathbb{R}$</span>, we want to compute   <span>$\frac{\partial y_j}{\partial x_i}\in \mathbb{R}^3$</span> For efficiency, we can compute the gradients of all batches simultaneously, i.e., for all <span>$i$</span></p><pre><code class="language-c">auto t = out.sum(0);
t[0].sum().backward();
in.grad().fill_(0.0);</code></pre><p>where we compute 8 vectors of <span>$\mathbb{R}^3$</span>, i.e., <code>in.grad()</code> is a <span>$8\times 3$</span> matrix (the same size as <code>in</code>). </p><p><strong>Access Neural Network Weights and Biases</strong></p><p>The neural network weights and biases can be assessed with </p><pre><code class="language-c">std::cout &lt;&lt; nn-&gt;fc1-&gt;bias &lt;&lt; std::endl;
std::cout &lt;&lt; nn-&gt;fc1-&gt;weights &lt;&lt; std::endl;</code></pre><p>We can also manually set the weight values</p><pre><code class="language-c">nn-&gt;fc1-&gt;bias.set_data(torch::ones({64}));</code></pre><p>The grads can also be computed</p><pre><code class="language-c">std::cout &lt;&lt;  nn-&gt;fc1-&gt;weight.grad() &lt;&lt; std::endl;</code></pre><p><strong>Compile</strong></p><p>To compile the script, in <code>CMakeLists.txt</code>, we have</p><pre><code class="language-txt">cmake_minimum_required(VERSION 3.5)
project(TorchExample)

set(CMAKE_PREFIX_PATH libtorch)
find_package(Torch REQUIRED)

include_directories(&lt;path/to/la.h&gt;)
add_executable(main main.cpp)
target_link_libraries(main &quot;${TORCH_LIBRARIES}&quot;)
set_property(TARGET main PROPERTY CXX_STANDARD 11)</code></pre><p><strong>Full Script</strong></p><pre><code class="language-c">#include &quot;la.h&quot;

struct Net : torch::nn::Module {
  Net() {
    fc1 = register_module(&quot;fc1&quot;, torch::nn::Linear(3, 64));
    fc2 = register_module(&quot;fc2&quot;, torch::nn::Linear(64, 32));
    fc3 = register_module(&quot;fc3&quot;, torch::nn::Linear(32, 10));
  }

  torch::Tensor forward(torch::Tensor x) {
    x = torch::tanh(fc1-&gt;forward(x));
    x = torch::tanh(fc2-&gt;forward(x));
    x = fc3-&gt;forward(x);
    return x;
  }

  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};



int main(){

    auto nn = std::make_shared&lt;Net&gt;();

    auto in = torch::rand({8,3},optf.requires_grad(true));
    auto out = nn-&gt;forward(in);
    
    auto t = out.sum(0);
    t[0].sum().backward();
    in.grad().fill_(0.0);
    std::cout &lt;&lt; out &lt;&lt; std::endl;

    
    std::cout &lt;&lt; nn-&gt;fc1-&gt;bias &lt;&lt; std::endl;
    nn-&gt;fc1-&gt;bias.set_data(torch::ones({64}));
    std::cout &lt;&lt; nn-&gt;fc1-&gt;bias &lt;&lt; std::endl;

    std::cout &lt;&lt; nn-&gt;fc1-&gt;weight &lt;&lt; std::endl;
    std::cout &lt;&lt;  nn-&gt;fc1-&gt;weight.grad() &lt;&lt; std::endl;
    
    return 1;
}</code></pre><footer><hr/><a class="previous" href="../julia_customop/"><span class="direction">Previous</span><span class="title">Julia Custom Operators</span></a><a class="next" href="../extra/"><span class="direction">Next</span><span class="title">Miscellaneous Tools</span></a></footer></article></body></html>
