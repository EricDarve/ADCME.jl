var documenterSearchIndex = {"docs":
[{"location":"pytorchnn/#Neural-Network-in-C-1","page":"Neural Network in C++","title":"Neural Network in C++","text":"","category":"section"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"In this section, we describe how we can implement a neural network in C++. This is useful when we want to create a custom operator in ADCME and a neural network is embedded in the operator (we cannot simply \"pass\" the neural network to the C++ backend). ","category":"page"},{"location":"pytorchnn/#PyTorch-1","page":"Neural Network in C++","title":"PyTorch","text":"","category":"section"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The first method is by using PyTorch C++ APIs.","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"We first need to download LibTorch source. Uncompress the library to your working directory. I have created a simple wrapper for some utility functions in ADCME. To use the wrapper, simply add la.h to your include directories.","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To create a neural network, the following self-explained C++ code can be used","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"struct Net : torch::nn::Module {\n  Net() {\n    fc1 = register_module(\"fc1\", torch::nn::Linear(3, 64));\n    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\n    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\n  }\n\n  torch::Tensor forward(torch::Tensor x) {\n    x = torch::tanh(fc1->forward(x));\n    x = torch::tanh(fc2->forward(x));\n    x = fc3->forward(x);\n    return x;\n  }\n\n  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\n};","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"note: Note\nTo create a linear layer with double precison, run fc1->to(torch::kDouble) after construction. ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Create a Neural Network","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"auto nn = std::make_shared<Net>();","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Evaluate an input","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"auto in = torch::rand({8,3},optf.requires_grad(true));\nauto out = nn->forward(in);","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Here we required gradients with respect to the input in and put optf.requires_grad(true) in the argument.","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Compute Gradients","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To compute gradients, we need to call backward of a scalar to populate the gradient entries. For example, assume our neural network model is y = f_theta(x) and we want to compute fracpartial ypartial x. In our case, x is a 8times 3 matrix (8 instances of data, each with 3 features). Each output is 10 dimensional. For each input x_iinmathbbR^3 and each output feature y_jinmathbbR, we want to compute   fracpartial y_jpartial x_iin mathbbR^3 For efficiency, we can compute the gradients of all batches simultaneously, i.e., for all i","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"auto t = out.sum(0);\nt[0].sum().backward();\nin.grad().fill_(0.0);","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"where we compute 8 vectors of mathbbR^3, i.e., in.grad() is a 8times 3 matrix (the same size as in). ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Access Neural Network Weights and Biases","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The neural network weights and biases can be assessed with ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"std::cout << nn->fc1->bias << std::endl;\nstd::cout << nn->fc1->weights << std::endl;","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"We can also manually set the weight values","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"nn->fc1->bias.set_data(torch::ones({64}));","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The grads can also be computed","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"std::cout <<  nn->fc1->weight.grad() << std::endl;","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Compile","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To compile the script, in CMakeLists.txt, we have","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"cmake_minimum_required(VERSION 3.5)\nproject(TorchExample)\n\nset(CMAKE_PREFIX_PATH libtorch)\nfind_package(Torch REQUIRED)\n\ninclude_directories(<path/to/la.h>)\nadd_executable(main main.cpp)\ntarget_link_libraries(main \"${TORCH_LIBRARIES}\")\nset_property(TARGET main PROPERTY CXX_STANDARD 11)","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Full Script","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"#include \"la.h\"\n\nstruct Net : torch::nn::Module {\n  Net() {\n    fc1 = register_module(\"fc1\", torch::nn::Linear(3, 64));\n    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\n    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\n  }\n\n  torch::Tensor forward(torch::Tensor x) {\n    x = torch::tanh(fc1->forward(x));\n    x = torch::tanh(fc2->forward(x));\n    x = fc3->forward(x);\n    return x;\n  }\n\n  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\n};\n\n\n\nint main(){\n\n    auto nn = std::make_shared<Net>();\n\n    auto in = torch::rand({8,3},optf.requires_grad(true));\n    auto out = nn->forward(in);\n    \n    auto t = out.sum(0);\n    t[0].sum().backward();\n    in.grad().fill_(0.0);\n    std::cout << out << std::endl;\n\n    \n    std::cout << nn->fc1->bias << std::endl;\n    nn->fc1->bias.set_data(torch::ones({64}));\n    std::cout << nn->fc1->bias << std::endl;\n\n    std::cout << nn->fc1->weight << std::endl;\n    std::cout <<  nn->fc1->weight.grad() << std::endl;\n    \n    return 1;\n}","category":"page"},{"location":"pytorchnn/#Adept-2-1","page":"Neural Network in C++","title":"Adept-2","text":"","category":"section"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The second approach is to use a third-party automatic differentiation library. Here we use Adept-2. The idea is that we code a neural network in C++ using array operations. ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To start with, download and compile Adept-2 with the following command","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"if [ ! -d \"Adept-2\" ]; then\n  git clone https://github.com/rjhogan/Adept-2\nfi\ncd Adept-2\nautoreconf -i\n./configure\nmake -j\nmake check\nmake install","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"All the libraries should be available in Adept-2/adept/.libs. The following script shows a simple neural network implementation","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"#include \"adept.h\"\n#include \"adept_arrays.h\"\n#include <iostream>\nusing namespace adept;\nusing namespace std;\n\nint main()\n{\n  Stack stack;\n  Array<2, double, true> X(100,3), W1(3,20), W2(20,20), W3(20,4);\n  Array<1, double, true> b1(20), b2(20), b3(4);\n  double V[400];\n  for(int i=0;i<300;i++) X[i] = 0.01*i;\n  for(int i=0;i<60;i++) W1[i] = 0.01*i;\n  for(int i=0;i<400;i++) W2[i] = 0.01*i;\n  for(int i=0;i<80;i++) W3[i] = 0.01*i;\n  for(int i=0;i<20;i++) b1[i] = 0.01*i;\n  for(int i=0;i<20;i++) b2[i] = 0.01*i;\n  for(int i=0;i<4;i++) b3[i] = 0.01*i;\n\n  stack.new_recording();\n  auto x = X**W1;\n  Array<2, double, true> y1(x.size(0),x.size(1));\n  for(int i=0;i<x.size(0);i++) \n    for(int j=0;j<x.size(1);j++)\n        y1(i,j) = tanh(x(i,j)+b1(j));\n    \n  auto w = y1**W2;\n  Array<2, double, true> y2(w.size(0),w.size(1));\n  for(int i=0;i<w.size(0);i++) \n    for(int j=0;j<w.size(1);j++)\n        y2(i,j) = tanh(w(i,j)+b2(j));\n\n  auto z = y2**W3;\n  Array<2, double, true> y3(z.size(0),z.size(1));\n  for(int i=0;i<z.size(0);i++) \n    for(int j=0;j<z.size(1);j++)\n        y3(i,j) = z(i,j)+b3(j);\n  \n  auto out = sum(y3, 0);\n  \n  out[0].set_gradient(1.0);\n  stack.compute_adjoint();\n  auto g1 = X.get_gradient();\n  cout << g1 << endl;\n\n  auto g2 = W3.get_gradient();\n  cout << g2 << endl;\n\n  out[1].set_gradient(1.0);\n  stack.compute_adjoint();\n  auto g1_ = X.get_gradient();\n  cout << g1_ << endl;\n\n  auto g2_ = W3.get_gradient();\n  cout << g2_ << endl;\n\n\n  y3(0,0).set_gradient(1.0);\n  stack.compute_adjoint();\n  auto g1__ = X.get_gradient();\n  cout << g1__ << endl;\n  auto g2__ = W3.get_gradient();\n  cout << g2__ << endl;\n}","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The following codes might be useful ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"typedef Array<2, double, true> Array2D;\ntypedef Array<1, double, true> Array1D;\n\nvoid setv(Array1D& v,  const double *val){\n    int n = v.size(0);\n    for(int i=0;i<n;i++) v(i).set_value(val[i]);\n}\n\nvoid setv(Array2D& v, const double *val){\n    int k = 0;\n    int m = v.size(0), n = v.size(1);\n    for(int i=0;i<m;i++){\n        for(int j=0;j<n;j++)\n            v(i,j).set_value(val[k++]);\n    }\n}\n\nvoid getg(Array1D& v, double *val){\n    int n = v.size(0);\n    auto gv = v.get_gradient();\n    for(int i=0;i<n;i++) val[i] = value(gv(i));\n}\n\nvoid getg(Array2D& v, double *val){\n    int k = 0;\n    int m = v.size(0), n = v.size(1);\n    auto gv = v.get_gradient();\n    for(int i=0;i<m;i++){\n        for(int j=0;j<n;j++)\n            val[k++] = value(gv(i, j));\n    }\n}","category":"page"},{"location":"customop/#Custom-Operators-1","page":"Custom Operators","title":"Custom Operators","text":"","category":"section"},{"location":"customop/#Basic-Usage-1","page":"Custom Operators","title":"Basic Usage","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Custom operators are ways to add missing features in ADCME. Typically users do not have to worry about custom operators. However, in the following situation custom opreators might be very useful","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Direct implementation in ADCME is inefficient (bottleneck). \nThere are legacy codes users want to reuse, such as GPU-accelerated codes. \nSpecial acceleration techniques such as checkpointing scheme. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In the following, we present an example of implementing the sparse solver custom operator for Ax=b.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Input: row vector ii, column vectorjj and value vector vv for the sparse coefficient matrix; row vector kk and value vector ff, matrix dimension d","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Output: solution vector u","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 1: Create and modify the template file","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The following command helps create the wrapper","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"customop()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There will be a custom_op.txt in the current directory. Modify the template file ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"MySparseSolver\nint32 ii(?)\nint32 jj(?)\ndouble vv(?)\nint32 kk(?)\ndouble ff(?)\nint32 d()\ndouble u(?) -> output","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The first line is the name of the operator. It should always be in the camel case. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The 2nd to the 7th lines specify the input arguments, the signature is type+variable name+shape. For the shape, () corresponds to a scalar, (?) to a vector and (?,?) to a matrix. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The last line is the output, denoted by -> output. Note there must be a space before and after ->. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The following types are accepted: int32, int64, double, float, string, bool. The name of the arguments must all be in lower cases. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 2: Implement core codes","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Run customop() again and there will be CMakeLists.txt, gradtest.jl, MySparseSolver.cpp appearing in the current directory. MySparseSolver.cpp is the main wrapper for the codes and gradtest.jl is used for testing the operator and its gradients. CMakeLists.txt is the file for compilation. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Create a new file MySparseSolver.h and implement both the forward simulation and backward simulation (gradients)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"#include <eigen3/Eigen/Sparse>\n#include <eigen3/Eigen/SparseLU>\n#include <vector>\n#include <iostream>\nusing namespace std;\ntypedef Eigen::SparseMatrix<double> SpMat; // declares a column-major sparse matrix type of double\ntypedef Eigen::Triplet<double> T;\n\nSpMat A;\n\nvoid forward(double *u, const int *ii, const int *jj, const double *vv, int nv, const int *kk, const double *ff,int nf,  int d){\n    vector<T> triplets;\n    Eigen::VectorXd rhs(d); rhs.setZero();\n    for(int i=0;i<nv;i++){\n      triplets.push_back(T(ii[i]-1,jj[i]-1,vv[i]));\n    }\n    for(int i=0;i<nf;i++){\n      rhs[kk[i]-1] += ff[i];\n    }\n    A.resize(d, d);\n    A.setFromTriplets(triplets.begin(), triplets.end());\n    auto C = Eigen::MatrixXd(A);\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(A);\n    solver.factorize(A);\n    auto x = solver.solve(rhs);\n    for(int i=0;i<d;i++) u[i] = x[i];\n}\n\nvoid backward(double *grad_vv, const double *grad_u, const int *ii, const int *jj, const double *u, int nv, int d){\n    Eigen::VectorXd g(d);\n    for(int i=0;i<d;i++) g[i] = grad_u[i];\n    auto B = A.transpose();\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(B);\n    solver.factorize(B);\n    auto x = solver.solve(g);\n    // cout << x << endl;\n    for(int i=0;i<nv;i++) grad_vv[i] = 0.0;\n    for(int i=0;i<nv;i++){\n      grad_vv[i] -= x[ii[i]-1]*u[jj[i]-1];\n    }\n}","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nIn this implementation we have used Eigen library for solving sparse matrix. Other choices are also possible, such as algebraic multigrid methods. Note here for convenience we have created a global variable SpMat A;. This is not recommend if you want to run the code concurrently. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 3: Compile","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"It is recommended that you use the cmake, make and gcc provided by ADCME. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Variable Description\nADCME.CXX C++ Compiler\nADCME.CC C Compiler\nADCME.TFLIB libtensorflow_framework.so location\nADCME.CMAKE Cmake binary location\nADCME.MAKE Make binary location","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Make a build directory in bash.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"mkdir build\ncd build","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Configure CMake files.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"julia> using ADCME\njulia> ADCME.cmake()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Build. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"make -j","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nIf the system make command is not compatible, try the pre-installed ADCME make located at ADCME.MAKE. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Based on your operation system, you will create libMySparseSolver.{so,dylib,dll}. This will be the dynamic library to link in TensorFlow. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 4: Test","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Finally, you could use gradtest.jl to test the operator and its gradients (specify appropriate data in gradtest.jl first). If you implement the gradients correctly, you will be able to obtain first order convergence for finite difference and second order convergence for automatic differentiation. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"(Image: custom_op)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"If the process fails, it is most probability the GCC compiler is not compatible with which was used to compile libtensorflow_framework.{so,dylib}. In the Linux system, you can check the compiler using ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"readelf -p .comment libtensorflow_framework.so","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Compatibility issues are frustrating. We hope you can submit an issue to ADCME developers; we are happy to resolve the compatibility issue and improve the robustness of ADCME.","category":"page"},{"location":"customop/#GPU-Operators-1","page":"Custom Operators","title":"GPU Operators","text":"","category":"section"},{"location":"customop/#Dependencies-1","page":"Custom Operators","title":"Dependencies","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To create a GPU custom operator, you must have NVCC compiler and CUDA toolkit installed on your system. To install NVCC, see the installation guide. To check you have successfully installed NVCC, type","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"which nvcc","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"It should gives you the location of nvcc compiler.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"For quick installation, you can try","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using ADCME\nenable_gpu()","category":"page"},{"location":"customop/#Manual-Installation-1","page":"Custom Operators","title":"Manual Installation","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In case ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To install CUDA toolkit (if you do not have one), you can install via conda","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using Conda\nConda.add(\"cudatoolkit\", channel=\"anaconda\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The next step is to cp the CUDA include file to tensorflow include directory. This could be done with ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using ADCME\ngpus = joinpath(splitdir(tf.__file__)[1], \"include/third_party/gpus\")\nif !isdir(gpus)\n  mkdir(gpus)\nend\ngpus = joinpath(gpus, \"cuda\")\nif !isdir(gpus)\n  mkdir(gpus)\nend\nincpath = joinpath(splitdir(strip(read(`which nvcc`, String)))[1], \"../include/\")\nif !isdir(joinpath(gpus, \"include\"))\n    mv(incpath, gpus)\nend","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Finally, add the CUDA library path to LD_LIBRARY_PATH. This can be done by adding the following line to .bashrc","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"export LD_LIBRARY_PATH=<path>:$LD_LIBRARY_PATH","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"where <path> is ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"joinpath(Conda.ROOTENV, \"pkgs/cudatoolkit-10.1.168-0/lib/\")","category":"page"},{"location":"customop/#File-Organization-1","page":"Custom Operators","title":"File Organization","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There should be three files in your source directories","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"MyOp.cpp: driver file\nMyOp.cu: GPU implementation\nMyOp.h: CPU implementation","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The first two files have been generated for you by customop(). The following are two important notes on the implementation.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In MyOp.cu, the implementation usually has the structure","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"namespace tensorflow{\n  typedef Eigen::GpuDevice GPUDevice;\n\n    __global__ void forward_(const int nthreads, double *out, const double *y, const double *H0, int n){\n      for(int i : CudaGridRangeX(nthreads)) {\n          // do something here\n      }\n    }\n\n    void forwardGPU(double *out, const double *y, const double *H0, int n, const GPUDevice& d){\n      // forward_<<<(n+255)/256, 256>>>(out, y, H0, n);\n      GpuLaunchConfig config = GetGpuLaunchConfig(n, d);\n      TF_CHECK_OK(GpuLaunchKernel(\n          forward_, config.block_count, config.thread_per_block, 0,\n          d.stream(), config.virtual_thread_count, out, y, H0, n));\n      }\n}","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In MyOp.cpp, the device information (const GPUDevice& d above) is obtained with ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"context->eigen_device<GPUDevice>()","category":"page"},{"location":"customop/#Mutable-Inputs-1","page":"Custom Operators","title":"Mutable Inputs","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Sometimes we want to modify tensors in place. In this case we can use mutable inputs. Mutable inputs must be Variable and it must be forwarded to one of the output. We consider implement a my_assign operator, with signature","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"my_assign(u::PyObject, v::PyObject)::PyObject","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Here u is a Variable and we copy the data from v to u. In the MyAssign.cpp file, we modify the input and output specifications to ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":".Input(\"u : Ref(double)\")\n.Input(\"v : double\")\n.Output(\"w : Ref(double)\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In addition, the input tensor is obtained through","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Tensor u = context->mutable_input(0, true);","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The second argument lock_held specifies whether the input mutex is acquired (false) before the operation. Note the output must be a Tensor instead of a reference. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To forward the input, use","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"context->forward_ref_input_to_ref_output(0,0);","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"We use the following code snippet to test the program","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"my_assign = load_op(\"./build/libMyAssign\",\"my_assign\")\nu = Variable([0.1,0.2,0.3])\nv = constant(Array{Float64}(1:3))\nu2 = u^2\nw = my_assign(u,v)\nsess = tf.Session()\ninit(sess)\n@show run(sess, u)\n@show run(sess, u2)\n@show run(sess, w)\n@show run(sess, u2)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The output is ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"[0.1,0.2,0.3]\n[0.1,0.04,0.09]\n[1.0,2.0,3.0]\n[1.0,4.0,9.0]","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"We can see that the tensors depending on u are also aware of the assign operator. The complete programs can be downloaded here: CMakeLists.txt, MyAssign.cpp, gradtest.jl.","category":"page"},{"location":"customop/#Third-party-Plugins-1","page":"Custom Operators","title":"Third-party Plugins","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"ADCME also allows third-party custom operators hosted on Github. To build your own custom operators, implement your own custom operators in a Github repository. The root directory of the repository should have the following files","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"formula.txt, which tells how ADCME should interact with the custom operator. It is a Julia Pair, which has the format\nsignature => (source_directory, library_name, signature, has_gradient)\nFor example\n\"ot_network\"=>(\"OTNetwork\", \"libOTNetwork\", \"ot_network\", true)\nCMakeLists.txt, which is used for compiling the library. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Users are free to arrange other source files or other third-party libraries. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Upon using those libraries in ADCME, users first download those libraries to deps directory via","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"install(\"https://github.com/ADCMEMarket/OTNetwork\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The official plugins are hosted on https://github.com/ADCMEMarket. To get access to the custom operators in ADCME, use","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"op = load_system_op(\"OTNetwork\")","category":"page"},{"location":"customop/#A-List-of-Official-Custom-Operators-1","page":"Custom Operators","title":"A List of Official Custom Operators","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Name Description URL\nOTNetwork Network Flow Solver for Optimal Transport https://github.com/ADCMEMarket/OTNetwork/","category":"page"},{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-Julia-Custom-Operator-in-ADCMAE-1","page":"-","title":"Quick Reference for Implementing Julia Custom Operator in ADCMAE","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Header files","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"  #include \"julia.h\"\n  #include \"Python.h\"","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"For Python GIL workround","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n...\nPyGILState_Release(py_threadstate);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Get function from Julia main module ","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_get_function(jl_main_module, \"myfun\");","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ to Julia","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t *a = jl_box_float64(3.0);\njl_value_t *b = jl_box_float32(3.0f);\njl_value_t *c = jl_box_int32(3);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia to C++","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double ret_unboxed = jl_unbox_float64(ret);\nfloat  ret_unboxed = jl_unbox_float32(ret);\nint32  ret_unboxed = jl_unbox_int32(ret);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ Arrays to Julia Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\njl_array_t* x          = jl_alloc_array_1d(array_type, 10);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"or for existing arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *existingArray = (double*)malloc(sizeof(double)*10);\njl_array_t *x = jl_ptr_to_array_1d(array_type, existingArray, 10, 0);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia Arrays to C++ Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *xData = (double*)jl_array_data(x);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Call Julia Functions","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_array_t *y = (jl_array_t*)jl_call1(func, (jl_value_t*)x);\njl_value_t *jl_call(jl_function_t *f, jl_value_t **args, int32_t nargs)","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Gabage collection","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 2); // args can now hold 2 `jl_value_t*` objects\nargs[0] = some_value;\nargs[1] = some_other_value;\n// Do something with args (e.g. call jl_... functions)\nJL_GC_POP();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Reference: Embedding Julia","category":"page"},{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1","page":"-","title":"Quick Reference for Implementing C++ Custom Operators in ADCME","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Set output shape","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"c->set_output(0, c->Vector(n));\nc->set_output(0, c->Matrix(m, n));\nc->set_output(0, c->Scalar());","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Names","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":".Input and .Ouput : names must be in lower case, no _, only letters.","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"TensorFlow Input/Output to TensorFlow Tensors","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.vec<double>();\ngrad.scalar<double>();\ngrad.matrix<double>();\ngrad.flat<double>();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Obtain flat arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.flat<double>().data()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Scalars","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate scalars using TensorShape()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate Shapes","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Although you can use -1 for shape reference, you must allocate exact shapes in Compute","category":"page"},{"location":"extra/#Miscellaneous-Tools-1","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"There are many handy tools implemented in ADCME for analysis, benchmarking, input/output, etc. ","category":"page"},{"location":"extra/#Debugging-and-Printing-1","page":"Miscellaneous Tools","title":"Debugging and Printing","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Add the following line before Session and change tf.Session to see verbose printing (such as GPU/CPU information)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.debugging.set_log_device_placement(true)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.print can be used for printing tensor values. It must be binded with an executive operator.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"# a, b are tensors, and b is executive\nop = tf.print(a)\nb = bind(b, op)","category":"page"},{"location":"extra/#Debugging-Python-Codes-1","page":"Miscellaneous Tools","title":"Debugging Python Codes","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"If the error comes from Python (through PyCall), we can print out the Python trace with the following commands","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"import traceback\ntry:\n    # Your codes here \nexcept Exception:\n    print(traceback.format_exc())","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"This Python script can be inserted to Julia and use interpolation to invoke Julia functions (in the comment line).","category":"page"},{"location":"extra/#Profiling-1","page":"Miscellaneous Tools","title":"Profiling","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Profiling can be done with the help of run_profile and save_profile","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"a = normal(2000, 5000)\nb = normal(5000, 1000)\nres = a*b \nrun_profile(sess, res)\nsave_profile(\"test.json\")","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Open Chrome and navigate to chrome://tracing\nLoad the timeline file","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Below shows an example of profiling results.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"(Image: )","category":"page"},{"location":"extra/#Save-and-Load-Python-Object-1","page":"Miscellaneous Tools","title":"Save and Load Python Object","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"psave\npload","category":"page"},{"location":"extra/#ADCME.psave","page":"Miscellaneous Tools","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.pload","page":"Miscellaneous Tools","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-Diary-1","page":"Miscellaneous Tools","title":"Save and Load Diary","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"We can use TensorBoard to track a scalar value easily","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"d = Diary(\"test\")\np = placeholder(1.0, dtype=Float64)\nb = constant(1.0)+p\ns = scalar(b, \"variable\")\nfor i = 1:100\n    write(d, i, run(sess, s, Dict(p=>Float64(i))))\nend\nactivate(d)","category":"page"},{"location":"ode/#PDE/ODE-Solvers-1","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"","category":"section"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"info: Info\nThe PDE/ODE solver features are currently under heavy development. We aim to provide a complete set of built-in PDE/ODE solvers.","category":"page"},{"location":"ode/#Runge-Kutta-Method-1","page":"PDE/ODE Solvers","title":"Runge Kutta Method","text":"","category":"section"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"The Runge Kutta method is one of the workhorses for solving ODEs. The method is a higher order interpolation to the derivatives. The system of ODE has the form","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"fracdydt = f(y t theta)","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"where t denotes time, y denotes states and theta denotes parameters. ","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"The Runge-Kutta method is defined as","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"beginaligned\nk_1 = Delta t f(t_n y_n theta)\nk_2 = Delta t f(t_n+Delta t2 y_n + k_12 theta)\nk_3 = Delta t f(t_n+Delta t2 y_n + k_22 theta)\nk_4 = Delta t f(t_n+Delta t y_n + k_3 theta)\ny_n+1 = y_n + frack_16 +frack_23 +frack_33 +frack_46\nendaligned","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"ADCME provides a built-in Runge Kutta solver rk4 and ode45. Consider an example: the Lorentz equation","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"beginaligned\nfracdxdt = 10(y-x) \nfracdydt = x(27-z)-y \nfracdzdt = xy -frac83z\nendaligned","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"Let the initial condition be x_0 = 100, the following code snippets solves the Lorentz equation with ADCME","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"function f(t, y, θ)\n    [10*(y[2]-y[1]);y[1]*(27-y[3])-y[2];y[1]*y[2]-8/3*y[3]]\nend\nx0 = [1.;0.;0.]\nrk4(f, 30.0, 10000, x0)","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"(Image: )","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"We can also solve three body problem with the Runge-Kutta method. The full script is ","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"# \n# adapted from \n# https://github.com/pjpmarques/Julia-Modeling-the-World/\n# \nusing Revise\nusing ADCME\nusing PyPlot\nusing Printf\n\nfunction f(t, y, θ)\n    # Extract the position and velocity vectors from the g array\n    r0, v0 = y[1:2], y[3:4]\n    r1, v1 = y[5:6], y[7:8]\n    r2, v2 = y[9:10], y[11:12]\n    \n    # The derivatives of the position are simply the velocities\n    dr0 = v0\n    dr1 = v1\n    dr2 = v2\n    \n    # Now calculate the the derivatives of the velocities, which are the accelarations\n    # Start by calculating the distance vectors between the bodies (assumes m0, m1 and m2 are global variables)\n    # Slightly rewriten the expressions dv0, dv1 and dv2 comprared to the normal equations so we can reuse d0, d1 and d2\n    d0  = (r2 - r1) / ( norm(r2 - r1)^3.0 )\n    d1  = (r0 - r2) / ( norm(r0 - r2)^3.0 )\n    d2  = (r1 - r0) / ( norm(r1 - r0)^3.0 )    \n    \n    dv0 = m1*d2 - m2*d1\n    dv1 = m2*d0 - m0*d2\n    dv2 = m0*d1 - m1*d0\n    \n    # Reconstruct the derivative vector\n    [dr0; dv0; dr1; dv1; dr2; dv2]\nend\n\nfunction plot_trajectory(t1, t2)\n\n    t1i = round(Int,NT * t1/T) + 1\n    t2i = round(Int,NT * t2/T) + 1\n    \n    # Plot the initial and final positions\n    # In these vectors, the first coordinate will be X and the second Y\n    X = 1\n    Y = 2\n    \n    # figure(figsize=(6,6))\n    plot(r0[t1i,X], r0[t1i,Y], \"ro\")\n    plot(r0[t2i,X], r0[t2i,Y], \"rs\")\n    plot(r1[t1i,X], r1[t1i,Y], \"go\")\n    plot(r1[t2i,X], r1[t2i,Y], \"gs\")\n    plot(r2[t1i,X], r2[t1i,Y], \"bo\")\n    plot(r2[t2i,X], r2[t2i,Y], \"bs\")\n    \n    # Plot the trajectories\n    plot(r0[t1i:t2i,X], r0[t1i:t2i,Y], \"r-\")\n    plot(r1[t1i:t2i,X], r1[t1i:t2i,Y], \"g-\")\n    plot(r2[t1i:t2i,X], r2[t1i:t2i,Y], \"b-\")\n    \n    # Plot cente of mass\n    # plot(cx[t1i:t2i], cy[t1i:t2i], \"kx\")\n    \n    # Setup the axis and titles\n    xmin = minimum([r0[t1i:t2i,X]; r1[t1i:t2i,X]; r2[t1i:t2i,X]]) * 1.10\n    xmax = maximum([r0[t1i:t2i,X]; r1[t1i:t2i,X]; r2[t1i:t2i,X]]) * 1.10\n    ymin = minimum([r0[t1i:t2i,Y]; r1[t1i:t2i,Y]; r2[t1i:t2i,Y]]) * 1.10\n    ymax = maximum([r0[t1i:t2i,Y]; r1[t1i:t2i,Y]; r2[t1i:t2i,Y]]) * 1.10\n    \n    axis([xmin, xmax, ymin, ymax])\n    title(@sprintf \"3-body simulation for t=[%.1f .. %.1f]\" t1 t2)\nend;\n\nm0 = 5.0\nm1 = 4.0\nm2 = 3.0\n\n# Initial positions and velocities of each body (x0, y0, vx0, vy0) \ngi0 = [ 1.0; -1.0; 0.0; 0.0]\ngi1 = [ 1.0;  3.0; 0.0; 0.0]\ngi2 = [-2.0; -1.0; 0.0; 0.0]\n\n\nT  = 30.0\nNT  = 500*300\ng0  = [gi0; gi1; gi2]\n\nres_ = ode45(f, T, NT, g0)\n\nsess = Session(); init(sess)\nres = run(sess, res_)\n\nr0, v0, r1, v1, r2, v2 = res[:,1:2], res[:,3:4], res[:,5:6], res[:,7:8], res[:,9:10], res[:,11:12]\n\nfigure(figsize=[4,1])\nsubplot(131); plot_trajectory(0.0,10.0)\nsubplot(132); plot_trajectory(10.0,20.0)\nsubplot(133); plot_trajectory(20.0,30.0)","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"(Image: )","category":"page"},{"location":"apps_ana/#Adversarial-Numerical-Analysis-1","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"","category":"section"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Kailai Xu, and Eric Darve. \"Adversarial Numerical Analysis for Inverse Problems\"","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Project Website","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Many scientific and engineering applications are formulated as inverse problems associated with stochastic models. In such cases the unknown quantities are distributions. The applicability of traditional methods is limited because of their demanding assumptions or prohibitive computational consumptions; for example, maximum likelihood methods require closed-form density functions, and Markov Chain Monte Carlo needs a large number of simulations. ","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Consider the forward model","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"x = F(w theta)","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Here w is a known stochastic process such as Gaussian processes, theta is an unknown parameter, distribution or stochastic processes. Consequently, the output of the model x is also a stochastic process. F can be a very complicated model such as a system of partial differential equations. Many models fall into this category; here  we solve an inverse modeling problem of boundary value Poisson equations","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"\\begin{cases}     -\\nabla \\cdot (a(x)\\nabla u(x)) = 1 & x\\in(0,1)\\\\\n    u(0) = u(1) = 0 & \\mbox{otherwise} \\end{cases}","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"a(x) = 1-09expleft( -frac(x-mu)^22sigma^2 right)","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Here (mu, sigma) is subject to unknown distribution (theta in the forward model). w=emptyset and x is the solution to the equation, u. Assume we have observed a set of solutions u_i, and we want to estimate the distribution of (mu, sigma). Adversarial numerical analysis works as follows","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"The distribution (mu, sigma) is parametrized by a deep neural network G_eta.\nFor each instance of (mu, sigma) sampled from the neural network parametrized distribution, we can compute a solution u_mu sigma using the finite difference method. \nWe compute a metric between the distribution u_mu sigma and u_i with a discriminative neural network D_xi.\nMinimize the metric by adjusting the weights of G_eta and D_xi. ","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"The distribution of (mu, sigma) is given by G_eta. The following plots show the workflow of adversarial numerical analysis and a sample result for the Dirichlet distribution. ","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"(Image: )","category":"page"},{"location":"ot/#Optimal-Transport-1","page":"Optimal Transport","title":"Optimal Transport","text":"","category":"section"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Optimal transport (OT) can be used to measure the \"distance\" between two probability distribution. In this chapter, we introduce a novel approach for training a general model: SinkHorn Generative Networks (SGN). In this approach, a neural network is used to transform a sample from uniform distributions to a sample of targeted distribution. We train the neural network by minimizing the discrepancy between the targeted distribution and the desired distribution, which is described by optimal transport distance. Different from generative adversarial nets (GAN), we do not use a discriminator neural network to construct the discrepancy; instead, it is computed directly with efficient SinkHorn algorithm. The minimization is conducted via a gradient-based optimizer, where the gradients are computed with reverse mode automatic differentiation. ","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"To begin with, we first construct the sample x of the targeted distribution and the sample s from the desired distribution and compute the loss function with sinkhorn","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"using Revise\nusing ADCME\nusing PyPlot\n\nreset_default_graph()\nK = 64\nz = placeholder(Float64, shape=[K, 10])\nx = squeeze(ae(z, [20,20,20,1]))\ns = placeholder(Float64, shape=[K])\nM = abs(reshape(x, -1, 1) - reshape(s, 1, -1))\nloss = sinkhorn(ones(K)/K, ones(K)/K, M, reg=0.1)","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Example 1 In the first example, we assume the desired distribution is the standard Gaussian. We minimize the loss function with AdamOptimizer","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"opt = AdamOptimizer().minimize(loss)\nsess = Session(); init(sess)\nfor i = 1:10000\n    _, l = run(sess, [opt, loss], z=>rand(K, 10), s=>randn(K))\n    @show i, l\nend","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"The result is shown below","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"V = []\nfor k = 1:100\n    push!(V,run(sess, x, z=>rand(K,10)))\nend\nV = vcat(V...)\nhist(V, bins=50, density=true)\nx0 = LinRange(-3.,3.,100)\nplot(x0, (@. 1/sqrt(2π)*exp(-x0^2/2)), label=\"Reference\")\nxlabel(\"x\")\nylabel(\"f(x)\")\nlegend()","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"(Image: )","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Example 2 In the first example, we assume the desired distribution is the positive part of the the standard Gaussian. ","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"opt = AdamOptimizer().minimize(loss)\nsess = Session(); init(sess)\nfor i = 1:10000\n    _, l = run(sess, [opt, loss], z=>rand(K, 10), s=>abs.(randn(K)))\n    @show i, l\nend","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"(Image: )","category":"page"},{"location":"array/#Tensor-Operations-1","page":"Tensor Operations","title":"Tensor Operations","text":"","category":"section"},{"location":"array/#","page":"Tensor Operations","title":"Tensor Operations","text":"Description API\nConstant creation constant(rand(10))\nVariable creation Variable(rand(10))\nGet size size(x)\nGet size of dimension size(x,i)\nGet length length(x)\nResize reshape(x,5,3)\nVector indexing v[1:3],v[[1;3;4]],v[3:end],v[:]\nMatrix indexing m[3,:], m[:,3], m[1,3],m[[1;2;5],[2;3]]\nIndex relative to end v[end], m[1,end]\nExtract row (most efficient) m[2], m[2,:]\nExtract column m[:,3]\nConvert to dense diagonal matrix diagm(v)\nConvert to sparse diagonal matrix spdiag(v)\nExtract diagonals as vector diag(m)\nElementwise multiplication a.*b\nMatrix (vector) multiplication a*b\nMatrix transpose m'\nDot product sum(a*b)\nSolve A\\b\nInversion inv(m)\nAverage all elements mean(x)\nAverage along dimension mean(x, dims=1)\nMaximum/Minimum of all elements maximum(x), minimum(x)\nSqueeze all single dimensions squeeze(x)\nSqueeze along dimension squeeze(x, dims=1), squeeze(x, dims=[1;2])\nReduction (along dimension) norm(a), sum(a, dims=1)\nElementwise Multiplication a.*b\nElementwise Power a^2\nSVD svd(a)","category":"page"},{"location":"parallel/#Parallel-Computing-1","page":"Parallel Computing","title":"Parallel Computing","text":"","category":"section"},{"location":"parallel/#Manually-Place-Operators-on-Devices-1","page":"Parallel Computing","title":"Manually Place Operators on Devices","text":"","category":"section"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"ADCME backend TensorFlow treats each operator as the smallest computation unit. Users are allowed to manually assign the device locations for each operator. This is usually done with the @pywith tf.device(\"/cpu:0\") syntax. For example, if we want to create a variable a and compute sin(a) on GPU:0 we can write","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(\"/GPU:0\") begin\n    global a = Variable(1.0)\n    global b = sin(a)\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Custom Device Placement Functions","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"This syntax is useful and simple for placing operators on certain GPU devices without changing original codes. However, sometimes we want to place certain operators on certain devices. This can be done by implementing a custom assign_to_device function. As an example, we want to place all Variables on CPU:0 while placing all other operators on GPU:0, the function has the following form","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"PS_OPS = [\"Variable\", \"VariableV2\", \"AutoReloadVariable\"]\nfunction assign_to_device(device, ps_device=\"/device:CPU:0\")\n    function _assign(op)\n        node_def = pybuiltin(\"isinstance\")(op, tf.NodeDef) ? op : op.node_def\n        if node_def.op in PS_OPS\n            return ps_device\n        else\n            return device\n        end\n    end\n\n    return _assign\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Then we can write something like","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(assign_to_device(\"/device:GPU:0\")) begin\n    global a = Variable(1.0)\n    global b = sin(a)\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"We can check the location of a and b by inspecting their device attributes","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"julia> a.device\n\"/device:CPU:0\"\n\njulia> b.device\n\"/device:GPU:0\"","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Colocate Gradient Operators","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"When we call gradients, TensorFlow actually creates a set of new operators, one for each operator in the forward computation. By default, those operators are placed on the default device (GPU:0 if GPU is available; otherwise it's CPU:0). Sometimes we want to place the operators created by gradients on the same devices as the corresponding original operators. For example, if the operator b (sin) in the last example is on GPU:0, we hope the corresponding gradient computation (cos) is also on GPU:0. This can be done by specifying colocate keyword arguments in gradients","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(assign_to_device(\"/device:GPU:0\")) begin\n    global a = Variable(1.0)\n    global b = sin(a)\nend\n\n@pywith tf.device(\"/CPU:0\") begin\n    global c = cos(b)\nend\n\ng = gradients(c, a, colocate=true)","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"In the following figure, we show the effects of colocate of the above codes. The test code snippet is","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"g = gradients(c, a, colocate=true)\nsess = Session(); init(sess)\nrun_profile(sess, g+c)\nsave_profile(\"true.json\")\n\ng = gradients(c, a, colocate=false)\nsess = Session(); init(sess)\nrun_profile(sess, g+c)\nsave_profile(\"false.json\")","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"(Image: )","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Batch Normalization Update Operators","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"If you use bn (batch normalization) on multi-GPUs, you must be careful to update the parameters in batch normalization on CPUs. This can be done by explicitly specify ","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(\"/cpu:0\") begin\n  global update_ops = get_collection(tf.GraphKeys.UPDATE_OPS)\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"and bind update_ops to an active operator (or explictly execute it in run(sess,...)).","category":"page"},{"location":"sparse/#Sparse-Linear-Algebra-1","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"ADCME augments TensorFlow APIs by adding sparse linear algebra support. In ADCME, sparse matrices are represented by SparseTensor. This data structure stores indices, rows and cols of the sparse matrices and keep track of relevant information such as whether it is diagonal for performance consideration. The default is row major (due to TensorFlow backend). ","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"When evaluating SparseTensor, the output will be SparseMatrixCSC, the native Julia representation of sparse matrices","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"A = run(sess, s) # A has type SparseMatrixCSC{Float64,Int64}","category":"page"},{"location":"sparse/#Sparse-Matrix-Construction-1","page":"Sparse Linear Algebra","title":"Sparse Matrix Construction","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"By passing columns (Int64), rows (Int64) and values (Float64) arrays","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"ii = [1;2;3;4]\njj = [1;2;3;4]\nvv = [1.0;1.0;1.0;1.0]\ns = SparseTensor(ii, jj, vv, 4, 4)","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"By passing a SparseMatrixCSC","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"using SparseArrays\ns = SparseTensor(sprand(10,10,0.3))","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"By passing a dense array (tensor or numerical array)","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"D = Array(sprand(10,10,0.3)) # a dense array\nd = constant(D)\ns = dense_to_sparse(d)","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"There are also special constructors. ","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Description Code\nDiagonal matrix with diagonal v spdiag(v)\nEmpty matrix with size m, n spzero(m, n)\nIdentity matrix with size m spdiag(m)","category":"page"},{"location":"sparse/#Matrix-Traits-1","page":"Sparse Linear Algebra","title":"Matrix Traits","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Size of the matrices\nsize(s) # (10,20)\nsize(s,1) # 10\nReturn row, col, val arrays (also known as COO arrays)\nii,jj,vv = find(s)","category":"page"},{"location":"sparse/#Arithmetic-Operations-1","page":"Sparse Linear Algebra","title":"Arithmetic Operations","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Add Subtract\ns = s1 + s2\ns = s1 - s2\n\nScalar Product\ns = 2.0 * s1\ns = s1 / 2.0\nSparse Product\ns = s1 * s2\nTransposition\ns = s1'","category":"page"},{"location":"sparse/#Sparse-Solvers-1","page":"Sparse Linear Algebra","title":"Sparse Solvers","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Solve a linear system (s is a square matrix)\nsol = s\\rhs\nSolve a least square system (s is a tall matrix)\nsol = s\\rhs","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"note: Note\nThe least square solvers are implemented using Eigen sparse linear packages, and the gradients are also implemented. Thus, the following codes will work as expected (the gradients functions will correctly compute the gradients):ii = [1;2;3;4]\njj = [1;2;3;4]\nvv = constant([1.0;1.0;1.0;1.0])\nrhs = constant(rand(4))\ns = SparseTensor(ii, jj, vv, 4, 4)\nsol = s\\rhs\nrun(sess, sol)\nrun(sess, gradients(sum(sol), rhs))\nrun(sess, gradients(sum(sol), vv))","category":"page"},{"location":"apps_constitutive_law/#Learning-Constitutive-Relations-from-Indirect-Observations-Using-Deep-Neural-Networks-1","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"","category":"section"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"Kailai Xu (co-first author), Huang, Daniel Z. (co-first author), Charbel Farhat, and Eric Darve. \"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks\"","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"Project Website","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"We present a new approach for predictive modeling and its uncertainty quantification for mechanical systems, where coarse-grained models such as constitutive relations are derived directly from observation data. We explore the use of neural networks to represent the unknowns functions (e.g., constitutive relations). Its counterparts, like piecewise linear functions and radial basis functions, are compared, and the strength of neural networks is explored. The training and predictive processes in this framework seamlessly combine the finite element method, automatic differentiation, and neural networks (or its counterparts). Under mild assumptions, we establish convergence guarantees. This framework also allows uncertainty quantification analysis in the form of intervals of confidence. Numerical examples on a multiscale fiber-reinforced plate problem and a nonlinear rubbery membrane problem from solid mechanics demonstrate the effectiveness of the framework.","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"The solid mechanics equation can be formulated as","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"mathcalP(u(mathbfx) mathcalM(u(mathbfx)dot u(mathbfx) mathbfx)) = mathcalF(u(mathbfx) mathbfx p)","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"where u is the displacement, mathbfx is the location, p is the external pressure, mathcalF is the external force, mathcalM(u(mathbfx)dot u(mathbfx) mathbfx) is the stress (mathcalM is also called the constitutive law), mathcalP(u(mathbfx) mathcalM(u(mathbfx)dot u(mathbfx) mathbfx)) is the internal force. For a new material or nonhomogeneous material, the constitutive relation mathcalM is not known and we want to estimate it. In laboratory, usually only u(mathbfx) can be measured but the stress cannot. The idea is to substitute the constitutive law relation–in this work, we assume mathcalM only depends on u(mathbfx) and the neural network is mathcalM_theta(u(mathbfx)), where theta is the unknown parameter. ","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"We train the neural network by solving the optimization problem","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"min_thetamathcalP(mathbfu mathcalM_theta(mathbfu)) - mathcalF(mathbfu mathbfx p) ^2_2","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"(Image: image-20191031200808697)","category":"page"},{"location":"apps_levy/#Calibrating-Multivariate-Lévy-Processes-with-Neural-Networks-1","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"","category":"section"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Kailai Xu and Eric Darve. \"Calibrating Multivariate Lévy Processes with Neural Networks\" ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Project Website","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Calibrating a Lévy process usually requires characterizing its jump distribution. Traditionally this problem can be solved with nonparametric estimation using the empirical characteristic functions (ECF), assuming certain regularity, and results to date are mostly in 1D. For multivariate Lévy processes and less smooth Lévy densities, the problem becomes challenging as ECFs decay slowly and have large uncertainty because of limited observations. We solve this problem by approximating the Lévy density with a parametrized functional form; the characteristic function is then estimated using numerical integration. In our benchmarks, we used deep neural networks and found that they are robust and can capture sharp transitions in the Lévy density. They perform favorably compared to piecewise linear functions and radial basis functions. The methods and techniques developed here apply to many other problems that involve nonparametric estimation of functions embedded in a system model.","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The Lévy process can be described by the Lévy-Khintchine formula","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"phi(xi) = mathbbEe^mathrmi langle xi mathbfX_t rangle =explefttleft( mathrmi langle mathbfb xi rangle - frac12langle xi mathbfAxirangle  +int_mathbbR^d left( e^mathrmi langle xi mathbfxrangle - 1 - mathrmi langle xi mathbfxrangle mathbf1_mathbfxleq 1right)nu(dmathbfx)right) right","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Here the multivariate Lévy process is described by three parameters: a positive semi-definite matrix mathbfA = SigmaSigma^T in mathbbR^dtimes d, where Sigmain mathbbR^dtimes d; a vector mathbfbin mathbbR^d; and a measure nuin mathbbR^dbackslashmathbf0. ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Given a sample path mathbfX_iDelta t, i=123ldots, we focus on estimating mathbfb, mathbfA and nu. In this work, we focus on the functional inverse problem–estimate nu–and assume mathbfb=0mathbfA=0. The idea is","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The Lévy density is approximated by a parametric functional form–-such as piecewise linear functions–-with parameters theta,","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"    nu(mathbfx) approx nu_theta(mathbfx)","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The characteristic function is approximated by numerical integration ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"phi(xi)approx    phi_theta(xi) = expleft Delta t sum_i=1^n_q left(e^mathrmi langlexi mathbfx_i rangle-1-mathrmilanglexi mathbfx_i ranglemathbf1_mathbfx_ileq 1  right)nu_theta(mathbfx_i) w_i right","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"where (mathbfx_i w_i)_i=1^n_q are quadrature nodes and weights.","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The empirical characteristic functions are computed given observations mathbfX_iDelta t_i=0^n","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"hatphi_n(xi) = frac1nsum_i=1^n exp(mathrmilangle xi mathbfX_iDelta t-mathbfX_(i-1)Delta trangle )  xi in mathbbR^d","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Solve the following optimization problem with a gradient based method. Here xi_i _i=1^m are collocation points depending on the data. ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"min_thetafrac1m sum_i=1^m hatphi_n(xi_i)-phi_theta(xi_i)  ^2","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"We show the schematic description of the method and some results on calibrating a discontinuous Lévy density function nu. ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"(Image: image-20191031200808697)","category":"page"},{"location":"api/#API-Reference-1","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Core-Functions-1","page":"API Reference","title":"Core Functions","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"ADCME.jl\", \"core.jl\", \"run.jl\"]","category":"page"},{"location":"api/#ADCME.add_collection-Tuple{String,PyObject}","page":"API Reference","title":"ADCME.add_collection","text":"add_collection(name::String, v::PyObject)\n\nAdds v to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.add_collection-Tuple{String,Vararg{PyObject,N} where N}","page":"API Reference","title":"ADCME.add_collection","text":"add_collection(name::String, vs::PyObject...)\n\nAdds operators vs to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.get_collection","page":"API Reference","title":"ADCME.get_collection","text":"get_collection(name::Union{String, Missing})\n\nReturns the collection with name name. If name is missing, returns all the trainable variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.has_gpu-Tuple{}","page":"API Reference","title":"ADCME.has_gpu","text":"has_gpu()\n\nChecks if GPU is available.\n\nnote: Note\nADCME will use GPU automatically if GPU is available. To disable GPU, set the environment variable ENV[\"CUDA_VISIBLE_DEVICES\"]=\"\" before importing ADCME \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.if_else-Tuple{Union{Bool, PyObject, Array},Any,Any,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.if_else","text":"if_else(condition::Union{PyObject,Array,Bool}, fn1, fn2, args...;kwargs...)\n\nIf condition is a scalar boolean, it outputs fn1 or fn2 (a function with no input argument or a tensor) based on whether condition is true or false.\nIf condition is a boolean array, if returns condition .* fn1 + (1 - condition) .* fn2\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.reset_default_graph-Tuple{}","page":"API Reference","title":"ADCME.reset_default_graph","text":"reset_default_graph()\n\nResets the graph by removing all the operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.stop_gradient-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.stop_gradient","text":"stop_gradient(o::PyObject, args...;kwargs...)\n\nDisconnects o from gradients backpropagation. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Tuple{String}","page":"API Reference","title":"ADCME.tensor","text":"tensor(s::String)\n\nReturns the tensor with name s. See tensorname.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensorname-Tuple{PyObject}","page":"API Reference","title":"ADCME.tensorname","text":"tensorname(o::PyObject)\n\nReturns the name of the tensor. See tensor.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.run_profile-Tuple","page":"API Reference","title":"ADCME.run_profile","text":"run_profile(args...;kwargs...)\n\nRuns the session with tracing information.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save_profile","page":"API Reference","title":"ADCME.save_profile","text":"save_profile(filename::String=\"default_timeline.json\")\n\nSave the timeline information to file filename. \n\nOpen Chrome and navigate to chrome://tracing\nLoad the timeline file\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.bind-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"Base.bind","text":"bind(op::PyObject, ops...)\n\nAdding operations ops to the dependencies of op. The function is useful when we want to execute ops but ops is not  in the dependency of the final output. For example, if we want to print i each time i is evaluated\n\ni = constant(1.0)\nop = tf.print(i)\ni = bind(i, op)\n\n\n\n\n\n","category":"method"},{"location":"api/#Variables-1","page":"API Reference","title":"Variables","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"variable.jl\"]","category":"page"},{"location":"api/#ADCME.TensorArray","page":"API Reference","title":"ADCME.TensorArray","text":"TensorArray(size_::Int64=0, args...;kwargs...)\n\nConstructs a tensor array for while_loop.  \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.Variable-Tuple{Any}","page":"API Reference","title":"ADCME.Variable","text":"Variable(initial_value;kwargs...)\n\nConstructs a ref tensor from value. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.cell-Tuple{Array,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.cell","text":"cell(arr::Array, args...;kwargs...)\n\nConstruct a cell tensor. \n\nExample\n\njulia> r = cell([[1.],[2.,3.]])\njulia> run(sess, r[1])\n1-element Array{Float32,1}:\n 1.0\njulia> run(sess, r[2])\n2-element Array{Float32,1}:\n 2.0\n 3.0\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.constant-Tuple{Any}","page":"API Reference","title":"ADCME.constant","text":"constant(value; kwargs...)\n\nConstructs a non-trainable tensor from value.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.convert_to_tensor-Union{Tuple{Union{Missing, Nothing, Number, PyObject, Array{T,N} where N}}, Tuple{T}} where T<:Number","page":"API Reference","title":"ADCME.convert_to_tensor","text":"convert_to_tensor(o::Union{PyObject, Number, Array{T}, Missing, Nothing}; dtype::Union{Type, Missing}=missing) where T<:Number\n\nConverts the input o to tensor. If o is already a tensor and dtype (if provided) is the same as that of o, the operator does nothing. Otherwise, convert_to_tensor converts the numerical array to a constant tensor or casts the data type.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.gradient_checkpointing","page":"API Reference","title":"ADCME.gradient_checkpointing","text":"gradient_checkpointing(type::String=\"speed\")\n\nUses checkpointing scheme for gradients. \n\n'speed':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,   so checkpointing them maximizes the running speed   (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n'memory': try to minimize the memory usage   (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n'collection': look for a tensorflow collection named 'checkpoints', which holds the tensors to checkpoint\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.gradients-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.gradients","text":"gradients(ys::PyObject, xs::PyObject; kwargs...)\n\nComputes the gradients of ys w.r.t xs. \n\nIf ys is a scalar, gradients returns the gradients with the same shape as xs.\nIf ys is a vector, gradients returns the Jacobian fracpartial ypartial x\n\nnote: Note\nThe second usage is not suggested since ADCME adopts reverse mode automatic differentiation.  Although in the case ys is a vector and xs is a scalar, gradients cleverly uses forward mode automatic differentiation, it requires that the second order gradients are implemented for relevant operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.hessian-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.hessian","text":"hessian computes the hessian of a scalar function f with respect to vector inputs xs\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Union{Tuple{Array{T,1}}, Tuple{T}} where T","page":"API Reference","title":"ADCME.tensor","text":"tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Union{Tuple{Array{T,2}}, Tuple{T}} where T","page":"API Reference","title":"ADCME.tensor","text":"tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T\n\nConvert a generic array v to a tensor. For example, \n\nv = [0.0 constant(1.0) 2.0\n    constant(2.0) 0.0 1.0]\nu = tensor(v)\n\nu will be a 2times 3 tensor. \n\nnote: Note\nThis function is expensive. Use with caution.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.read-Tuple{PyObject,Union{Integer, PyObject}}","page":"API Reference","title":"Base.read","text":"read(ta::PyObject, i::Union{PyObject,Integer})\n\nReads data from TensorArray at index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.write-Tuple{PyObject,Union{Integer, PyObject},PyObject}","page":"API Reference","title":"Base.write","text":"write(ta::PyObject, i::Union{PyObject,Integer}, obj)\n\nWrites data obj to TensorArray at index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#Random-Variables-1","page":"API Reference","title":"Random Variables","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"random.jl\"]","category":"page"},{"location":"api/#ADCME.categorical-Tuple{Union{Integer, PyObject}}","page":"API Reference","title":"ADCME.categorical","text":"categorical(n::Union{PyObject, Integer}; kwargs...)\n\nkwargs has a keyword argument logits, a 2-D Tensor with shape [batch_size, num_classes].   Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.choice-Tuple{Union{PyObject, Array},Union{Integer, PyObject}}","page":"API Reference","title":"ADCME.choice","text":"choice(inputs::Union{PyObject, Array}, n_samples::Union{PyObject, Integer};replace::Bool=false)\n\nChoose n_samples samples from inputs with/without replacement. \n\n\n\n\n\n","category":"method"},{"location":"api/#Sparse-Matrix-1","page":"API Reference","title":"Sparse Matrix","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"sparse.jl\"]","category":"page"},{"location":"api/#ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}","page":"API Reference","title":"ADCME.SparseTensor","text":"SparseTensor(A::SparseMatrixCSC)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseTensor-Union{Tuple{S}, Tuple{T}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S},Union{Nothing, PyObject, S}}} where S<:Integer where T<:Integer","page":"API Reference","title":"ADCME.SparseTensor","text":"SparseTensor(I::Union{PyObject,Array{T,1}}, J::Union{PyObject,Array{T,1}}, V::Union{Array{Float64,1}, PyObject}, m::Union{S, PyObject, Nothing}=nothing, n::Union{S, PyObject, Nothing}=nothing) where {T<:Integer, S<:Integer}\n\nConstructs a sparse tensor.  Examples:\n\nii = [1;2;3;4]\njj = [1;2;3;4]\nvv = [1.0;1.0;1.0;1.0]\ns = SparseTensor(ii, jj, vv, 4, 4)\ns = SparseTensor(sprand(10,10,0.3))\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseAssembler-Tuple{}","page":"API Reference","title":"ADCME.SparseAssembler","text":"accumulator, creater, initializer = SparseAssembler()\n\nReturns 3 functions that can be used for assembling sparse matrices concurrently.\n\ninitializer must be called before the working session\naccumulator accumulates column indices and values \ncreator accepts no input and outputs row indices, column indices and values for the sparse matrix\n\nExample\n\naccumulator, creater, initializer = SparseAssembler()\ninitializer(5)\nop1 = accumulator(1, [1;2;3], ones(3))\nop2 = accumulator(1, [3], [1.])\nop3 = accumulator(2, [1;3], ones(2))\nrun(sess, [op1,op2,op3])\nii,jj,vv = creater()\ni,j,v = run(sess, [ii,jj,vv])\nA = sparse(i,j,v,5,5)\n@assert Array(A)≈[1.0  1.0  2.0  0.0  0.0\n                1.0  0.0  1.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0]\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.find-Tuple{SparseTensor}","page":"API Reference","title":"ADCME.find","text":"find(s::SparseTensor)\n\nReturns the row, column and values for sparse tensor s.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{Int64}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(n::Int64)\n\nConstructs a sparse identity matrix of size ntimes n.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{PyObject}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(o::PyObject)\n\nConstructs a sparse diagonal matrix where the diagonal entries are o\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spzero","page":"API Reference","title":"ADCME.spzero","text":"spzero(m::Int64, n::Union{Missing, Int64}=missing)\n\nConstructs a empty sparse matrix of size mtimes n. n=m if n is missing\n\n\n\n\n\n","category":"function"},{"location":"api/#Operations-1","page":"API Reference","title":"Operations","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"ops.jl\"]","category":"page"},{"location":"api/#ADCME.pmap-Tuple{Function,Union{PyObject, Array{PyObject,N} where N}}","page":"API Reference","title":"ADCME.pmap","text":"pmap(fn::Function, o::Union{Array{PyObject}, PyObject})\n\nParallel for loop. There should be no data dependency between different iterations.\n\nExample\n\nx = constant(ones(10))\ny1 = pmap(x->2.0*x, x)\ny2 = pmap(x->x[1]+x[2], [x,x])\ny3 = pmap(1:10, x) do z\n    i = z[1]\n    xi = z[2]\n    xi + cast(Float64, i)\nend\nrun(sess, y1)\nrun(sess, y2)\nrun(sess, y3)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.vector-Union{Tuple{T}, Tuple{Union{PyObject, StepRange, UnitRange, Array{T,N} where N},Union{PyObject, Array{Float64,N} where N},Union{Int64, PyObject}}} where T<:Integer","page":"API Reference","title":"ADCME.vector","text":"vector(i::Union{Array{T}, PyObject, UnitRange, StepRange}, v::Union{Array{Float64},PyObject},s::Union{Int64,PyObject})\n\nReturns a vector V with length s such that\n\nV[i] = v\n\n\n\n\n\n","category":"method"},{"location":"api/#LinearAlgebra.svd-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"LinearAlgebra.svd","text":"svd(o::PyObject, args...; kwargs...)\n\nReturns a TFSVD structure which holds the following data structures\n\nS::PyObject\nU::PyObject\nV::PyObject\nVt::PyObject\n\nWe have the equality o = USV\n\n\n\n\n\n","category":"method"},{"location":"api/#IO-1","page":"API Reference","title":"IO","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"io.jl\"]","category":"page"},{"location":"api/#ADCME.Diary","page":"API Reference","title":"ADCME.Diary","text":"Diary(suffix::Union{String, Nothing}=nothing)\n\nCreates a diary at a temporary directory path. It returns a writer and the corresponding directory path\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.activate","page":"API Reference","title":"ADCME.activate","text":"activate(sw::Diary, port::Int64=6006)\n\nRunning Diary at http://localhost:port.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load","page":"API Reference","title":"ADCME.load","text":"load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nLoads the values of variables to the session sess from the file file. If vars is nothing, it loads values to all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load-Tuple{Diary,String}","page":"API Reference","title":"ADCME.load","text":"load(sw::Diary, dirp::String)\n\nLoads Diary from dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.pload-Tuple{String}","page":"API Reference","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.psave-Tuple{PyObject,String}","page":"API Reference","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save","page":"API Reference","title":"ADCME.save","text":"save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nSaves the values of vars in the session sess. The result is written into file as a dictionary. If vars is nothing, it saves all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.save-Tuple{Diary,String}","page":"API Reference","title":"ADCME.save","text":"save(sw::Diary, dirp::String)\n\nSaves Diary to dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.scalar","page":"API Reference","title":"ADCME.scalar","text":"scalar(o::PyObject, name::String)\n\nReturns a scalar summary object.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.write-Tuple{Diary,Int64,Union{String, Array{String,N} where N}}","page":"API Reference","title":"Base.write","text":"write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})\n\nWrites to Diary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Optimization-1","page":"API Reference","title":"Optimization","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"optim.jl\"]","category":"page"},{"location":"api/#ADCME.BFGS!","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, max_iter::Int64=15000; \nvars::Array{PyObject}=PyObject[], callback::Union{Function, Nothing}=nothing, kwargs...)\n\nBFGS! is a simplified interface for BFGS optimizer. See also ScipyOptimizerInterface. callback is a callback function with signature \n\ncallback(vs::Array{Float64}, iter::Int64, loss::Float64)\n\nvars is an array consisting of tensors and its values will be the input to vs.\n\nexample\n\na = Variable(1.0)\nloss = (a - 10.0)^2\nBFGS!(sess, loss)\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(value_and_gradients_function::Function, initial_position::Union{PyObject, Array{Float64}}, max_iter::Int64=50, args...;kwargs...)\n\nApplies the BFGS optimizer to value_and_gradients_function\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!-Union{Tuple{T}, Tuple{PyObject,PyObject,Union{Nothing, PyObject, Array{T,N} where N},Union{PyObject, Array{PyObject,N} where N}}} where T<:Union{Nothing, PyObject}","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, grads::Union{Array{T},Nothing,PyObject}, \n    vars::Union{Array{PyObject},PyObject}; kwargs...) where T<:Union{Nothing, PyObject}\n\nRunning BFGS algorithm min_textttvars textttloss(textttvars) The gradients grads must be provided. Typically, grads[i] = gradients(loss, vars[i]).  grads[i] can exist on different devices (GPU or CPU). \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.CustomOptimizer-Tuple{Function}","page":"API Reference","title":"ADCME.CustomOptimizer","text":"CustomOptimizer(opt::Function, name::String)\n\ncreates a custom optimizer with struct name name. For example, we can integrate Optim.jl with ADCME by  constructing a new optimizer\n\nCustomOptimizer(\"Con\") do f, df, c, dc, x0, nineq, neq, x_L, x_U\n    opt = Opt(:LD_MMA, length(x0))\n    bd = zeros(length(x0)); bd[end-1:end] = [-Inf, 0.0]\n    opt.lower_bounds = bd\n    opt.xtol_rel = 1e-4\n    opt.min_objective = (x,g)->(g[:]= df(x); return f(x)[1])\n    inequality_constraint!(opt, (x,g)->( g[:]= dc(x);c(x)[1]), 1e-8)\n    (minf,minx,ret) = NLopt.optimize(opt, x0)\n    minx\nend\n\nThen we can create an optimizer with \n\nopt = Con(loss, inequalities=[c1], equalities=[c2])\n\nTo trigger the optimization, use\n\nopt.minimize(sess)\n\nor \n\nminimize(opt, sess)\n\nNote thanks to the global variable scope of Julia, step_callback, optimizer_kwargs can actually  be passed from Julia environment directly.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyObject},Union{PyObject, Array{Float64,N} where N}}} where T<:Real","page":"API Reference","title":"ADCME.NonlinearConstrainedProblem","text":"NonlinearConstrainedProblem(f::Function, L::Function, θ::PyObject, u0::Union{PyObject, Array{Float64}}; options::Union{Dict{String, T}, Missing}=missing) where T<:Integer\n\nComputes the gradients fracpartial Lpartial theta\n\nmin  L(u) quad mathrmst  F(theta u) = 0\n\nu0 is the initial guess for the numerical solution u, see newton_raphson.\n\nCaveats: Assume r, A = f(θ, u) and θ are the unknown parameters, gradients(r, θ) must be defined (backprop works properly)\n\nReturns: It returns a tuple (L: loss, C: constraints, and Graidents)\n\nleft(L(u) u fracpartial Lpartial θright)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerInterface-Tuple{Any}","page":"API Reference","title":"ADCME.ScipyOptimizerInterface","text":"ScipyOptimizerInterface(loss; method=\"L-BFGS-B\", options=Dict(\"maxiter\"=> 15000, \"ftol\"=>1e-12, \"gtol\"=>1e-12), kwargs...)\n\nA simple interface for Scipy Optimizer. See also ScipyOptimizerMinimize and BFGS!.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerMinimize-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.ScipyOptimizerMinimize","text":"ScipyOptimizerMinimize(sess::PyObject, opt::PyObject; kwargs...)\n\nMinimizes a scalar Tensor. Variables subject to optimization are updated in-place at the end of optimization.\n\nNote that this method does not just return a minimization Op, unlike minimize; instead it actually performs minimization by executing commands to control a Session https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface. See also ScipyOptimizerInterface and BFGS!.\n\nfeed_dict: A feed dict to be passed to calls to session.run.\nfetches: A list of Tensors to fetch and supply to loss_callback as positional arguments.\nstep_callback: A function to be called at each optimization step; arguments are the current values of all optimization variables flattened into a single vector.\nloss_callback: A function to be called every time the loss and gradients are computed, with evaluated fetches supplied as positional arguments.\nrun_kwargs: kwargs to pass to session.run.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.newton_raphson-Union{Tuple{T}, Tuple{Function,Union{PyObject, Array}}, Tuple{Function,Union{PyObject, Array},Union{Missing, PyObject, Array{#s110,N} where N where #s110<:Real}}} where T<:Real","page":"API Reference","title":"ADCME.newton_raphson","text":"newton_raphson(f::Function, u::Union{Array,PyObject}, θ::Union{Missing,PyObject}; options::Union{Dict{String, T}, Missing}=missing)\n\nNewton Raphson solver for solving a nonlinear equation.  f has the signature \n\nf(θ::Union{Missing,PyObject}, u::PyObject)->(r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is off)\nf(θ::Union{Missing,PyObject}, u::PyObject)->(fval::PyObject, r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is on)\n\nwhere r is the residual and A is the Jacobian matrix; in the case where linesearch is on, the function value fval must also be supplied. θ are external parameters. u0 is the initial guess for u options:\n\nmax_iter: maximum number of iterations (default=100)\nverbose: whether details are printed (default=false)\nrtol: relative tolerance for termination (default=1e-12)\ntol: absolute tolerance for termination (default=1e-12)\nLM: a float number, Levenberg-Marquardt modification x^k+1 = x^k - (J^k + mu^k)^-1g^k (default=0.0)\nlinesearch: whether linesearch is used (default=false)\n\nCurrently, the backtracing algorithm is implemented. The parameters for linesearch are also supplied via options\n\nls_c1: stop criterion, f(x^k)  f(0) + alpha c_1  f(0)\nls_ρ_hi: the new step size alpha_1leq rho_hialpha_0 \nls_ρ_lo: the new step size alpha_1geq rho_loalpha_0 \nls_iterations: maximum number of iterations for linesearch\nls_maxstep: maximum allowable steps\nls_αinitial: initial guess for the step size alpha\n\n\n\n\n\n","category":"method"},{"location":"api/#Neural-Networks-1","page":"API Reference","title":"Neural Networks","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"layers.jl\"]","category":"page"},{"location":"api/#ADCME.ae","page":"API Reference","title":"ADCME.ae","text":"ae(x::PyObject, output_dims::Array{Int64}, scope::String = \"default\")\n\nCreates a neural network with intermediate numbers of neurons output_dims.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.ae-Tuple{Union{PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{PyObject, Array{Float64,N} where N}}","page":"API Reference","title":"ADCME.ae","text":"ae(x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64}, θ::Union{Array{Float64}, PyObject})\n\nCreates a neural network with intermediate numbers of neurons output_dims. The weights are given by θ\n\nExample 1: Explicitly construct weights and biases\n\nx = constant(rand(10,2))\nn = ae_num([2,20,20,20,2])\nθ = Variable(randn(n)*0.001)\ny = ae(x, [20,20,20,2], θ)\n\nExample 2: Implicitly construct weights and biases\n\nθ = ae_init([10,20,20,20,2]) \nx = constant(rand(10,10))\ny = ae(x, [20,20,20,2], θ)\n\nSee also ae_num, ae_init.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_init-Tuple{Array{Int64,N} where N}","page":"API Reference","title":"ADCME.ae_init","text":"ae_init(output_dims::Array{Int64}; T::Type=Float64, method::String=\"xavier\")\n\nReturn the initial weights and bias values by TensorFlow as a vector. Three types of  random initializers are provided\n\nxavier (default). It is useful for tanh fully connected neural network. \n\nW^l_i sim sqrtfrac1n_l-1\n\nxavier_avg. A variant of xavier\n\nW^l_i sim sqrtfrac2n_l + n_l-1\n\nhe. This is the activation aware initialization of weights and helps mitigate the problem\n\nof vanishing/exploding gradients. \n\nW^l_i sim sqrtfrac2n_l-1\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_num-Tuple{Array{Int64,N} where N}","page":"API Reference","title":"ADCME.ae_num","text":"ae_num(output_dims::Array{Int64})\n\nEstimates the number of weights and biases for the neural network. Note the first dimension should be the feature dimension (this is different from ae since in ae the feature dimension can be inferred), and the last dimension should be the output dimension. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_to_code-Tuple{String,String}","page":"API Reference","title":"ADCME.ae_to_code","text":"ae_to_code(file::String, scope::String)\n\nReturn the code string from the feed-forward neural network data in file. Usually we can immediately evaluate  the code string into Julia session by \n\neval(Meta.parse(s))\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.bn-Tuple","page":"API Reference","title":"ADCME.bn","text":"bn(args...;center = true, scale=true, kwargs...)\n\nbn accepts a keyword parameter is_training. \n\nExample\n\nbn(inputs, name=\"batch_norm\", is_training=true)\n\nnote: Note\nbn should be used with control_dependencyupdate_ops = get_collection(UPDATE_OPS)\ncontrol_dependencies(update_ops) do \n    global train_step = AdamOptimizer().minimize(loss)\nend \n\n\n\n\n\n","category":"method"},{"location":"api/#Generative-Neural-Nets-1","page":"API Reference","title":"Generative Neural Nets","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"gan.jl\"]","category":"page"},{"location":"api/#ADCME.GAN","page":"API Reference","title":"ADCME.GAN","text":"GAN(dat::PyObject, \n    generator::Function, \n    gan::GAN,\n    loss::Union{String, Function, Missing}=missing; \n    latent_dim::Union{Missing, Int64}=missing, \n    batch_size::Union{Missing, Int64}=missing)\n\nCreates a GAN instance. \n\ndat in mathbbR^ntimes d is the training data for the GAN, where n is the number of training data, and d is the dimension per training data.\ngeneratormathbbR^d rightarrow mathbbR^d is the generator function, d is the hidden dimension.\ndiscriminatormathbbR^d rightarrow mathbbR is the discriminator function. \nloss is the loss function. See klgan, rklgan, wgan, lsgan for examples.\nlatent_dim (default=d) is the latent dimension.\nbatch_size (default=32) is the batch size in training.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.jsgan-Tuple{GAN}","page":"API Reference","title":"ADCME.jsgan","text":"jsgan(gan::GAN)\n\nComputes the vanilla GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.klgan-Tuple{GAN}","page":"API Reference","title":"ADCME.klgan","text":"klgan(gan::GAN)\n\nComputes the KL-divergence GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.lsgan-Tuple{GAN}","page":"API Reference","title":"ADCME.lsgan","text":"lsgan(gan::GAN)\n\nComputes the least square GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.predict-Tuple{GAN,Union{PyObject, Array}}","page":"API Reference","title":"ADCME.predict","text":"predict(gan::GAN, input::Union{PyObject, Array})\n\nPredicts the GAN gan output given input input. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.rklgan-Tuple{GAN}","page":"API Reference","title":"ADCME.rklgan","text":"rklgan(gan::GAN)\n\nComputes the reverse KL-divergence GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.sample-Tuple{GAN,Int64}","page":"API Reference","title":"ADCME.sample","text":"sample(gan::GAN, n::Int64)\n\nSamples n instances from gan.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.wgan-Tuple{GAN}","page":"API Reference","title":"ADCME.wgan","text":"wgan(gan::GAN)\n\nComputes the Wasserstein GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.build!-Tuple{GAN}","page":"API Reference","title":"ADCME.build!","text":"build!(gan::GAN)\n\nBuilds the GAN instances. This function returns gan for convenience.\n\n\n\n\n\n","category":"method"},{"location":"api/#Tools-1","page":"API Reference","title":"Tools","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"extra.jl\"]","category":"page"},{"location":"api/#ADCME.compile_op-Tuple{String}","page":"API Reference","title":"ADCME.compile_op","text":"compile_op(oplibpath::String; check::Bool=false)\n\nCompile the library operator by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.customop-Tuple{}","page":"API Reference","title":"ADCME.customop","text":"customop()\n\nCreate a new custom operator.\n\nexample\n\njulia> customop() # create an editable `customop.txt` file\n[ Info: Edit custom_op.txt for custom operators\njulia> customop() # after editing `customop.txt`, call it again to generate interface files.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.install-Tuple{String}","page":"API Reference","title":"ADCME.install","text":"install(s::String; force::Bool = false)\n\nInstall a custom operator via URL. s can be\n\nA URL. ADCME will download the directory through git\nA string. ADCME will search for the associated package on https://github.com/ADCMEMarket\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op-Tuple{String,String}","page":"API Reference","title":"ADCME.load_op","text":"load_op(oplibpath::String, opname::String)\n\nLoads the operator opname from library oplibpath.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op_and_grad-Tuple{String,String}","page":"API Reference","title":"ADCME.load_op_and_grad","text":"load_op_and_grad(oplibpath::String, opname::String; multiple::Bool=false)\n\nLoads the operator opname from library oplibpath; gradients are also imported.  If multiple is true, the operator is assumed to have multiple outputs. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_system_op","page":"API Reference","title":"ADCME.load_system_op","text":"load_system_op(s::String, oplib::String, grad::Bool=true)\n\nLoads custom operator from CustomOps directory (shipped with ADCME instead of TensorFlow) For example \n\ns = \"SparseOperator\"\noplib = \"libSO\"\ngrad = true\n\nthis will direct Julia to find library CustomOps/SparseOperator/libSO.dylib on MACOSX\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.test_jacobian-Tuple{Function,Array{Float64,N} where N}","page":"API Reference","title":"ADCME.test_jacobian","text":"test_jacobian(f::Function, x0::Array{Float64}; scale::Float64 = 1.0)\n\nTesting the gradients of a vector function f: y, J = f(x) where y is a vector output and J is the Jacobian.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.xavier_init","page":"API Reference","title":"ADCME.xavier_init","text":"xavier_init(size, dtype=Float64)\n\nReturns a matrix of size size and its values are from Xavier initialization. \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.compile-Tuple{String}","page":"API Reference","title":"ADCME.compile","text":"compile(s::String)\n\nCompiles the library s by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#Datasets-1","page":"API Reference","title":"Datasets","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"dataset.jl\"]","category":"page"},{"location":"while_loop/#While-Loops-1","page":"While Loops","title":"While Loops","text":"","category":"section"},{"location":"while_loop/#Motivation-1","page":"While Loops","title":"Motivation","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"In engineering, we usually need to do for loops, e.g., time stepping, finite element matrix assembling, etc. In pseudocode, we have","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"for i = 1:1000000\n  global x\n\tx = do_some_simulation(x)\nend","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"To do automatic differentiation in ADCME, direct implemnetation in the above way incurs creation of 1000000 subgraphs, which requires large memories and long dependency parsing time. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"TensorFlow provides us a clever way to do loops, where only one graph is created for the whole loops. The basic idea is to create a while_loop graph based on five primitives, and the corresponding graph for backpropagation is constructed thereafter. ","category":"page"},{"location":"while_loop/#A-Basic-Example-1","page":"While Loops","title":"A Basic Example","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"As a simple example, we consider assemble the external load vector for linear finite elements in 1D. Assume that the load distribution is f(x)=1-x^2, xin01. The goal is to compute a vector mathbfv with v_i=int_0^1 f(x)phi_i(x)dx, where phi_i(x) is the i-th linear element. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The pseudocode for this problem is shown in the following","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"F = zeros(ne+1) // ne is the total number of elements\nfor e = 1:ne\n\tadd load contribution to F[e] and F[e+1]\nend","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"(Image: )","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"However, if ne is very large, writing explicit loops is unwise since it will create ne subgraphs. while_loop can be very helpful in this case (the script can also be found in https://github.com/kailaix/ADCME.jl/tree/master/examples/whileloop/whileloop_simple.jl)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"using ADCME\n\nne = 100\nh = 1/ne\nf = x->1-x^2\nfunction cond0(i, F_arr)\n    i<=ne+1\nend\nfunction body(i, F_arr)\n    fmid = f(cast(i-2, Float64)*h+h/2)\n    F = vector([i-1;i], [fmid*h/2;fmid*h/2], ne+1)\n    F_arr = write(F_arr, i, F)\n    i+1, F_arr\nend\n\nF_arr = TensorArray(ne+1)\nF_arr = write(F_arr, 1, constant(zeros(ne+1))) # inform `F_arr` of the data type by writing at index 1\ni = constant(2, dtype=Int32)\n_, out = while_loop(cond0, body, [i,F_arr]; parallel_iterations=10)\nF = sum(stack(out), dims=1)\nsess = Session(); init(sess)\nF0 = run(sess, F)","category":"page"},{"location":"while_loop/#Finite-Element-Analysis-1","page":"While Loops","title":"Finite Element Analysis","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"In this section, we demonstrate how to assemble a finite element matrix based on while_loop for a 2D Poisson problem. We consider the following problem","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"beginaligned\nnabla cdot ( Dnabla u(mathbfx) ) = f(mathbfx) mathbfxin Omega\nu(mathbfx) = 0  mathbfxin partial Omega\nendaligned","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Here Omega is the unit disk. We consider a simple case, where","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"beginaligned\nD=mathbfI\nf(mathbfx)=-4\nendaligned","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Then the exact solution will be ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"u(mathbfx) = 1-x^2-y^2","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The weak formulation is","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"langle nabla v(mathbfx) Dnabla u(mathbfx) rangle = langle f(mathbfx)v(mathbfx) rangle","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We  split Omega into triangles mathcalT and use piecewise linear basis functions. Typically, we would iterate over all elements and compute the local stiffness matrix for each element. However, this could result in a large loop if we use a fine mesh. Instead, we can use while_loop to complete the task. In ADCME, the syntax for while_loop is ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"while_loop(condition, body, loop_vars)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"here condition and body take loop_vars as inputs. The former outputs a bool tensor indicating whether to terminate the loop while the latter outputs the updated loop_vars. TensorArry is used to store variables that change during the loops. The codes for assembling FEM is","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"function assemble_FEM(Ds, Fs, nodes, elem)\n    NT = size(elem,1)\n    cond0 = (i,tai,taj,tav, tak, taf) -> i<=NT\n    elem = constant(elem)\n    nodes = constant(nodes)\n    function body(i, tai, taj, tav, tak, taf)\n        el = elem[i]\n        x1, y1 = nodes[el[1]][1], nodes[el[1]][2]\n        x2, y2 = nodes[el[2]][1], nodes[el[2]][2]\n        x3, y3 = nodes[el[3]][1], nodes[el[3]][2]\n        T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)\n        D = Ds[i]; F = Fs[i]*T/3\n        v = T*stack([D*((-x2 + x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y2 - y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2)])\n        tav = write(tav, i, v)\n        ii = vec([elem[i] elem[i] elem[i]]')\n        jj = [elem[i]; elem[i]; elem[i]]\n        tai = write(tai, i, ii)\n        taj = write(taj, i, jj)\n        tak = write(tak, i, elem[i])\n        taf = write(taf, i, stack([F,F,F]))\n        return i+1, tai, taj, tav, tak, taf\n    end\n    tai = TensorArray(NT, dtype=Int32)\n    taj = TensorArray(NT, dtype=Int32)\n    tak = TensorArray(NT, dtype=Int32)\n    tav = TensorArray(NT)\n    taf = TensorArray(NT)\n    i = constant(1, dtype=Int32)\n    i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)\n    _, tai, taj, tav, tak, taf = while_loop(cond0, body, [i, tai, taj, tav, tak, taf]; parallel_iterations=10)\n    vec(stack(tai)[1:NT]'), vec(stack(taj)[1:NT]'), vec(stack(tav)[1:NT]'),\n                        vec(stack(tak)[1:NT]'), vec(stack(taf)[1:NT]')\nend","category":"page"},{"location":"while_loop/#Explanation-1","page":"While Loops","title":"Explanation","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We now explain the codes. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We assume that nodes is a n_vtimes 2 tensor holding all n_v coordinates of the nodes, elem is a n_etimes 3  tensor holding all n_e triangle vertex index triples. We create five TensorArray to hold the row indices, column indices and values for the stiffness matrix, and row indices and values for the right hand side (Here NT denotes n_e):","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"tai = TensorArray(NT, dtype=Int32)\ntaj = TensorArray(NT, dtype=Int32)\ntak = TensorArray(NT, dtype=Int32)\ntav = TensorArray(NT)\ntaf = TensorArray(NT)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Within each loop (body), we extract the coordinates of each vertex coordinate","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"el = elem[i]\nx1, y1 = nodes[el[1]][1], nodes[el[1]][2]\nx2, y2 = nodes[el[2]][1], nodes[el[2]][2]\nx3, y3 = nodes[el[3]][1], nodes[el[3]][2]","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"and compute the area of ith triangle","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The local stiffness matrix is computed and vectorized (v). It is computed symbolically.  To store the computed value into TensorArray, we call the write API (there is also read API, which reads a value from TensorArray)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"tav = write(tav, i, v)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Note we have called ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"before we call while_loop. This is because we need to initialize the TensorArrays (i.e., telling them the size and type of elements in the arrays). We must guarantee that the sizes and types of the elements in the arrays are consistent in while_loop. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Finally, we stack the TensorArray into a tensor and vectorized it according to the row major. This serves as the output of assemble_FEM. The complete script for solving this problem is here and the following plot shows the numerical result and corresponding reference solution. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"(Image: Result for the Poisson Problem)","category":"page"},{"location":"while_loop/#Gradients-through-while_loop-1","page":"While Loops","title":"Gradients through while_loop","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"To inspect the gradients through the loops, we can run ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"println(run(sess, gradients(sum(u), Ds))) # a sparse tensor","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"This outputs a sparse tensor instead of a full tensor. To obtain the full tensor, we could call tf.convert_to_tensor","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"println(run(sess, tf.convert_to_tensor(gradients(sum(u), Ds)))) # full tensor","category":"page"},{"location":"inverse_modeling/#Inverse-Modeling-1","page":"Inverse Modeling","title":"Inverse Modeling","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Inverse modeling (IM) identifies a certain set of parameters or functions with which the outputs of the forward analysis matches the desired result or measurement. IM can usually be solved by formulating it as an optimization problem. But the major difference is that IM aims at getting information not accessible to forward analysis, instead of obtaining an optimal value of a fixed objective function and set of constraints. In IM, the objective function and constraints can be adjusted, and prior information of the unknown parameters or functions can be imposed in the form of regularizers, to better reflect the physical laws. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"For example, given an image xinmathbbR^1024times 1024, the forward analysis is given by y = F(x) = sum_ij x_ij, i.e., the summation of all pixel values. One possible IM problem requires you to estimate x given the measurement y. It can be formulated an optimization problem min_x (F(x)-y)^2, which is underdetermined. However, if we have the prior that the image is a pure color image, then the inverse problem is well-defined and has a unique solution. There are many ways to impose this prior as contraints to the optimization problem, but the IM problem itself may not be described as an optimization problem. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: )","category":"page"},{"location":"inverse_modeling/#Automatic-Differentiation-1","page":"Inverse Modeling","title":"Automatic Differentiation","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"One powerful tool in inverse modeling is automatic differentiation (AD). Automatic differentiation is a general way to compute gradients based on the chain rule. By tracing the forward-pass computation, the gradient at the final step can propagate back to every operator and every parameter in a computational graph. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"As an example, a neural network model mainly consists of a sequence of linear transforms and non-linear activation functions. The goal of the training process is to minimize the error between its prediction and the label of ground truth. Automatic differentiation is used to calculate the gradients of every variable by back-propagating the gradients from the loss function to the trainable parameters, i.e., the weights and biases of neural networks. The gradients are then used in a gradient-based optimizer such as gradient descent methods to update the parameters. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"For another example, the physical forward simulation is similar to the neural network model in that they are both sequences of linear/non-linear transforms. One popular method in physical simulation, the FDTD (Finite-Difference Time-Domain) method, applies a finite difference operator to a consecutive time steps to solve time-dependent partial differential equations (PDEs). In seismic problems, we can specify parameters such as earthquake source functions and earth media properties to simulate the received seismic signals. In seismic inversion problems, those parameters are unknown and we can invert the underlining source characteristic and media property by minimizing the difference between the simulated seismic signals and the observed ones. In the framework of automatic differentiation, the gradients of the difference can be computed automatically and thus used in a gradient-based optimizer. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: )","category":"page"},{"location":"inverse_modeling/#AD-Implementation-in-ADCME-1","page":"Inverse Modeling","title":"AD Implementation in ADCME","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"ADCME uses TensorFlow as the backend for automatic differentiation. However, one major difference of ADCME compared with TensorFlow is that it provides a friendly syntax for scientific computing (essentially the same syntax as native Julia). This substantially reduces development time. In addition, ADCME augments TensorFlow libraries by adding missing features that are useful for scientific computing, such as sparse matrix solve, sparse least square, sparse assembling, etc. Additionally, Julia interfaces make it possible for directly implementing efficient numerical computation parts of the simulation (requires no automatic differentiation), for interacting with other languages (MATLAB, C/C++, R, etc.) and for built-in Julia parallelism. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"As an example, we show how a convoluted acoustic wave equation simulation with PML boundary condition can be translated to Julia codes with AD feature very neatly. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: )","category":"page"},{"location":"inverse_modeling/#Forward-Operator-Types-1","page":"Inverse Modeling","title":"Forward Operator Types","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: Operators)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called input, x, and turns out a parameter called ouput, y=f(x). For reverse mode automatic differentiation, besides evaluating f(x), we need also to compute fracpartial Jpartial x given fracpartial Jpartial y where J is a functional of y. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Note  the operator y=f(x) may be implicit in the sense that f is not given directly. In general, we can write the relationship between x and y as F(xy)=0. The operator is well-defined if for given x, there exists one and only one y such that F(xy)=0. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"For automatic differentiation, besides the well-definedness of F, we also require that we can compute fracpartial Jpartial x given fracpartial Jpartial y. It is easy to see that","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = -fracpartial Jpartial yF_y^-1F_x","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Therefore, we call an operator F is well-posed if F_y^-1 exists. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"All operators can be classified into four types based on the linearity and explicitness.","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Linear and explicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"This type of operators has the form ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"y = Ax","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"where A is a matrix. In this case, ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"F(xy) = Ax-y","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"and therefore ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = fracpartial Jpartial yA","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In Tensorflow, such an operator can be implemented as (assuming A is )","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"import tensorflow as tf\n@tf.custom_gradient\ndef F(x):\n​      u = tf.linalg.matvec(A, x)\n​      def grad(dy):\n​          return tf.linalg.matvec(tf.transpose(A), dy)\n​      return u, grad","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Nonlinear and explicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this case, we have ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"y = F(x)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"where F is explicitly given. We have","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"F(xy) = F(x)-yRightarrow fracpartial Jpartial x = fracpartial Jpartial y F_x(x)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"One challenge here is we need to implement the matrix vector production fracpartial Jpartial y F_x(x) for grad. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Linear and implicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this case ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Ay = x","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"We have F(xy) = x-Ay and ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = fracpartial Jpartial yA^-1","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Nonlinear and implicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this case F(xy)=0 and the corresponding gradient is ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = -fracpartial Jpartial yF_y^-1F_x","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"This case is the most challenging of the four but widely seen in scientific computing code. In many numerical simulation code, F_y is usually sparse and therefore it is rewarding to exploit the sparse structure for computation acceleration in practice.","category":"page"},{"location":"newton_raphson/#Newton-Raphson-1","page":"Newton Raphson","title":"Newton Raphson","text":"","category":"section"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"Newton-Raphson algorithm is widely used in scientific computing. In ADCME, the function for the algorithm is newton_raphson. And the signature is","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"newton_raphson","category":"page"},{"location":"newton_raphson/#ADCME.newton_raphson","page":"Newton Raphson","title":"ADCME.newton_raphson","text":"newton_raphson(f::Function, u::Union{Array,PyObject}, θ::Union{Missing,PyObject}; options::Union{Dict{String, T}, Missing}=missing)\n\nNewton Raphson solver for solving a nonlinear equation.  f has the signature \n\nf(θ::Union{Missing,PyObject}, u::PyObject)->(r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is off)\nf(θ::Union{Missing,PyObject}, u::PyObject)->(fval::PyObject, r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is on)\n\nwhere r is the residual and A is the Jacobian matrix; in the case where linesearch is on, the function value fval must also be supplied. θ are external parameters. u0 is the initial guess for u options:\n\nmax_iter: maximum number of iterations (default=100)\nverbose: whether details are printed (default=false)\nrtol: relative tolerance for termination (default=1e-12)\ntol: absolute tolerance for termination (default=1e-12)\nLM: a float number, Levenberg-Marquardt modification x^k+1 = x^k - (J^k + mu^k)^-1g^k (default=0.0)\nlinesearch: whether linesearch is used (default=false)\n\nCurrently, the backtracing algorithm is implemented. The parameters for linesearch are also supplied via options\n\nls_c1: stop criterion, f(x^k)  f(0) + alpha c_1  f(0)\nls_ρ_hi: the new step size alpha_1leq rho_hialpha_0 \nls_ρ_lo: the new step size alpha_1geq rho_loalpha_0 \nls_iterations: maximum number of iterations for linesearch\nls_maxstep: maximum allowable steps\nls_αinitial: initial guess for the step size alpha\n\n\n\n\n\n","category":"function"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"As an example, assume we want to solve ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"u_i^2 - 1 = 0 i=12ldots 10","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"We first need to construct a function ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"function f(θ, u)\n    return u^2 - 1, 2*spdiag(u)\nend","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"Here 2textttspdiag(u) is the Jacobian matrix for the equation. Then we construct a Newton Raphson solver via","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr = newton_raphson(f, constant(rand(10)))","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr is a NRResult struct which is runnable and can be materialized by ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr = run(sess, nr)","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"The signature for NRResult is ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"struct NRResult\n    x::Union{PyObject, Array{Float64}} # final solution\n    res::Union{PyObject, Array{Float64, 1}} # residual\n    u::Union{PyObject, Array{Float64, 2}} # solution history\n    converged::Union{PyObject, Bool} # whether it converges\n    iter::Union{PyObject, Int64} # number of iterations\nend","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"uin mathbbR^ptimes n where p is the solution dimension and n is the number of iterations. ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"note: Note\nSometimes we want to construct f via some external variables theta, e.g., when theta is a trainable variable and embeded in the Newton-Raphson solver, we can pass this parameter to newton_raphson via the third parameternr = newton_raphson(f, constant(rand(10)),θ)","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"note: Note\nnewton_raphson also accepts a keyword argument options through which we can specify special options for the optimization. For examplenr = newton_raphson(f, constant(rand(10)), missing, \n            options=Dict(\"verbose\"=>true, \"tol\"=>1e-12))This might be useful for debugging.","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"In the case we want to apply a linesearch step in our Newton-Raphson solver, we can turn on the linesearch option in options. However, in this case, we must provide the function value for f (assuming we are solving a minimization problem).  ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"function f(θ, u)\n    return sum(1/3*u^3-u), u^2 - 1, 2*spdiag(u)\nend","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"The corresponding driver code is","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr = newton_raphson(f, constant(rand(10)), missing, \n                options=Dict(\"verbose\"=>false, \"tol\"=>1e-12, \"linesearch\"=>true, \"ls_αinitial\"=>1.0))","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"Finally we consider an advanced usage of the code, where we want to create a custom operator that solves","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"y^3-x=0","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"We compute the forward using Newton-Raphson and the backward with the implicit function theorem.","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"using Random\nfunction myop_(x)\n    function f(θ, y)\n        y^3 - x, spdiag(3y^2)\n    end\n    nr = newton_raphson(f, constant(ones(length(x))), options=Dict(\"verbose\"=>true))\n    y = nr.x\n    function myop_grad(dy, y)\n        dy/3y^2\n    end\n    # move variables to python space\n    s = randstring(8)\npy\"\"\"\ny_$$s = $y\ngrad_$$s = $myop_grad\n\"\"\"\n    # workaround \n    g = py\"\"\"lambda dy: grad_$$s(dy, y_$$s)\"\"\"\n    return y, g\nend\ntf_myop = tf.custom_gradient(myop_)","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"note: Note\nHere py\"\"\"lambda dy: grad_$$s(dy, y_$$s)\"\"\" is related to a workaround for converting Julia function to Python function.  Also we need to explicitly put Julia object to Python. ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"x = constant(8ones(5))\ny = tf_myop(x)\nprintln(run(sess, y))\n\nl = sum(y)\nrun(sess, gradients(l, x))","category":"page"},{"location":"#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"ADCME is suitable for conducting inverse modeling in scientific computing. The purpose of the package is to: (1) provide differentiable programming framework for scientific computing based on TensorFlow automatic differentiation (AD) backend; (2) adapt syntax to facilitate implementing scientific computing, particularly for numerical PDE discretization schemes; (3) supply missing functionalities in the backend (TensorFlow) that are important for engineering, such as sparse linear algebra, constrained optimization, etc. Applications include","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"full wavelength inversion\nreduced order modeling in solid mechanics\nlearning hidden geophysical dynamics\nphysics based machine learning\nparameter estimation in stochastic processes","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The package inherents the scalability and efficiency from the well-optimized backend TensorFlow. Meanwhile, it provides access to incooperate existing C/C++ codes via the custom operators. For example, some functionalities for sparse matrices are implemented in this way and serve as extendable \"plugins\" for ADCME. ","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Read more about the methodology and philosophy about ADCME: slides.","category":"page"},{"location":"#Getting-Started-1","page":"Overview","title":"Getting Started","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"To install ADCME, simply type the following commands in Julia REPL","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"julia> using Pkg; Pkg.add(\"ADCME\")","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"To enable GPU support for custom operators (if you do not need to compile custom operators, you do not need this step), make sure nvcc command is available on your machine, then","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using ADCME\nenable_gpu()","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"We consider a simple inverse modeling problem: consider the following partial differential equation","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"-bu(x)+u(x)=f(x)quad xin01 u(0)=u(1)=0","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"where ","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"f(x) = 8 + 4x - 4x^2","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Assume that we have observed u(05)=1, we want to estimate b. The true value in this case should be b=1. We can discretize the system using finite difference method, and the resultant linear system will be","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"(bA+I)mathbfu = mathbff","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"where","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"A = beginbmatrix\n        frac2h^2  -frac1h^2  dots  0\n         -frac1h^2  frac2h^2  dots  0\n         dots \n         0  0  dots  frac2h^2\n    endbmatrix quad mathbfu = beginbmatrix\n        u_2\n        u_3\n        vdots\n        u_n\n    endbmatrix quad mathbff = beginbmatrix\n        f(x_2)\n        f(x_3)\n        vdots\n        f(x_n)\n    endbmatrix","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The idea for implementing the inverse modeling method in ADCME is that we make the unknown b a Variable and then solve the forward problem pretending b is known. The following code snippet shows the implementation","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using LinearAlgebra\nusing ADCME\n\nn = 101 # number of grid nodes in [0,1]\nh = 1/(n-1)\nx = LinRange(0,1,n)[2:end-1]\n\nb = Variable(10.0) # we use Variable keyword to mark the unknowns\nA = diagm(0=>2/h^2*ones(n-2), -1=>-1/h^2*ones(n-3), 1=>-1/h^2*ones(n-3)) \nB = b*A + I  # I stands for the identity matrix\nf = @. 4*(2 + x - x^2) \nu = B\\f # solve the equation using built-in linear solver\nue = u[div(n+1,2)] # extract values at x=0.5\n\nloss = (ue-1.0)^2 \n\n# Optimization\nsess = Session(); init(sess) \nBFGS!(sess, loss)\n\nprintln(\"Estimated b = \", run(sess, b))","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The expected output is","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Estimated b = 0.9995582304494237","category":"page"},{"location":"julia_customop/#Julia-Custom-Operators-1","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"warning: Warning\nCurrently, embedding Julia suffers from multithreading issues: calling Julia from a non-Julia thread is not supported in ADCME. When TensorFlow kernel codes are executed concurrently, it is difficult to invoke the Julia functions. See issue.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"In scientific and engineering applications, the operators provided by TensorFlow are not sufficient for high performance computing. In addition, constraining oneself to TensorFlow environment sacrifices the powerful scientific computing ecosystems provided by other languages such as Julia and Python. For example, one might want to code a finite volume method for a sophisticated fluid dynamics problem; it is hard to have the flexible syntax to achieve this goal, obtain performance boost from existing fast solvers such as AMG, and benefit from many other third-party packages within TensorFlow. This motivates us to find a way to \"plugin\" custom operators to TensorFlow.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We have already introduced how to incooperate C++ custom operators.  For many researchers, they usually prototype the solvers in a high level language such as MATLAB, Julia or Python. To enjoy the parallelism and automatic differentiation feature of TensorFlow, they need to port them into C/C++. However, this is also cumbersome sometimes, espeically the original solvers depend on many packages in the high-level language. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We solve this problem by incorporating Julia functions directly into TensorFlow. That is, for any Julia functions, we can immediately convert it to a TensorFlow operator. At runtime, when this operator is executed, the corresponding Julia function is executed. That implies we have the Julia speed. Most importantly, the function is perfectly compitable with the native Julia environment; third-party packages, global variables, nested functions, etc. all work smoothly. Since Julia has the ability to call other languages in a quite elegant and simple manner, such as C/C++, Python, R, Java, this means it is possible to incoporate packages/codes from any supported languages into TensorFlow ecosystem. We need to point out that in TensorFlow, tf.numpy_function can be used to convert a Python function to a TensorFlow operator. However, in the runtime, the speed for this operator falls back to Python (or numpy operation for related parts). This is a drawback. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The key for implementing the mechanism is embedding Julia in C++. Still we need to create a C++ dynamic library for TensorFlow. However, the library is only an interface for invoking Julia code. At runtime, jl_get_function is called to search for the related function in the main module. C++ arrays, which include all the relavant data, are passed to this function through jl_call. It requires routine convertion from C++ arrays to Julia array interfaces jl_array_t*. However, those bookkeeping tasks are programatic and possibly will be automated in the future. Afterwards,Julia returns the result to C++ and thereafter the data are passed to the next operator. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"There are two caveats in the implementation. The first is that due to GIL of Python, we must take care of the thread lock while interfacing with Julia. This was done by putting a guard around th eJulia interface","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n// code here \nPyGILState_Release(py_threadstate);","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The second is the memory mangement of Julia arrays. This was done by defining gabage collection markers explicitly","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\nargs[0] = ...\nargs[1] = ...\n# do something\nJL_GC_POP();","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"This technique is remarkable and puts together one of the best langages in scientific computing and that in machine learning. The work that can be built on ADCME is enormous and significantly reduce the development time. ","category":"page"},{"location":"julia_customop/#Example-1","page":"Julia Custom Operators","title":"Example","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Here we present a simple example. Suppose we want to compute the Jacobian of a two layer neural network fracpartial ypartial x","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"y = W_2tanh(W_1x+b_1)+b_2","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"where x b_1 b_2 yin mathbbR^10, W_1 W_2in mathbbR^100. In TensorFlow, this can be done by computing the gradients fracpartial y_ipartial x for each i. In Julia, we can use ForwardDiff to do it automatically. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"function twolayer(J, x, w1, w2, b1, b2)\n    f = x -> begin\n        w1 = reshape(w1, 10, 10)\n        w2 = reshape(w2, 10, 10)\n        z = w2*tanh.(w1*x+b1)+b2\n    end\n    J[:] = ForwardDiff.jacobian(f, x)[:]\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"To make a custom operator, we first generate a wrapper","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"using ADCME\nmkdir(\"TwoLayer\")\ncd(\"TwoLayer\")\ncustomop()","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We modify custom_op.txt","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"TwoLayer\ndouble x(?)\ndouble w1(?)\ndouble b1(?)\ndouble w2(?)\ndouble b2(?)\ndouble y(?) -> output","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"and run ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"customop()","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Three files are generatedCMakeLists.txt, TwoLayer.cpp and gradtest.jl. Now create a new file TwoLayer.h","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"#include \"julia.h\"\n#include \"Python.h\"\n\nvoid forward(double *y, const double *x, const double *w1, const double *w2, const double *b1, const double *b2, int n){\n    PyGILState_STATE py_threadstate;\n    py_threadstate = PyGILState_Ensure();\n    jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\n    jl_value_t **args;\n    JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\n    args[0] = (jl_value_t*)jl_ptr_to_array_1d(array_type, y, n*n, 0);\n    args[1] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(x), n, 0);\n    args[2] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w1), n*n, 0);\n    args[3] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w2), n*n, 0);\n    args[4] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b1), n, 0);\n    args[5] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b2), n, 0);\n    auto fun = jl_get_function(jl_main_module, \"twolayer\");\n  \tif (fun==NULL) jl_errorf(\"Function not found in Main module.\");\n    else jl_call(fun, args, 6);\n    JL_GC_POP();\n    if (jl_exception_occurred())\n        printf(\"%s \\n\", jl_typeof_str(jl_exception_occurred()));\n    PyGILState_Release(py_threadstate);\n}","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Most of the codes have been explanined except jl_ptr_to_array_1d. This function generates a Julia array wrapper from C++ arrays. The last argument 0 indicates that Julia is not responsible for gabage collection. TwoLayer.cpp should also be modified according to https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Finally, we can test in gradtest.jl ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"two_layer = load_op(\"build/libTwoLayer\", \"two_layer\")\n\n\nw1 = rand(100)\nw2 = rand(100)\nb1 = rand(10)\nb2 = rand(10)\nx = rand(10)\nJ = rand(100)\ntwolayer(J, x, w1, w2, b1, b2)\n\ny = two_layer(constant(x), constant(w1), constant(b1), constant(w2), constant(b2))\nsess = Session(); init(sess)\nJ0 = run(sess, y)\n@show norm(J-J0)","category":"page"},{"location":"julia_customop/#Embedded-in-Modules-1","page":"Julia Custom Operators","title":"Embedded in Modules","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"If the custom operator is intended to be used in a precompiled module, we can load the dynamic library at initialization","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"global my_op \nfunction __init__()\n\tglobal my_op = load_op(\"$(@__DIR__)/path/to/libMyOp\", \"my_op\")\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The corresponding Julia function called by my_op must be exported in the module (such that it is in the Main module when invoked). One such example is given in MyModule","category":"page"},{"location":"julia_customop/#Reference-Sheet-1","page":"Julia Custom Operators","title":"Reference Sheet","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"For implementation reference, see Reference Sheet","category":"page"}]
}
