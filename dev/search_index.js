var documenterSearchIndex = {"docs":
[{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-Julia-Custom-Operator-in-ADCMAE-1","page":"-","title":"Quick Reference for Implementing Julia Custom Operator in ADCMAE","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Header files","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"  #include \"julia.h\"\n  #include \"Python.h\"","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"For Python GIL workround","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n...\nPyGILState_Release(py_threadstate);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Get function from Julia main module ","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_get_function(jl_main_module, \"myfun\");","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ to Julia","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t *a = jl_box_float64(3.0);\njl_value_t *b = jl_box_float32(3.0f);\njl_value_t *c = jl_box_int32(3);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia to C++","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double ret_unboxed = jl_unbox_float64(ret);\nfloat  ret_unboxed = jl_unbox_float32(ret);\nint32  ret_unboxed = jl_unbox_int32(ret);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ Arrays to Julia Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\njl_array_t* x          = jl_alloc_array_1d(array_type, 10);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"or for existing arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *existingArray = (double*)malloc(sizeof(double)*10);\njl_array_t *x = jl_ptr_to_array_1d(array_type, existingArray, 10, 0);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia Arrays to C++ Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *xData = (double*)jl_array_data(x);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Call Julia Functions","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_array_t *y = (jl_array_t*)jl_call1(func, (jl_value_t*)x);\njl_value_t *jl_call(jl_function_t *f, jl_value_t **args, int32_t nargs)","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Gabage collection","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 2); // args can now hold 2 `jl_value_t*` objects\nargs[0] = some_value;\nargs[1] = some_other_value;\n// Do something with args (e.g. call jl_... functions)\nJL_GC_POP();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Reference: Embedding Julia","category":"page"},{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1","page":"-","title":"Quick Reference for Implementing C++ Custom Operators in ADCME","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Set output shape","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"c->set_output(0, c->Vector(n));\nc->set_output(0, c->Matrix(m, n));\nc->set_output(0, c->Scalar());","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Names","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":".Input and .Ouput : names must be in lower case, no _, only letters.","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"TensorFlow Input/Output to TensorFlow Tensors","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.vec<double>();\ngrad.scalar<double>();\ngrad.matrix<double>();\ngrad.flat<double>();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Obtain flat arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.flat<double>().data()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Scalars","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate scalars using TensorShape()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate Shapes","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Although you can use -1 for shape reference, you must allocate exact shapes in Compute","category":"page"},{"location":"extra/#Miscellaneous-Tools-1","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"There are many handy tools implemented in ADCME for analysis, benchmarking, input/output, etc. ","category":"page"},{"location":"extra/#Debugging-and-printing-1","page":"Miscellaneous Tools","title":"Debugging and printing","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Add the following line before Session and change tf.Session to see verbose printing (such as GPU/CPU information)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.debugging.set_log_device_placement(true)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.print can be used for printing tensor values. It must be binded with an executive operator.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"# a, b are tensors, and b is executive\nop = tf.print(a)\nb = bind(b, op)","category":"page"},{"location":"extra/#Benchmarking-1","page":"Miscellaneous Tools","title":"Benchmarking","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"The functions tic and toc can be used for recording the runtime between two operations. tic starts a timer for performance measurement while toc marks the termination of the measurement. Both functions are bound with one operations. For example, we can benchmark the runtime for svd","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"A = constant(rand(10,20))\nA = tic(A)\nr = svd(A)\nB = r.U*diagm(r.S)*r.Vt \nB, t = toc(B)\nrun(sess, B)\nrun(sess, t)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tic\ntoc","category":"page"},{"location":"extra/#ADCME.tic","page":"Miscellaneous Tools","title":"ADCME.tic","text":"tic(o::PyObject, i::Union{PyObject, Integer}=0)\n\nConstruts a TensorFlow timer with index i. The start time record is right before o is executed.\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.toc","page":"Miscellaneous Tools","title":"ADCME.toc","text":"toc(o::PyObject, i::Union{PyObject, Integer}=0)\n\nReturns the elapsed time from last tic call with index i (default=0). The terminal time record is right before o is executed.\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-Python-Object-1","page":"Miscellaneous Tools","title":"Save and Load Python Object","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"psave\npload","category":"page"},{"location":"extra/#ADCME.psave","page":"Miscellaneous Tools","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.pload","page":"Miscellaneous Tools","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-TensorFlow-Session-1","page":"Miscellaneous Tools","title":"Save and Load TensorFlow Session","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"load\nsave","category":"page"},{"location":"extra/#ADCME.load","page":"Miscellaneous Tools","title":"ADCME.load","text":"load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nLoads the values of variables to the session sess from the file file. If vars is nothing, it loads values to all the trainable variables. See also save, load\n\n\n\n\n\nload(sw::Diary, dirp::String)\n\nLoads Diary from dirp.\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.save","page":"Miscellaneous Tools","title":"ADCME.save","text":"save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nSaves the values of vars in the session sess. The result is written into file as a dictionary. If vars is nothing, it saves all the trainable variables. See also save, load\n\n\n\n\n\nsave(sw::Diary, dirp::String)\n\nSaves Diary to dirp.\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-Diary-1","page":"Miscellaneous Tools","title":"Save and Load Diary","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"We can use TensorBoard to track a scalar value easily","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"d = Diary(\"test\")\np = placeholder(1.0, dtype=Float64)\nb = constant(1.0)+p\ns = scalar(b, \"variable\")\nfor i = 1:100\n    write(d, i, run(sess, s, Dict(p=>Float64(i))))\nend\nactivate(d)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Diary\nscalar\nactivate\nload\nsave\nwrite","category":"page"},{"location":"extra/#ADCME.Diary","page":"Miscellaneous Tools","title":"ADCME.Diary","text":"Diary(suffix::Union{String, Nothing}=nothing)\n\nCreates a diary at a temporary directory path. It returns a writer and the corresponding directory path\n\n\n\n\n\n","category":"type"},{"location":"extra/#ADCME.scalar","page":"Miscellaneous Tools","title":"ADCME.scalar","text":"scalar(o::PyObject, name::String)\n\nReturns a scalar summary object.\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.activate","page":"Miscellaneous Tools","title":"ADCME.activate","text":"activate(sw::Diary, port::Int64=6006)\n\nRunning Diary at http://localhost:port.\n\n\n\n\n\n","category":"function"},{"location":"extra/#Base.write","page":"Miscellaneous Tools","title":"Base.write","text":"write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})\n\nWrites to Diary.\n\n\n\n\n\n","category":"function"},{"location":"api/#Core-Functions-1","page":"-","title":"Core Functions","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"ADCME.jl\", \"core.jl\", \"run.jl]","category":"page"},{"location":"api/#ADCME.Diary","page":"-","title":"ADCME.Diary","text":"Diary(suffix::Union{String, Nothing}=nothing)\n\nCreates a diary at a temporary directory path. It returns a writer and the corresponding directory path\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.GAN","page":"-","title":"ADCME.GAN","text":"GAN(dat::PyObject, generator::Function, gan::GAN, loss::Union{String, Function, Missing}=missing; latent_dim::Union{Missing, Int64}=missing, batch_size::Union{Missing, Int64}=missing)\n\nUsers must provide: dat, generator, discriminator, loss (string or function)  Alternative argument: latent_dim, batch_size Training process is the most creative part and therefore no built-in algorithm is provided.  Users have access to d_loss, g_loss, d_vars, g_vars, which are sufficient for designing training algorithms.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}","page":"-","title":"ADCME.SparseTensor","text":"SparseTensor(A::SparseMatrixCSC)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseTensor-Union{Tuple{S}, Tuple{T}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S},Union{Nothing, PyObject, S}}} where S<:Integer where T<:Integer","page":"-","title":"ADCME.SparseTensor","text":"SparseTensor(I::Union{PyObject,Array{T,1}}, J::Union{PyObject,Array{T,1}}, V::Union{Array{Float64,1}, PyObject}, m::Union{S, PyObject, Nothing}=nothing, n::Union{S, PyObject, Nothing}=nothing) where {T<:Integer, S<:Integer}\n\nConstructs a sparse tensor.  Examples:\n\nii = [1;2;3;4]\njj = [1;2;3;4]\nvv = [1.0;1.0;1.0;1.0]\ns = SparseTensor(ii, jj, vv, 4, 4)\ns = SparseTensor(sprand(10,10,0.3))\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.BFGS!","page":"-","title":"ADCME.BFGS!","text":"BFGS!(value_and_gradients_function::Function, initial_position::Union{PyObject, Array{Float64}}, max_iter::Int64=50, args...;kwargs...)\n\nApplies the BFGS optimizer to value_and_gradients_function\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!","page":"-","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, max_iter::Int64=15000; kwargs...)\n\nBFGS! is a simplified interface for BFGS optimizer. \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.CustomOptimizer-Tuple{Function}","page":"-","title":"ADCME.CustomOptimizer","text":"CustomOptimizer(opt::Function, name::String)\n\ncreates a custom optimizer with struct name name. For example, we can integrate Optim.jl with ADCME by  constructing a new optimizer\n\nCustomOptimizer(\"Con\") do f, df, c, dc, x0, nineq, neq, x_L, x_U\n    opt = Opt(:LD_MMA, length(x0))\n    bd = zeros(length(x0)); bd[end-1:end] = [-Inf, 0.0]\n    opt.lower_bounds = bd\n    opt.xtol_rel = 1e-4\n    opt.min_objective = (x,g)->(g[:]= df(x); return f(x)[1])\n    inequality_constraint!(opt, (x,g)->( g[:]= dc(x);c(x)[1]), 1e-8)\n    (minf,minx,ret) = NLopt.optimize(opt, x0)\n    minx\nend\n\nThen we can create an optimizer with \n\nopt = Con(loss, inequalities=[c1], equalities=[c2])\n\nTo trigger the optimization, use\n\nopt.minimize(sess)\n\nor \n\nminimize(opt, sess)\n\nNote thanks to the global variable scope of Julia, step_callback, optimizer_kwargs can actually  be passed from Julia environment directly.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyObject},Union{PyObject, Array{Float64,N} where N}}} where T<:Real","page":"-","title":"ADCME.NonlinearConstrainedProblem","text":"NonlinearConstrainedProblem(f::Function, L::Function, θ::PyObject, u0::Union{PyObject, Array{Float64}}; options::Union{Dict{String, T}, Missing}=missing) where T<:Integer\n\nComputes the gradients fracpartial Lpartial theta\n\nbeginalign\nmin   L(u) \nmathrmst   F(theta u) = 0\nendalign\n\nu0 is the initial guess for the numerical solution u, see newton_raphson.\n\nCaveats: Assume r, A = f(θ, u) and θ are the unknown parameters, gradients(r, θ) must be defined (backprop works properly)\n\nReturns: It returns a tuple (L: loss, C: constraints, and Graidents)\n\nleft(L(u) u fracpartial Lpartial θright)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerInterface-Tuple{Any}","page":"-","title":"ADCME.ScipyOptimizerInterface","text":"ScipyOptimizerInterface(     loss,     varlist=None,     equalities=None,     inequalities=None,     vartobounds=None,     **optimizerkwargs ) https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerMinimize-Tuple{PyObject,PyObject}","page":"-","title":"ADCME.ScipyOptimizerMinimize","text":"ScipyOptimizerMinimize(     session=None,     feeddict=None,     fetches=None,     stepcallback=None,     losscallback=None,     **runkwargs ) Minimize a scalar Tensor.\n\nVariables subject to optimization are updated in-place at the end of optimization.\n\nNote that this method does not just return a minimization Op, unlike minimize; instead it actually performs minimization by executing commands to control a Session https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface\n\nkwargs\n\n– feeddict: A feed dict to be passed to calls to session.run. – fetches: A list of Tensors to fetch and supply to losscallback as positional arguments. – stepcallback: A function to be called at each optimization step; arguments are the current values of all optimization variables flattened into a single vector. – losscallback: A function to be called every time the loss and gradients are computed, with evaluated fetches supplied as positional arguments. – run_kwargs: kwargs to pass to session.run.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseAssembler-Tuple{}","page":"-","title":"ADCME.SparseAssembler","text":"accumulator, creater, initializer = SparseAssembler()\n\nReturns 3 functions that can be used for assembling sparse matrices concurrently.\n\ninitializer must be called before the working session\naccumulator accumulates column indices and values \ncreator accepts no input and outputs row indices, column indices and values for the sparse matrix\n\nExample:\n\naccumulator, creater, initializer = SparseAssembler()\ninitializer(5)\nop1 = accumulator(1, [1;2;3], ones(3))\nop2 = accumulator(1, [3], [1.])\nop3 = accumulator(2, [1;3], ones(2))\nrun(sess, [op1,op2,op3])\nii,jj,vv = creater()\ni,j,v = run(sess, [ii,jj,vv])\nA = sparse(i,j,v,5,5)\n@assert Array(A)≈[1.0  1.0  2.0  0.0  0.0\n                1.0  0.0  1.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0]\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.activate","page":"-","title":"ADCME.activate","text":"activate(sw::Diary, port::Int64=6006)\n\nRunning Diary at http://localhost:port.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.add_collection-Tuple{String,PyObject}","page":"-","title":"ADCME.add_collection","text":"add_collection(name::String, v::PyObject)\n\nAdds v to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.add_collection-Tuple{String,Vararg{PyObject,N} where N}","page":"-","title":"ADCME.add_collection","text":"add_collection(name::String, vs::PyObject...)\n\nAdds operators vs to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae","page":"-","title":"ADCME.ae","text":"ae(x::PyObject, output_dims::Array{Int64}, scope::String = \"default\")\n\nCreates a neural network with intermediate numbers of neurons output_dims.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.ae-Tuple{Union{PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{PyObject, Array{Float64,N} where N}}","page":"-","title":"ADCME.ae","text":"ae(x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64}, θ::Union{Array{Float64}, PyObject})\n\nCreates a neural network with intermediate numbers of neurons output_dims. The weights are given by θ\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_init-Tuple{PyObject,Union{PyObject, Array{Float64,N} where N},Array{Int64,N} where N}","page":"-","title":"ADCME.ae_init","text":"ae_init(sess::PyObject, x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64})\n\nReturn the initial weights and bias values by TensorFlow as a vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.bn-Tuple","page":"-","title":"ADCME.bn","text":"example: bn(inputs, name=\"batchnorm\", istraining=true)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.categorical-Tuple{Union{Integer, PyObject}}","page":"-","title":"ADCME.categorical","text":"categorical(n::Union{PyObject, Integer}; kwargs...)\n\nkwargs has a keyword argument logits, a 2-D Tensor with shape [batch_size, num_classes].   Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.choice-Tuple{Union{PyObject, Array},Union{Integer, PyObject}}","page":"-","title":"ADCME.choice","text":"choice(inputs::Union{PyObject, Array}, n_samples::Union{PyObject, Integer};replace::Bool=false)\n\nChoose n_samples samples from inputs with/without replacement. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.compile_op-Tuple{String}","page":"-","title":"ADCME.compile_op","text":"compile_op(oplibpath::String, opname::String)\n\nCompile the library operator by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.customop","page":"-","title":"ADCME.customop","text":"customop(torch=false; julia=false)\n\nCreate a new custom operator.\n\nexample\n\njulia> customop() # create an editable `customop.txt` file\n[ Info: Custom operator wrapper generated; Torch is disabled\n\njulia> customop() # after editing `customop.txt`, call it again to generate interface files.\n[ Info: Custom operator wrapper generated; Torch is disabled\n\nThe option torch adds support for PyTorch backend in CMakeLists.txt\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.find-Tuple{SparseTensor}","page":"-","title":"ADCME.find","text":"find(s::SparseTensor)\n\nReturns the row, column and values for sparse tensor s.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.get_collection","page":"-","title":"ADCME.get_collection","text":"get_collection(name::Union{String, Missing})\n\nReturns the collection with name name. If name is missing, returns all the trainable variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.gradients-Tuple{PyObject,PyObject}","page":"-","title":"ADCME.gradients","text":"gradients compute the gradients of ys w.r.t xs It incoperates jacobian and hessian\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.group_assign-Tuple{Array{PyObject,N} where N,Any,Vararg{Any,N} where N}","page":"-","title":"ADCME.group_assign","text":"group_assign(os::Array{PyObject}, values, args...; kwargs...)\n\napply assign to each element of os and values\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.has_gpu-Tuple{}","page":"-","title":"ADCME.has_gpu","text":"has_gpu()\n\nChecks if GPU is available.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.hessian-Tuple{PyObject,PyObject}","page":"-","title":"ADCME.hessian","text":"hessian computes the hessian of a scalar function f with respect to vector inputs xs\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.if_else-Tuple{Union{Bool, PyObject, Array},Any,Any,Vararg{Any,N} where N}","page":"-","title":"ADCME.if_else","text":"if_else(condition::Union{PyObject,Array,Bool}, fn1, fn2, args...;kwargs...)\n\nIf condition is a scalar boolean, it outputs fn1 or fn2 (a function with no input argument or a tensor) based on whether condition is true or false.\nIf condition is a boolean array, if returns condition .* fn1 + (1 - condition) .* fn2\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load","page":"-","title":"ADCME.load","text":"load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nLoads the values of variables to the session sess from the file file. If vars is nothing, it loads values to all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load-Tuple{Diary,String}","page":"-","title":"ADCME.load","text":"load(sw::Diary, dirp::String)\n\nLoads Diary from dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op-Tuple{String,String}","page":"-","title":"ADCME.load_op","text":"load_op(oplibpath::String, opname::String)\n\nloads the operator opname from library oplibpath,  if the surfix of oplibpath is not given, it will be inferred from system\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.newton_raphson-Union{Tuple{T}, Tuple{Function,Union{PyObject, Array}}, Tuple{Function,Union{PyObject, Array},Union{Missing, PyObject, Array{#s92,N} where N where #s92<:Real}}} where T<:Real","page":"-","title":"ADCME.newton_raphson","text":"newton_raphson(f::Function, u::Union{Array,PyObject}, θ::Union{Missing,PyObject}; options::Union{Dict{String, T}, Missing}=missing)\n\nNewton Raphson solver for solving a nonlinear equation.  f has the signature f(u::PyObject, θ::Union{Missing,PyObject})->(r::PyObject, A::Union{PyObject,SparseTensor}) where r is the residual and A is the Jacobian matrix.  θ are external parameters. u0 is the initial guess for u options:\n\n\"max_iter\": maximum number of iterations (default=100)\n\"verbose\": whether details are printed (default=false)\n\"rtol\": relative tolerance for termination (default=1e-12)\n\"tol\": absolute tolerance for termination (default=1e-12)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.num_ae-Tuple{Array{Int64,N} where N}","page":"-","title":"ADCME.num_ae","text":"num_ae(output_dims::Array{Int64})\n\nEstimates the number of weights for the neural network.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.pload-Tuple{String}","page":"-","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.psave-Tuple{PyObject,String}","page":"-","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.reset_default_graph-Tuple{}","page":"-","title":"ADCME.reset_default_graph","text":"reset_default_graph()\n\nResets the graph by removing all the operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save","page":"-","title":"ADCME.save","text":"save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nSaves the values of vars in the session sess. The result is written into file as a dictionary. If vars is nothing, it saves all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.save-Tuple{Diary,String}","page":"-","title":"ADCME.save","text":"save(sw::Diary, dirp::String)\n\nSaves Diary to dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.scalar","page":"-","title":"ADCME.scalar","text":"scalar(o::PyObject, name::String)\n\nReturns a scalar summary object.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.spdiag-Tuple{Int64}","page":"-","title":"ADCME.spdiag","text":"spdiag(n::Int64)\n\nConstructs a sparse identity matrix of size ntimes n.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{PyObject}","page":"-","title":"ADCME.spdiag","text":"spdiag(o::PyObject)\n\nConstructs a sparse diagonal matrix where the diagonal entries are o\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spzero","page":"-","title":"ADCME.spzero","text":"spzero(m::Int64, n::Union{Missing, Int64}=missing)\n\nConstructs a empty sparse matrix of size mtimes n. n=m if n is missing\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.tensor-Tuple{String}","page":"-","title":"ADCME.tensor","text":"tensor(s::String)\n\nReturns the tensor with name s. See tensorname.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensorname-Tuple{PyObject}","page":"-","title":"ADCME.tensorname","text":"tensorname(o::PyObject)\n\nReturns the name of the tensor. See tensor.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tic","page":"-","title":"ADCME.tic","text":"tic(o::PyObject, i::Union{PyObject, Integer}=0)\n\nConstruts a TensorFlow timer with index i. The start time record is right before o is executed.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.toc","page":"-","title":"ADCME.toc","text":"toc(o::PyObject, i::Union{PyObject, Integer}=0)\n\nReturns the elapsed time from last tic call with index i (default=0). The terminal time record is right before o is executed.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.vector-Union{Tuple{T}, Tuple{Union{PyObject, StepRange, UnitRange, Array{T,N} where N},Union{PyObject, Array{Float64,N} where N},Union{Int64, PyObject}}} where T<:Integer","page":"-","title":"ADCME.vector","text":"vector(i::Union{Array{T}, PyObject, UnitRange, StepRange}, v::Union{Array{Float64},PyObject},s::Union{Int64,PyObject})\n\nReturns a vector V with length s such that\n\nV[i] = v\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.compile-Tuple{String}","page":"-","title":"ADCME.compile","text":"compile(s::String) Compile the library s by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.gradients10-Tuple{PyObject,PyObject}","page":"-","title":"ADCME.gradients10","text":"gradients_v computes the gradients of a vector function f(x) w.r.t. a single variable x ys is the n dimensional vector function  xs is a scalar \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.gradients11-Tuple{PyObject,PyObject}","page":"-","title":"ADCME.gradients11","text":"jacobian computes the jacobian of a vector function f with respect to a vector variable x the output is |f| x |x| matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_system_op","page":"-","title":"ADCME.load_system_op","text":"function loadsystemop(s::String, oplib::String, grad::Bool=true)\n\nLoad custom operator from CustomOps directory (shipped with ADCME instead of TensorFlow) For example \n\ns = \"SparseOperator\"\noplib = \"libSO\"\ngrad = true\n\nthis will direct Julia to find library CustomOps/SparseOperator/libSO.dylib on MACOSX\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.bind-Tuple{PyObject,Vararg{Any,N} where N}","page":"-","title":"Base.bind","text":"bind(op::PyObject, ops...)\n\nAdding operations ops to the dependencies of op. The function is useful when we want to execute ops but ops is not  in the dependency of the final output. For example, if we want to print i each time i is evaluated\n\ni = constant(1.0)\nop = tf.print(i)\ni = bind(i, op)\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.reshape-Tuple{PyObject,Integer}","page":"-","title":"Base.reshape","text":"reshape is designed so that we can think of tensors in column major\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.write-Tuple{Diary,Int64,Union{String, Array{String,N} where N}}","page":"-","title":"Base.write","text":"write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})\n\nWrites to Diary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Variables-1","page":"-","title":"Variables","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"variable.jl\"]","category":"page"},{"location":"api/#Random-Variables-1","page":"-","title":"Random Variables","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"random.jl\"]","category":"page"},{"location":"api/#Sparse-Matrix-1","page":"-","title":"Sparse Matrix","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"sparse.jl\"]","category":"page"},{"location":"api/#Operations-1","page":"-","title":"Operations","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"ops.jl\"]","category":"page"},{"location":"api/#Neural-Network-Utilities-1","page":"-","title":"Neural Network Utilities","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"layers.jl\"]","category":"page"},{"location":"api/#IO-1","page":"-","title":"IO","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"io.jl\"]","category":"page"},{"location":"api/#Optimization-1","page":"-","title":"Optimization","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"optim.jl\"]","category":"page"},{"location":"api/#Generative-Neural-Nets-1","page":"-","title":"Generative Neural Nets","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"gan.jl\"]","category":"page"},{"location":"api/#Tools-1","page":"-","title":"Tools","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"extra.jl\"]","category":"page"},{"location":"api/#Datasets-1","page":"-","title":"Datasets","text":"","category":"section"},{"location":"api/#","page":"-","title":"-","text":"Modules = [ADCME]\nPages   = [\"dataset.jl\"]","category":"page"},{"location":"while_loop/#The-Power-of-while_loop-–-Application-to-Finite-Element-Analysis-1","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"","category":"section"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"##Why are loops important and challenging?","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"In science and engineering computing, we often encounter loops. For inverse modeling, it is usually desirable that we can compute the gradients of a forward simulation code even if there exists sophisticated loops. This is, however, not a trivial task in consideration of the large number of loops. For example, we have a piece of forward simulation codes","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"for i = 1:1000000\n  global x\n\tx = do_some_simulation(x)\nend","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"To be able to compute the gradients, we need to create 1000000 subgraphs for do_some_simulation under the hood to track the data flow. This will be very inefficient especially when graph optimization is conducted before execution. ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"TensorFlow provides us a clever way to do loops, where only one graph is created for the whole loops. The basic idea is to create a while_loop graph based on five primitives, and the corresponding graph for backpropagation is constructed thereafter. ","category":"page"},{"location":"while_loop/#A-simple-example-1","page":"The Power of while_loop – Application to Finite Element Analysis","title":"A simple example","text":"","category":"section"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"As a simple example, we consider assemble the external load vector for linear finite elements in 1D. Assume that the load distribution is f(x)=1-x^2, xin01. The goal is to compute a vector mathbfv with v_i=int_0^1 f(x)phi_i(x)dx, where phi_i(x) is the i-th linear element. ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"The pseudocode for this problem is shown in the following","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"F = zeros(ne+1) // ne is the total number of elements\nfor e = 1:ne\n\tx_mid = middle point of element e\n\th = length of element e\n\tf_mid = f(x_mid)\n\tF[global index of left end point of e] += f_mid*h/2\n\tF[global index of right end point of e] += f_mid*h/2\nend","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"(Image: )","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"However, if ne is very large, writing explicit loops is unwise since it will create ne subgraphs. while_loop can be very helpful in this case (the script can also be found in https://github.com/kailaix/ADCME.jl/tree/master/examples/whileloop/whileloop_simple.jl)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"using ADCME\n\nne = 100\nh = 1/ne\nf = x->1-x^2\nfunction cond0(i, F_arr)\n    i<=ne+1\nend\nfunction body(i, F_arr)\n    fmid = f(cast(i-2, Float64)*h+h/2)\n    F = constant(zeros(ne+1))\n    F = scatter_add(F, [i-1;i], [fmid*h/2;fmid*h/2])\n    F_arr = write(F_arr, i, F)\n    i+1, F_arr\nend\n\nF_arr = TensorArray(ne+1)\nF_arr = write(F_arr, 1, constant(zeros(ne+1))) # inform `F_arr` of the data type by writing at index 1\ni = constant(2, dtype=Int32)\n_, out = while_loop(cond0, body, [i,F_arr]; parallel_iterations=10)\nF = sum(stack(out), dims=1)\nsess = Session(); init(sess)\nF0 = run(sess, F)\n","category":"page"},{"location":"while_loop/#A-practical-application-1","page":"The Power of while_loop – Application to Finite Element Analysis","title":"A practical application","text":"","category":"section"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"In this section, we demonstrate how to assemble a finite element matrix based on while_loop for a 2D Poisson problem. We consider the following problem $ \\begin{aligned} \\nabla \\cdot ( D\\nabla u(\\mathbf{x}) ) &= f(\\mathbf{x})& \\mathbf{x}\\in \\Omega\\\nu(\\mathbf{x}) &= 0 & \\mathbf{x}\\in \\partial \\Omega \\end{aligned} $ Here Omega is the unit disk. We consider a simple case, where $ \\begin{aligned} D&=\\mathbf{I}\\\nf(\\mathbf{x})&=-4 \\end{aligned} $ Then the exact solution will be  $ u(\\mathbf{x}) = 1-x^2-y^2 $ The weak formulation is $ \\langle \\nabla v(\\mathbf{x}), D\\nabla u(\\mathbf{x}) \\rangle = \\langle f(\\mathbf{x}),v(\\mathbf{x}) \\rangle $ We  split Omega into triangles mathcalT and use piecewise linear basis functions. Typically, we would iterate over all elements and compute the local stiffness matrix for each element. However, this could result in a large loop if we use a fine mesh. Instead, we can use while_loop to complete the task. In ADCME, the syntax for while_loop is ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"while_loop(condition, body, loop_vars)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"here condition and body take loop_vars as inputs. The former outputs a bool tensor indicating whether to terminate the loop while the latter outputs the updated loop_vars. TensorArry is used to store variables that change during the loops. The codes for assembling FEM is","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"function assemble_FEM(Ds, Fs, nodes, elem)\n    NT = size(elem,1)\n    cond0 = (i,tai,taj,tav, tak, taf) -> i<=NT\n    elem = constant(elem)\n    nodes = constant(nodes)\n    function body(i, tai, taj, tav, tak, taf)\n        el = elem[i]\n        x1, y1 = nodes[el[1]][1], nodes[el[1]][2]\n        x2, y2 = nodes[el[2]][1], nodes[el[2]][2]\n        x3, y3 = nodes[el[3]][1], nodes[el[3]][2]\n        T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)\n        D = Ds[i]; F = Fs[i]*T/3\n        v = T*stack([D*((-x2 + x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y2 - y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2)])\n        tav = write(tav, i, v)\n        ii = vec([elem[i] elem[i] elem[i]]')\n        jj = [elem[i]; elem[i]; elem[i]]\n        tai = write(tai, i, ii)\n        taj = write(taj, i, jj)\n        tak = write(tak, i, elem[i])\n        taf = write(taf, i, stack([F,F,F]))\n        return i+1, tai, taj, tav, tak, taf\n    end\n    tai = TensorArray(NT, dtype=Int32)\n    taj = TensorArray(NT, dtype=Int32)\n    tak = TensorArray(NT, dtype=Int32)\n    tav = TensorArray(NT)\n    taf = TensorArray(NT)\n    i = constant(1, dtype=Int32)\n    i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)\n    _, tai, taj, tav, tak, taf = while_loop(cond0, body, [i, tai, taj, tav, tak, taf]; parallel_iterations=10)\n    vec(stack(tai)[1:NT]'), vec(stack(taj)[1:NT]'), vec(stack(tav)[1:NT]'),\n                        vec(stack(tak)[1:NT]'), vec(stack(taf)[1:NT]')\nend","category":"page"},{"location":"while_loop/#Code-detail-explained-1","page":"The Power of while_loop – Application to Finite Element Analysis","title":"Code detail explained","text":"","category":"section"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"We now explain the codes. ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"We assume that nodes is a n_vtimes 2 tensor holding all n_v coordinates of the nodes, elem is a n_etimes 3  tensor holding all n_e triangle vertex index triples. We create five TensorArray to hold the row indices, column indices and values for the stiffness matrix, and row indices and values for the right hand side (Here NT denotes n_e):","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"tai = TensorArray(NT, dtype=Int32)\ntaj = TensorArray(NT, dtype=Int32)\ntak = TensorArray(NT, dtype=Int32)\ntav = TensorArray(NT)\ntaf = TensorArray(NT)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"Within each loop (body), we extract the coordinates of each vertex coordinate","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"el = elem[i]\nx1, y1 = nodes[el[1]][1], nodes[el[1]][2]\nx2, y2 = nodes[el[2]][1], nodes[el[2]][2]\nx3, y3 = nodes[el[3]][1], nodes[el[3]][2]","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"and compute the area of ith triangle","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"The local stiffness matrix is computed and vectorized (v). It is computed symbolically.  To store the computed value into TensorArray, we call the write API (there is also read API, which reads a value from TensorArray)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"tav = write(tav, i, v)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"Note we have called ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"before we call while_loop. This is because we need to initialize the TensorArrays (i.e., telling them the size and type of elements in the arrays). We must guarantee that the sizes and types of the elements in the arrays are consistent in while_loop. ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"Finally, we stack the TensorArray into a tensor and vectorized it according to the row major. This serves as the output of assemble_FEM. The complete script for solving this problem is here and the following plot shows the numerical result and corresponding reference solution. ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"(Image: Result for the Poisson Problem)","category":"page"},{"location":"while_loop/#Gradients-that-backpropagate-through-loops-1","page":"The Power of while_loop – Application to Finite Element Analysis","title":"Gradients that backpropagate through loops","text":"","category":"section"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"To inspect the gradients through the loops, we can run ","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"println(run(sess, gradients(sum(u), Ds))) # a sparse tensor","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"This outputs a sparse tensor instead of a full tensor. To obtain the full tensor, we could call tf.convert_to_tensor","category":"page"},{"location":"while_loop/#","page":"The Power of while_loop – Application to Finite Element Analysis","title":"The Power of while_loop – Application to Finite Element Analysis","text":"println(run(sess, tf.convert_to_tensor(gradients(sum(u), Ds)))) # full tensor","category":"page"},{"location":"customop/#Basic-Usage-1","page":"-","title":"Basic Usage","text":"","category":"section"},{"location":"customop/#","page":"-","title":"-","text":"Custom operators are ways to add missing features in ADCME. Typically users do not have to worry about custom operators. However, in the following situation custom opreators might be very useful","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Direct implementation in ADCME is inefficient (bottleneck). \nThere are legacy codes users want to reuse, such as GPU-accelerated codes. \nSpecial acceleration techniques such as checkpointing scheme. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"In the following, we present an example of implementing the sparse solver custom operator for Ax=b.","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Input: row vector ii, column vectorjj and value vector vv for the sparse coefficient matrix; row vector kk and value vector ff, matrix dimension d","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Output: solution vector u","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Step 1: Create and modify the template file","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"The following command helps create the wrapper","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"customop()","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"There will be a custom_op.txt in the current directory. Modify the template file ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"MySparseSolver\nint32 ii(?)\nint32 jj(?)\ndouble vv(?)\nint32 kk(?)\ndouble ff(?)\nint32 d()\ndouble u(?) -> output","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"The first line is the name of the operator. It should always be in the camel case. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"The 2nd to the 7th lines specify the input arguments, the signature is type+variable name+shape. For the shape, () corresponds to a scalar, (?) to a vector and (?,?) to a matrix. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"The last line is the output, denoted by -> output. Note there must be a space before and after ->. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"The following types are accepted: int32, int64, double, float, string, bool. The name of the arguments must all be in lower cases. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Step 2: Implement core codes","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Run customop() again and there will be CMakeLists.txt, gradtest.jl, MySparseSolver.cpp appearing in the current directory. MySparseSolver.cpp is the main wrapper for the codes and gradtest.jl is used for testing the operator and its gradients. CMakeLists.txt is the file for compilation. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Create a new file MySparseSolver.h and implement both the forward simulation and backward simulation (gradients)","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"#include <eigen3/Eigen/Sparse>\n#include <eigen3/Eigen/SparseLU>\n#include <vector>\n#include <iostream>\nusing namespace std;\ntypedef Eigen::SparseMatrix<double> SpMat; // declares a column-major sparse matrix type of double\ntypedef Eigen::Triplet<double> T;\n\nSpMat A;\n\nvoid forward(double *u, const int *ii, const int *jj, const double *vv, int nv, const int *kk, const double *ff,int nf,  int d){\n    vector<T> triplets;\n    Eigen::VectorXd rhs(d); rhs.setZero();\n    for(int i=0;i<nv;i++){\n      triplets.push_back(T(ii[i]-1,jj[i]-1,vv[i]));\n    }\n    for(int i=0;i<nf;i++){\n      rhs[kk[i]-1] += ff[i];\n    }\n    A.resize(d, d);\n    A.setFromTriplets(triplets.begin(), triplets.end());\n    auto C = Eigen::MatrixXd(A);\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(A);\n    solver.factorize(A);\n    auto x = solver.solve(rhs);\n    for(int i=0;i<d;i++) u[i] = x[i];\n}\n\nvoid backward(double *grad_vv, const double *grad_u, const int *ii, const int *jj, const double *u, int nv, int d){\n    Eigen::VectorXd g(d);\n    for(int i=0;i<d;i++) g[i] = grad_u[i];\n    auto B = A.transpose();\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(B);\n    solver.factorize(B);\n    auto x = solver.solve(g);\n    // cout << x << endl;\n    for(int i=0;i<nv;i++) grad_vv[i] = 0.0;\n    for(int i=0;i<nv;i++){\n      grad_vv[i] -= x[ii[i]-1]*u[jj[i]-1];\n    }\n}","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"In this implementation we have used Eigen library for solving sparse matrix. Other choices are also possible, such as algebraic multigrid methods. Note here for convenience we have created a global variable SpMat A;. This is not recommend if you want to run the code concurrently. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Step 3: Compile","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"It is recommended that you use the cmake, make and gcc provided by ADCME.  | Variable      | Description                           | | ––––––- | ––––––––––––––––––- | | ADCME.CXX   | C++ Compiler                          | | ADCME.CC    | C Compiler                            | | ADCME.TFLIB | libtensorflow_framework.so location | | ADCME.CMAKE | Cmake binary location                 | | ADCME.MAKE  | Make binary location                  |","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"A simple way is to set the environment by","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"export CC=<CC>\nexport CXX=<CXX>\nalias cmake=<CMAKE>\nalias make=<MAKE>","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"The values such as <CC> are obtained from the last table. Run the following command","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"mkdir build\ncd build\ncmake ..\nmake -j","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Based on your operation system, you will create libMySparseSolver.{so,dylib,dll}. This will be the dynamic library to link in TensorFlow. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Step 4: Test","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Finally, you could use gradtest.jl to test the operator and its gradients (specify appropriate data in gradtest.jl first). If you implement the gradients correctly, you will be able to obtain first order convergence for finite difference and second order convergence for automatic differentiation. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"(Image: custom_op)","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"If the process fails, it is most probability the GCC compiler is not compatible with which was used to compile libtensorflow_framework.{so,dylib}. In the Linux system, you can check the compiler using ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"readelf -p .comment libtensorflow_framework.so","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Compatibility issues are frustrating. We hope you can submit an issue to ADCME developers; we are happy to resolve the compatibility issue and improve the robustness of ADCME.","category":"page"},{"location":"customop/#Best-Practice-and-Caveats-1","page":"-","title":"Best Practice and Caveats","text":"","category":"section"},{"location":"customop/#Allocating-Memories-1","page":"-","title":"Allocating Memories","text":"","category":"section"},{"location":"customop/#","page":"-","title":"-","text":"Whenever memory is needed, one should allocate memory by TensorFlow context. ","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"Tensor* tmp_var = nullptr;\nTensorShae tmp_shape({10,10});\nOP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, tmp_shape, &tmp_var));","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"There are three methods to allocate Tensors when an Op kernel executes (details)","category":"page"},{"location":"customop/#","page":"-","title":"-","text":"allocate_persistent: if the memory is used between Op invocations.\nallocate_temp: if the memory is used only within Compute.\nallocate_output: if the memory will be used as output","category":"page"},{"location":"#ADCME-Documentation-1","page":"ADCME Documentation","title":"ADCME Documentation","text":"","category":"section"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"ADCME is suitable for conducting inverse modeling in scientific computing. The purpose of the package is to: (1) provide differentiable programming framework for scientific computing based on TensorFlow automatic differentiation (AD) backend; (2) adapt syntax to facilitate implementing scientific computing, particularly for numerical PDE discretization schemes; (3) supply missing functionalities in the backend (TensorFlow) that are important for engineering, such as sparse linear algebra, constrained optimization, etc. Applications include","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"full wavelength inversion\nreduced order modeling in solid mechanics\nlearning hidden geophysical dynamics\nphysics based machine learning\nparameter estimation in stochastic processes","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"The package inherents the scalability and efficiency from the well-optimized backend TensorFlow. Meanwhile, it provides access to incooperate existing C/C++ codes via the custom operators. For example, some functionalities for sparse matrices are implemented in this way and serve as extendable \"plugins\" for ADCME. ","category":"page"},{"location":"#Getting-Started-1","page":"ADCME Documentation","title":"Getting Started","text":"","category":"section"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"To install ADCME, use the following command:","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"using Pkg\nPkg.add(\"ADCME\")","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"to load the package, use","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"using ADCME","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"We consider a simple inverse modeling problem: consider the following partial differential equation -bu(x)+u(x)=f(x)quad xin01 u(0)=u(1)=0 where  f(x) = 8 + 4x - 4x^2 Assume that we have observed u(05)=1, we want to estimate b. The true value in this case should be b=1. We can discretize the system using finite difference method, and the resultant linear system will be (bA+I)bu = mathbff where A = beginbmatrix         frac2h^2  -frac1h^2  dots  0\n         -frac1h^2  frac2h^2  dots  0\n         dots \n         0  0  dots  frac2h^2     endbmatrix quad mathbfu = beginbmatrix         u_2\n        u_3\n        vdots\n        u_n     endbmatrix quad mathbff = beginbmatrix         f(x_2)\n        f(x_3)\n        vdots\n        f(x_n)     endbmatrix","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"The idea for implementing the inverse modeling method in ADCME is that making the unknown b a Variable and then solve the forward problem pretending b is known. The following code snippet shows the implementation","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"using LinearAlgebra\nusing ADCME\n\nn = 101 # number of grid nodes in [0,1]\nh = 1/(n-1)\nx = LinRange(0,1,n)[2:end-1]\n\nb = Variable(10.0) # we use Variable keyword to mark the unknowns\nA = diagm(0=>2/h^2*ones(n-2), -1=>-1/h^2*ones(n-3), 1=>-1/h^2*ones(n-3)) \nB = b*A + I  # I stands for the identity matrix\nf = @. 4*(2 + x - x^2) \nu = B\\f # solve the equation using built-in linear solver\nue = u[div(n+1,2)] # extract values at x=0.5\n\nloss = (ue-1.0)^2 \n\n# Optimization\nsess = Session(); init(sess) \nBFGS!(sess, loss)\n\nprintln(\"Estimated b = \", run(sess, b))","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"We obtained the expected output ","category":"page"},{"location":"#","page":"ADCME Documentation","title":"ADCME Documentation","text":"Estimated b = 0.9995582304494237","category":"page"},{"location":"julia_customop/#Julia-Custom-Operators-1","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"In scientific and engineering applications, the operators provided by TensorFlow are not sufficient for high performance computing. In addition, constraining oneself to TensorFlow environment sacrifices the powerful scientific computing ecosystems provided by other languages such as Julia and Python. For example, one might want to code a finite volume method for a sophisticated fluid dynamics problem; it is hard to have the flexible syntax to achieve this goal, obtain performance boost from existing fast solvers such as AMG, and benefit from many other third-party packages within TensorFlow. This motivates us to find a way to \"plugin\" custom operators to TensorFlow.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We have already introduced how to incooperate C++ custom operators.  For many researchers, they usually prototype the solvers in a high level language such as MATLAB, Julia or Python. To enjoy the parallelism and automatic differentiation feature of TensorFlow, they need to port them into C/C++. However, this is also cumbersome sometimes, espeically the original solvers depend on many packages in the high-level language. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We solve this problem by incorporating Julia functions directly into TensorFlow. That is, for any Julia functions, we can immediately convert it to a TensorFlow operator. At runtime, when this operator is executed, the corresponding Julia function is executed. That implies we have the Julia speed. Most importantly, the function is perfectly compitable with the native Julia environment; third-party packages, global variables, nested functions, etc. all work smoothly. Since Julia has the ability to call other languages in a quite elegant and simple manner, such as C/C++, Python, R, Java, this means it is possible to incoporate packages/codes from any supported languages into TensorFlow ecosystem. We need to point out that in TensorFlow, tf.numpy_function can be used to convert a Python function to a TensorFlow operator. However, in the runtime, the speed for this operator falls back to Python (or numpy operation for related parts). This is a drawback. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The key for implementing the mechanism is embedding Julia in C++. Still we need to create a C++ dynamic library for TensorFlow. However, the library is only an interface for invoking Julia code. At runtime, jl_get_function is called to search for the related function in the main module. C++ arrays, which include all the relavant data, are passed to this function through jl_call. It requires routine convertion from C++ arrays to Julia array interfaces jl_array_t*. However, those bookkeeping tasks are programatic and possibly will be automated in the future. Afterwards,Julia returns the result to C++ and thereafter the data are passed to the next operator. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"There are two caveats in the implementation. The first is that due to GIL of Python, we must take care of the thread lock while interfacing with Julia. This was done by putting a guard around th eJulia interface","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n// code here \nPyGILState_Release(py_threadstate);","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The second is the memory mangement of Julia arrays. This was done by defining gabage collection markers explicitly","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\nargs[0] = ...\nargs[1] = ...\n# do something\nJL_GC_POP();","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"This technique is remarkable and puts together one of the best langages in scientific computing and that in machine learning. The work that can be built on ADCME is enormous and significantly reduce the development time. ","category":"page"},{"location":"julia_customop/#Example-1","page":"Julia Custom Operators","title":"Example","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Here we present a simple example. Suppose we want to compute the Jacobian of a two layer neural network fracpartial ypartial x","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"y = W_2tanh(W_1x+b_1)+b_2","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"where x b_1 b_2 yin mathbbR^10, W_1 W_2in mathbbR^100. In TensorFlow, this can be done by computing the gradients fracpartial y_ipartial x for each i. In Julia, we can use ForwardDiff to do it automatically. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"function twolayer(J, x, w1, w2, b1, b2)\n    f = x -> begin\n        w1 = reshape(w1, 10, 10)\n        w2 = reshape(w2, 10, 10)\n        z = w2*tanh.(w1*x+b1)+b2\n    end\n    J[:] = ForwardDiff.jacobian(f, x)[:]\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"To make a custom operator, we first generate a wrapper","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"using ADCME\nmkdir(\"TwoLayer\")\ncd(\"TwoLayer\")\ncustomop()","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We modify custom_op.txt","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"TwoLayer\ndouble x(?)\ndouble w1(?)\ndouble b1(?)\ndouble w2(?)\ndouble b2(?)\ndouble y(?) -> output","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"and run ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"customop(;julia=true)","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Three files are generatedCMakeLists.txt, TwoLayer.cpp and gradtest.jl. Now create a new file TwoLayer.h","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"#include \"julia.h\"\n#include \"Python.h\"\n\nvoid forward(double *y, const double *x, const double *w1, const double *w2, const double *b1, const double *b2, int n){\n    PyGILState_STATE py_threadstate;\n    py_threadstate = PyGILState_Ensure();\n    jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\n    jl_value_t **args;\n    JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\n    args[0] = (jl_value_t*)jl_ptr_to_array_1d(array_type, y, n*n, 0);\n    args[1] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(x), n, 0);\n    args[2] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w1), n*n, 0);\n    args[3] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w2), n*n, 0);\n    args[4] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b1), n, 0);\n    args[5] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b2), n, 0);\n    auto fun = jl_get_function(jl_main_module, \"twolayer\");\n  \tif (fun==NULL) jl_errorf(\"Function not found in Main module.\");\n    else jl_call(fun, args, 6);\n    JL_GC_POP();\n    if (jl_exception_occurred())\n        printf(\"%s \\n\", jl_typeof_str(jl_exception_occurred()));\n    PyGILState_Release(py_threadstate);\n}","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Most of the codes have been explanined except jl_ptr_to_array_1d. This function generates a Julia array wrapper from C++ arrays. The last argument 0 indicates that Julia is not responsible for gabage collection. TwoLayer.cpp should also be modified according to https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Finally, we can test in gradtest.jl ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"two_layer = load_op(\"build/libTwoLayer\", \"two_layer\")\n\n\nw1 = rand(100)\nw2 = rand(100)\nb1 = rand(10)\nb2 = rand(10)\nx = rand(10)\nJ = rand(100)\ntwolayer(J, x, w1, w2, b1, b2)\n\ny = two_layer(constant(x), constant(w1), constant(b1), constant(w2), constant(b2))\nsess = Session(); init(sess)\nJ0 = run(sess, y)\n@show norm(J-J0)","category":"page"},{"location":"julia_customop/#Usage-in-a-Module-1","page":"Julia Custom Operators","title":"Usage in a Module","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"If the custom operator is intended to be used in a precompiled module, we can load the dynamic library at initialization","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"global my_op \nfunction __init__()\n\tglobal my_op = load_op(\"$(@__DIR__)/path/to/libMyOp\", \"my_op\")\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The corresponding Julia function called by my_op must be exported in the module (such that it is in the Main module when invoked). One such example is given in MyModule","category":"page"},{"location":"julia_customop/#Reference-Sheet-1","page":"Julia Custom Operators","title":"Reference Sheet","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"For implementation reference, see Reference Sheet","category":"page"},{"location":"four_types/#Forward-Operator-Types-1","page":"Forward Operator Types","title":"Forward Operator Types","text":"","category":"section"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"(Image: Operators)","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called input, x, and turns out a parameter called ouput, y=f(x). For reverse mode automatic differentiation, besides evaluating f(x), we need also to compute fracpartial Jpartial x given fracpartial Jpartial y where J is a functional of y. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"Note  the operator y=f(x) may be implicit in the sense that f is not given directly. In general, we can write the relationship between x and y as F(xy)=0. The operator is well-defined if for given x, there exists one and only one y such that F(xy)=0. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"For automatic differentiation, besides the well-definedness of F, we also require that we can compute fracpartial Jpartial x given fracpartial Jpartial y. It is easy to see that $ \\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}Fy^{-1}Fx $ Therefore, we call an operator F is well-posed if F_y^-1 exists. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"All operators can be classified into four types based on the linearity and explicitness","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"Linear and explicit This type of operators has the form  $ y = Ax $ where A is a matrix. In this case,  $ F(x,y) = Ax-y $ and therefore  $ \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}A $\nNonlinear and explicit In this case, we have  $ y = F(x) $ where F is explicitly given. We have $ F(x,y) = F(x)-y\\Rightarrow \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y} F_x(x) $\nLinear and implicit In this case  $ Ay = x $ We have F(xy) = x-Ay and  $ \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}A^{-1} $\nNonlinear and implicit\nIn this case F(xy)=0 and the corresponding gradient is  $ \\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}Fy^{-1}Fx $","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"In TensorFlow it is easy to implement linear/nonlinear and explicit operators and takes reasonable effor for linear and implicit operators. However, it is challenging to implement nonlinear and implicit method. We provide a solution by marrying PyTorch and TensorFlow here","category":"page"}]
}
