var documenterSearchIndex = {"docs":
[{"location":"pytorchnn/#Neural-Network-in-C-1","page":"Neural Network in C++","title":"Neural Network in C++","text":"","category":"section"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"In this section, we describe how we can implement a neural network in C++. This is useful when we want to create a custom operator in ADCME and a neural network is embedded in the operator (we cannot simply \"pass\" the neural network to the C++ backend). ","category":"page"},{"location":"pytorchnn/#PyTorch-1","page":"Neural Network in C++","title":"PyTorch","text":"","category":"section"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The first method is by using PyTorch C++ APIs.","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"We first need to download LibTorch source. Uncompress the library to your working directory. I have created a simple wrapper for some utility functions in ADCME. To use the wrapper, simply add la.h to your include directories.","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To create a neural network, the following self-explained C++ code can be used","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"struct Net : torch::nn::Module {\n  Net() {\n    fc1 = register_module(\"fc1\", torch::nn::Linear(3, 64));\n    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\n    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\n  }\n\n  torch::Tensor forward(torch::Tensor x) {\n    x = torch::tanh(fc1->forward(x));\n    x = torch::tanh(fc2->forward(x));\n    x = fc3->forward(x);\n    return x;\n  }\n\n  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\n};","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"note: Note\nTo create a linear layer with double precison, run fc1->to(torch::kDouble) after construction. ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Create a Neural Network","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"auto nn = std::make_shared<Net>();","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Evaluate an input","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"auto in = torch::rand({8,3},optf.requires_grad(true));\nauto out = nn->forward(in);","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Here we required gradients with respect to the input in and put optf.requires_grad(true) in the argument.","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Compute Gradients","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To compute gradients, we need to call backward of a scalar to populate the gradient entries. For example, assume our neural network model is y = f_theta(x) and we want to compute fracpartial ypartial x. In our case, x is a 8times 3 matrix (8 instances of data, each with 3 features). Each output is 10 dimensional. For each input x_iinmathbbR^3 and each output feature y_jinmathbbR, we want to compute   fracpartial y_jpartial x_iin mathbbR^3 For efficiency, we can compute the gradients of all batches simultaneously, i.e., for all i","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"auto t = out.sum(0);\nt[0].sum().backward();\nin.grad().fill_(0.0);","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"where we compute 8 vectors of mathbbR^3, i.e., in.grad() is a 8times 3 matrix (the same size as in). ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Access Neural Network Weights and Biases","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The neural network weights and biases can be assessed with ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"std::cout << nn->fc1->bias << std::endl;\nstd::cout << nn->fc1->weights << std::endl;","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"We can also manually set the weight values","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"nn->fc1->bias.set_data(torch::ones({64}));","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The grads can also be computed","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"std::cout <<  nn->fc1->weight.grad() << std::endl;","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Compile","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To compile the script, in CMakeLists.txt, we have","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"cmake_minimum_required(VERSION 3.5)\nproject(TorchExample)\n\nset(CMAKE_PREFIX_PATH libtorch)\nfind_package(Torch REQUIRED)\n\ninclude_directories(<path/to/la.h>)\nadd_executable(main main.cpp)\ntarget_link_libraries(main \"${TORCH_LIBRARIES}\")\nset_property(TARGET main PROPERTY CXX_STANDARD 11)","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"Full Script","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"#include \"la.h\"\n\nstruct Net : torch::nn::Module {\n  Net() {\n    fc1 = register_module(\"fc1\", torch::nn::Linear(3, 64));\n    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\n    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\n  }\n\n  torch::Tensor forward(torch::Tensor x) {\n    x = torch::tanh(fc1->forward(x));\n    x = torch::tanh(fc2->forward(x));\n    x = fc3->forward(x);\n    return x;\n  }\n\n  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\n};\n\n\n\nint main(){\n\n    auto nn = std::make_shared<Net>();\n\n    auto in = torch::rand({8,3},optf.requires_grad(true));\n    auto out = nn->forward(in);\n    \n    auto t = out.sum(0);\n    t[0].sum().backward();\n    in.grad().fill_(0.0);\n    std::cout << out << std::endl;\n\n    \n    std::cout << nn->fc1->bias << std::endl;\n    nn->fc1->bias.set_data(torch::ones({64}));\n    std::cout << nn->fc1->bias << std::endl;\n\n    std::cout << nn->fc1->weight << std::endl;\n    std::cout <<  nn->fc1->weight.grad() << std::endl;\n    \n    return 1;\n}","category":"page"},{"location":"pytorchnn/#Adept-2-1","page":"Neural Network in C++","title":"Adept-2","text":"","category":"section"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The second approach is to use a third-party automatic differentiation library. Here we use Adept-2. The idea is that we code a neural network in C++ using array operations. ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"To start with, download and compile Adept-2 with the following command","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"if [ ! -d \"Adept-2\" ]; then\n  git clone https://github.com/rjhogan/Adept-2\nfi\ncd Adept-2\nautoreconf -i\n./configure\nmake -j\nmake check\nmake install","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"All the libraries should be available in Adept-2/adept/.libs. The following script shows a simple neural network implementation","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"#include \"adept.h\"\n#include \"adept_arrays.h\"\n#include <iostream>\nusing namespace adept;\nusing namespace std;\n\nint main()\n{\n  Stack stack;\n  Array<2, double, true> X(100,3), W1(3,20), W2(20,20), W3(20,4);\n  Array<1, double, true> b1(20), b2(20), b3(4);\n  double V[400];\n  for(int i=0;i<300;i++) X[i] = 0.01*i;\n  for(int i=0;i<60;i++) W1[i] = 0.01*i;\n  for(int i=0;i<400;i++) W2[i] = 0.01*i;\n  for(int i=0;i<80;i++) W3[i] = 0.01*i;\n  for(int i=0;i<20;i++) b1[i] = 0.01*i;\n  for(int i=0;i<20;i++) b2[i] = 0.01*i;\n  for(int i=0;i<4;i++) b3[i] = 0.01*i;\n\n  stack.new_recording();\n  auto x = X**W1;\n  Array<2, double, true> y1(x.size(0),x.size(1));\n  for(int i=0;i<x.size(0);i++) \n    for(int j=0;j<x.size(1);j++)\n        y1(i,j) = tanh(x(i,j)+b1(j));\n    \n  auto w = y1**W2;\n  Array<2, double, true> y2(w.size(0),w.size(1));\n  for(int i=0;i<w.size(0);i++) \n    for(int j=0;j<w.size(1);j++)\n        y2(i,j) = tanh(w(i,j)+b2(j));\n\n  auto z = y2**W3;\n  Array<2, double, true> y3(z.size(0),z.size(1));\n  for(int i=0;i<z.size(0);i++) \n    for(int j=0;j<z.size(1);j++)\n        y3(i,j) = z(i,j)+b3(j);\n  \n  auto out = sum(y3, 0);\n  \n  out[0].set_gradient(1.0);\n  stack.compute_adjoint();\n  auto g1 = X.get_gradient();\n  cout << g1 << endl;\n\n  auto g2 = W3.get_gradient();\n  cout << g2 << endl;\n\n  out[1].set_gradient(1.0);\n  stack.compute_adjoint();\n  auto g1_ = X.get_gradient();\n  cout << g1_ << endl;\n\n  auto g2_ = W3.get_gradient();\n  cout << g2_ << endl;\n\n\n  y3(0,0).set_gradient(1.0);\n  stack.compute_adjoint();\n  auto g1__ = X.get_gradient();\n  cout << g1__ << endl;\n  auto g2__ = W3.get_gradient();\n  cout << g2__ << endl;\n}","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"The following codes might be useful ","category":"page"},{"location":"pytorchnn/#","page":"Neural Network in C++","title":"Neural Network in C++","text":"typedef Array<2, double, true> Array2D;\ntypedef Array<1, double, true> Array1D;\n\nvoid setv(Array1D& v,  const double *val){\n    int n = v.size(0);\n    for(int i=0;i<n;i++) v(i).set_value(val[i]);\n}\n\nvoid setv(Array2D& v, const double *val){\n    int k = 0;\n    int m = v.size(0), n = v.size(1);\n    for(int i=0;i<m;i++){\n        for(int j=0;j<n;j++)\n            v(i,j).set_value(val[k++]);\n    }\n}\n\nvoid getg(Array1D& v, double *val){\n    int n = v.size(0);\n    auto gv = v.get_gradient();\n    for(int i=0;i<n;i++) val[i] = value(gv(i));\n}\n\nvoid getg(Array2D& v, double *val){\n    int k = 0;\n    int m = v.size(0), n = v.size(1);\n    auto gv = v.get_gradient();\n    for(int i=0;i<m;i++){\n        for(int j=0;j<n;j++)\n            val[k++] = value(gv(i, j));\n    }\n}","category":"page"},{"location":"global/#Shared-Memory-Across-Kernels-1","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"","category":"section"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"In many use cases, we want to share data across multiple kernels. For example, if we want to design several custom operators for finite element analysis (e.g., one for assembling, one for solving the linear system and one for performing Newton's iteration), we might want to share the geometric data such as nodes and element connectivity matrices. This can be done by the share memory mechanism of dynamical shared libraries. ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Dynamical shared libraries have the following property: in Unix-like environments, shared libries export all extern global variables. That is, multiple shared libraries can change the same variable as long as the variable is marked as extern. However, extern variable itself is not a definition but only a declaration. The variable should be defined in one and only one shared library. ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Therefore, when we design custom operators and want to have global variables that will be reused by multiple custom kernels (each constitutes a separate dynamical shared library), we can link each of them to a \"data storage\" shared library. The \"data storage\" shared library should contain the definition of the global variable to be shared among those kernels. ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"(Image: )","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"As an example, consider we want to share Float64 vectors (with String keys). The data structure of the storage is given in Saver.h","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"#include <map>\n#include <string>\n#include <vector>\n\nstruct DataStore\n{        \n    std::map<std::string, std::vector<double>> vdata;\n};\nextern DataStore ds;","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Note we include extern DataStore ds; for convenience: we can include Saver.h for our custom operator kernels so that we have access to ds. ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Additionally, in Saver.cpp, we define ds","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"#include \"Saver.h\"\nDataStore ds;","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Now we can compile a dynamical shared library Saver.so (or Saver.dylib) with Saver.h and Saver.cpp. For all the other kernel implementation, we can include the header file Saver.h and link to Saver.so (or Saver.dylib) during compilation. ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"We show an example for storing, querying and deleting 10times 1 Float64 vectors with this technique. The main files are ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"SaverTensor.cpp","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"#include \"Saver.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/platform/default/logging.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include<cmath>\n#include<string> \n#include<eigen3/Eigen/Core>\nusing std::string;\nusing namespace tensorflow;\n\nREGISTER_OP(\"SaveTensor\")\n\n.Input(\"handle : string\")\n  .Input(\"val : double\")\n  .Output(\"out : string\")\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n    \n        shape_inference::ShapeHandle handle_shape;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle_shape));\n        shape_inference::ShapeHandle val_shape;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &val_shape));\n\n        c->set_output(0, c->Scalar());\n    return Status::OK();\n  });\n\nclass SaveTensorOp : public OpKernel {\nprivate:\n  \npublic:\n  explicit SaveTensorOp(OpKernelConstruction* context) : OpKernel(context) {\n\n  }\n\n  void Compute(OpKernelContext* context) override {    \n    DCHECK_EQ(2, context->num_inputs());\n    \n    \n    const Tensor& handle = context->input(0);\n    const Tensor& val = context->input(1);\n    \n    \n    const TensorShape& val_shape = val.shape();\n    \n    \n    DCHECK_EQ(val_shape.dims(), 1);\n\n    // extra check\n        \n    // create output shape\n    \n    TensorShape out_shape({});\n            \n    // create output tensor\n    \n    Tensor* out = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &out));\n    \n    // get the corresponding Eigen tensors for data access\n    auto handle_tensor = handle.flat<string>().data();\n    auto val_tensor = val.flat<double>().data();\n    auto out_tensor = out->flat<string>().data();   \n\n    // implement your forward function here \n    // context->tensors_[string(*handle_tensor)] = val;\n    ds.vdata[string(*handle_tensor)] = std::vector<double>(val_tensor, val_tensor+10);\n    *out_tensor = *handle_tensor;    \n    printf(\"Adding %s to collections.\\n\", string(*handle_tensor).c_str());\n    printf(\"\\n========Existing Keys========\\n\");\n    for(auto & kv: ds.vdata){\n      printf(\"Key %s\\n\", kv.first.c_str());\n    }\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"SaveTensor\").Device(DEVICE_CPU), SaveTensorOp);","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"GetTensor.cpp","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"#include \"Saver.h\"\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/platform/default/logging.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include<cmath>\n#include<string> \n#include<map>\n#include<eigen3/Eigen/Core>\nusing std::string;\n\nusing namespace tensorflow;\n\nREGISTER_OP(\"GetTensor\")\n.Input(\"handle : string\")\n  .Output(\"val : double\")\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n    \n        shape_inference::ShapeHandle handle_shape;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle_shape));\n\n        c->set_output(0, c->Vector(-1));\n    return Status::OK();\n  });\n\nclass GetTensorOp : public OpKernel {\nprivate:\n  \npublic:\n  explicit GetTensorOp(OpKernelConstruction* context) : OpKernel(context) {\n\n  }\n\n  void Compute(OpKernelContext* context) override {    \n    DCHECK_EQ(1, context->num_inputs());\n    \n    const Tensor& handle = context->input(0);    \n    auto handle_tensor = handle.flat<string>().data();\n\n    auto val_shape = TensorShape({10});   \n    Tensor *val = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, val_shape, &val));\n\n    if (!ds.vdata.count(string(*handle_tensor))){\n        printf(\"[Get] Key %s does not exist.\\n\", string(*handle_tensor).c_str());\n    }\n    else{\n      printf(\"[Get] Key %s exists.\\n\", string(*handle_tensor).c_str());\n      auto v = ds.vdata[string(*handle_tensor)];\n      for(int i=0;i<10;i++){\n        val->flat<double>().data()[i] = v[i];\n      }\n    }\n    printf(\"\\n========Existing Keys========\\n\");\n    for(auto & kv: ds.vdata){\n      printf(\"Key %s\\n\", kv.first.c_str());\n    }\n    \n\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"GetTensor\").Device(DEVICE_CPU), GetTensorOp);","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"DeleteTensor.cpp","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"#include \"Saver.h\"\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/platform/default/logging.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include<cmath>\n#include<string> \n#include<map>\n#include<eigen3/Eigen/Core>\nusing std::string;\n\nusing namespace tensorflow;\n\nREGISTER_OP(\"DeleteTensor\")\n.Input(\"handle : string\")\n  .Output(\"val : bool\")\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n    \n        shape_inference::ShapeHandle handle_shape;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle_shape));\n\n        c->set_output(0, c->Scalar());\n    return Status::OK();\n  });\n\nclass DeleteTensorOp : public OpKernel {\nprivate:\n  \npublic:\n  explicit DeleteTensorOp(OpKernelConstruction* context) : OpKernel(context) {\n\n  }\n\n  void Compute(OpKernelContext* context) override {    \n    DCHECK_EQ(1, context->num_inputs());\n    \n    const Tensor& handle = context->input(0);    \n    auto handle_tensor = handle.flat<string>().data();\n\n    auto val_shape = TensorShape({});   \n    Tensor *val = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, val_shape, &val));\n\n    if (ds.vdata.count(string(*handle_tensor))){\n      ds.vdata.erase(string(*handle_tensor));\n      printf(\"[Delete] Erase key %s.\\n\", string(*handle_tensor).c_str());\n      *(val->flat<bool>().data()) = true;\n    }\n    else{\n      printf(\"[Delete] Key %s does not exist.\\n\", string(*handle_tensor).c_str());\n      *(val->flat<bool>().data()) = false;\n    }\n    printf(\"\\n========Existing Keys========\\n\");\n    for(auto & kv: ds.vdata){\n      printf(\"Key %s\\n\", kv.first.c_str());\n    }\n  }\n};\nREGISTER_KERNEL_BUILDER(Name(\"DeleteTensor\").Device(DEVICE_CPU), DeleteTensorOp);","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Here is part of the CMakeLists.txt used for compilation, where we link XXTensor.cpp with Saver","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"add_library(Saver SHARED Saver.cpp)\nset_property(TARGET Saver PROPERTY POSITION_INDEPENDENT_CODE ON)\n\nadd_library(SaveTensor SHARED SaveTensor.cpp)\nset_property(TARGET SaveTensor PROPERTY POSITION_INDEPENDENT_CODE ON)\ntarget_link_libraries(SaveTensor ${TF_LIB_FILE} Saver)\n\nadd_library(GetTensor SHARED GetTensor.cpp)\nset_property(TARGET GetTensor PROPERTY POSITION_INDEPENDENT_CODE ON)\ntarget_link_libraries(GetTensor ${TF_LIB_FILE} Saver)\n\nadd_library(DeleteTensor SHARED DeleteTensor.cpp)\nset_property(TARGET DeleteTensor PROPERTY POSITION_INDEPENDENT_CODE ON)\ntarget_link_libraries(DeleteTensor ${TF_LIB_FILE} Saver)","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"We can test our implementation with ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"using ADCME\n\nsave_tensor = load_op_and_grad(\"./build/libSaveTensor\",\"save_tensor\")\nget_tensor = load_op_and_grad(\"./build/libGetTensor\",\"get_tensor\")\ndelete_tensor = load_op_and_grad(\"./build/libDeleteTensor\",\"delete_tensor\")\n\nval = constant(rand(10))\nt1 = tf.constant(\"tensor1\")\nt2 = tf.constant(\"tensor2\")\nt3 = tf.constant(\"tensor3\")\nu1 = save_tensor(t1,val)\nu2 = save_tensor(t2,2*val)\nu3 = save_tensor(t3,3*val)\n\nz1 = get_tensor(t1);\nz2 = get_tensor(t2);\nz3 = get_tensor(t3);\n\nd1 = delete_tensor(t1);\nd2 = delete_tensor(t2);\nd3 = delete_tensor(t3);\nsess = Session(); \nrun(sess, [u1,u2,u3])\n\n\nrun(sess, z1)\nrun(sess, z2)\nrun(sess, z3)\nrun(sess, d2)","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"The expected output is ","category":"page"},{"location":"global/#","page":"Shared Memory Across Kernels","title":"Shared Memory Across Kernels","text":"Adding tensor3 to collections.\n\n========Existing Keys========\nKey tensor1\nKey tensor3\nAdding tensor2 to collections.\n\n========Existing Keys========\nKey tensor1\nKey tensor2\nKey tensor3\nAdding tensor1 to collections.\n\n========Existing Keys========\nKey tensor1\nKey tensor2\nKey tensor3\n[Get] Key tensor1 exists.\n\n========Existing Keys========\nKey tensor1\nKey tensor2\nKey tensor3\n[Get] Key tensor2 exists.\n\n========Existing Keys========\nKey tensor1\nKey tensor2\nKey tensor3\n[Get] Key tensor3 exists.\n\n========Existing Keys========\nKey tensor1\nKey tensor2\nKey tensor3\n[Delete] Erase key tensor2.\n\n========Existing Keys========\nKey tensor1\nKey tensor3","category":"page"},{"location":"customop/#Custom-Operators-1","page":"Custom Operators","title":"Custom Operators","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nAs a reminder, there are many built-in custom operators in deps/CustomOps and they are good resources for understanding custom operators. The following is a step-by-step instruction on how custom operators are implemented. ","category":"page"},{"location":"customop/#CPU-Operators-1","page":"Custom Operators","title":"CPU Operators","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Custom operators are ways to add missing features in ADCME. Typically users do not have to worry about custom operators. However, in the following situation custom opreators might be very useful","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Direct implementation in ADCME is inefficient (bottleneck). \nThere are legacy codes users want to reuse, such as GPU-accelerated codes. \nSpecial acceleration techniques such as checkpointing scheme. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In the following, we present an example of implementing a sparse solver for Au=b as a custom operator.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Input: row vector ii, column vectorjj and value vector vv for the sparse coefficient matrix A; row vector kk and value vector ff for the right hand side b; the coefficient matrix dimension is dtimes d","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Output: solution vector uin mathbbR^d","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 1: Create and modify the template file","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The following command helps create the wrapper","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"customop()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There will be a custom_op.txt in the current directory. Modify the template file ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"MySparseSolver\nint32 ii(?)\nint32 jj(?)\ndouble vv(?)\nint32 kk(?)\ndouble ff(?)\nint32 d()\ndouble u(?) -> output","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The first line is the name of the operator. It should always be in the camel case. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The 2nd to the 7th lines specify the input arguments, the signature is type+variable name+shape. For the shape, () corresponds to a scalar, (?) to a vector and (?,?) to a matrix. The variable names must be in lower cases. Additionally, the supported types are: int32, int64, float, double, bool and string. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The last line is the output, denoted by -> output (do not forget the whitespace before and after ->).  ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\n","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"If there are non-real type outputs, the corresponding top gradients input to the gradient kernel should be removed.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 2: Implement the kernels","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Run customop() again and there will be CMakeLists.txt, gradtest.jl, MySparseSolver.cpp appearing in the current directory. MySparseSolver.cpp is the main wrapper for the codes and gradtest.jl is used for testing the operator and its gradients. CMakeLists.txt is the file for compilation. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Create a new file MySparseSolver.h and implement both the forward simulation and backward simulation (gradients)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"#include <eigen3/Eigen/Sparse>\n#include <eigen3/Eigen/SparseLU>\n#include <vector>\n#include <iostream>\nusing namespace std;\ntypedef Eigen::SparseMatrix<double> SpMat; // declares a column-major sparse matrix type of double\ntypedef Eigen::Triplet<double> T;\n\nSpMat A;\n\nvoid forward(double *u, const int *ii, const int *jj, const double *vv, int nv, const int *kk, const double *ff,int nf,  int d){\n    vector<T> triplets;\n    Eigen::VectorXd rhs(d); rhs.setZero();\n    for(int i=0;i<nv;i++){\n      triplets.push_back(T(ii[i]-1,jj[i]-1,vv[i]));\n    }\n    for(int i=0;i<nf;i++){\n      rhs[kk[i]-1] += ff[i];\n    }\n    A.resize(d, d);\n    A.setFromTriplets(triplets.begin(), triplets.end());\n    auto C = Eigen::MatrixXd(A);\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(A);\n    solver.factorize(A);\n    auto x = solver.solve(rhs);\n    for(int i=0;i<d;i++) u[i] = x[i];\n}\n\nvoid backward(double *grad_vv, const double *grad_u, const int *ii, const int *jj, const double *u, int nv, int d){\n    Eigen::VectorXd g(d);\n    for(int i=0;i<d;i++) g[i] = grad_u[i];\n    auto B = A.transpose();\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(B);\n    solver.factorize(B);\n    auto x = solver.solve(g);\n    // cout << x << endl;\n    for(int i=0;i<nv;i++) grad_vv[i] = 0.0;\n    for(int i=0;i<nv;i++){\n      grad_vv[i] -= x[ii[i]-1]*u[jj[i]-1];\n    }\n}","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nIn this implementation we have used Eigen library for solving sparse matrix. Other choices are also possible, such as algebraic multigrid methods. Note here for convenience we have created a global variable SpMat A;. This is not recommend if you want to run the code concurrently, since the variable A must be overwritten by another concurrent thread. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 3: Compile","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"It is recommended that you use the cmake, make and gcc provided by ADCME. The binary locations can be found via","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Variable Description\nADCME.CXX C++ Compiler\nADCME.CC C Compiler\nADCME.TFLIB libtensorflow_framework.so location\nADCME.CMAKE Cmake binary location\nADCME.MAKE Make binary location","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Make a build directory in bash.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"mkdir build\ncd build","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Configure CMakeLists.txt files.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"cmake ..","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"or use a safer way ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"julia> using ADCME\njulia> ADCME.cmake()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"This step requires Conda and PyCall be properly installed. Try the following if necessary","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"julia> pkg\npkg> add Conda\npkg> add PyCall","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Build. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"make -j","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nIf the system make or cmake command is not compatible, try the pre-installed ADCME make located at ADCME.MAKE or cmake located at ADCME.CMAKE. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Based on your operation system, you will create libMySparseSolver.{so,dylib,dll}. This will be the dynamic library to link in TensorFlow. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 4: Test","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Finally, you could use gradtest.jl to test the operator and its gradients (specify appropriate data in gradtest.jl first). If you implement the gradients correctly, you will be able to obtain first order convergence for finite difference and second order convergence for automatic differentiation. Note you need to modify this file first, e.g., creating data and modifying the function scalar_function. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"(Image: custom_op)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"info: Info\nIf the process fails, it is most probable the GCC compiler is not compatible with which was used to compile libtensorflow_framework.{so,dylib}. Using the built-in cmake and make will alleviate this problem in most cases. In the Linux system, you can check the compiler using ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"readelf -p .comment libtensorflow_framework.so","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Compatibility issues are frustrating. We hope you can submit an issue to ADCME developers; we are happy to resolve the compatibility issue and improve the robustness of ADCME.","category":"page"},{"location":"customop/#GPU-Operators-1","page":"Custom Operators","title":"GPU Operators","text":"","category":"section"},{"location":"customop/#Dependencies-1","page":"Custom Operators","title":"Dependencies","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To create a GPU custom operator, you must have an NVCC compiler and a CUDA toolkit installed on your system. To install NVCC, see the installation guide. To check you have successfully installed NVCC, type","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"which nvcc","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"It should gives you the location of nvcc compiler. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"For quick installation of other dependencies, you can try","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"ENV[\"GPU\"] = 1\nPkg.build(\"ADCME\")","category":"page"},{"location":"customop/#Manual-Installation-1","page":"Custom Operators","title":"Manual Installation","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In case ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To install CUDA toolkit (if you do not have one), you can install via conda","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using Conda\nConda.add(\"cudatoolkit\", channel=\"anaconda\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The next step is to cp the CUDA include file to tensorflow include directory. This could be done with ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using ADCME\ngpus = joinpath(splitdir(tf.__file__)[1], \"include/third_party/gpus\")\nif !isdir(gpus)\n  mkdir(gpus)\nend\ngpus = joinpath(gpus, \"cuda\")\nif !isdir(gpus)\n  mkdir(gpus)\nend\nincpath = joinpath(splitdir(strip(read(`which nvcc`, String)))[1], \"../include/\")\nif !isdir(joinpath(gpus, \"include\"))\n    mv(incpath, gpus)\nend","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Finally, add the CUDA library path to LD_LIBRARY_PATH. This can be done by adding the following line to .bashrc","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"export LD_LIBRARY_PATH=<path>:$LD_LIBRARY_PATH","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"where <path> is ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"joinpath(Conda.ROOTENV, \"pkgs/cudatoolkit-10.1.168-0/lib/\")","category":"page"},{"location":"customop/#File-Organization-1","page":"Custom Operators","title":"File Organization","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There should be three files in your source directories","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"MyOp.cpp: driver file\nMyOp.cu: GPU implementation\nMyOp.h: CPU implementation","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The first two files have been generated for you by customop(). The following are two important notes on the implementation.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In MyOp.cu, the implementation usually has the structure","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"namespace tensorflow{\n  typedef Eigen::GpuDevice GPUDevice;\n\n    __global__ void forward_(const int nthreads, double *out, const double *y, const double *H0, int n){\n      for(int i : CudaGridRangeX(nthreads)) {\n          // do something here\n      }\n    }\n\n    void forwardGPU(double *out, const double *y, const double *H0, int n, const GPUDevice& d){\n      // forward_<<<(n+255)/256, 256>>>(out, y, H0, n);\n      GpuLaunchConfig config = GetGpuLaunchConfig(n, d);\n      TF_CHECK_OK(GpuLaunchKernel(\n          forward_, config.block_count, config.thread_per_block, 0,\n          d.stream(), config.virtual_thread_count, out, y, H0, n));\n      }\n}","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In MyOp.cpp, the device information (const GPUDevice& d above) is obtained with ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"context->eigen_device<GPUDevice>()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"info: Info\nfor(int i : CudaGridRangeX(nthreads)) is interpreted as for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < nthreads; index += blockDim.x * gridDim.x)and the kernel launch semantic is equivalent to forward_<<<config.block_count, config.thread_per_block, 0,\n                            d.stream()>>>(config.virtual_thread_count,\n                                          \t\t\tout, y, H0, n);","category":"page"},{"location":"customop/#Miscellany-1","page":"Custom Operators","title":"Miscellany","text":"","category":"section"},{"location":"customop/#Mutable-Inputs-1","page":"Custom Operators","title":"Mutable Inputs","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Sometimes we want to modify tensors in place. In this case we can use mutable inputs. Mutable inputs must be Variable and it must be forwarded to one of the output. We consider implement a my_assign operator, with signature","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"my_assign(u::PyObject, v::PyObject)::PyObject","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Here u is a Variable and we copy the data from v to u. In the MyAssign.cpp file, we modify the input and output specifications to ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":".Input(\"u : Ref(double)\")\n.Input(\"v : double\")\n.Output(\"w : Ref(double)\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In addition, the input tensor is obtained through","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Tensor u = context->mutable_input(0, true);","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The second argument lock_held specifies whether the input mutex is acquired (false) before the operation. Note the output must be a Tensor instead of a reference. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To forward the input, use","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"context->forward_ref_input_to_ref_output(0,0);","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"We use the following code snippet to test the program","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"my_assign = load_op(\"./build/libMyAssign\",\"my_assign\")\nu = Variable([0.1,0.2,0.3])\nv = constant(Array{Float64}(1:3))\nu2 = u^2\nw = my_assign(u,v)\nsess = tf.Session()\ninit(sess)\n@show run(sess, u)\n@show run(sess, u2)\n@show run(sess, w)\n@show run(sess, u2)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The output is ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"[0.1,0.2,0.3]\n[0.1,0.04,0.09]\n[1.0,2.0,3.0]\n[1.0,4.0,9.0]","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"We can see that the tensors depending on u are also aware of the assign operator. The complete programs can be downloaded here: CMakeLists.txt, MyAssign.cpp, gradtest.jl.","category":"page"},{"location":"customop/#Third-party-Plugins-1","page":"Custom Operators","title":"Third-party Plugins","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"ADCME also allows third-party custom operators hosted on Github. To build your own custom operators, implement your own custom operators in a Github repository. The root directory of the repository should have the following files","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"formula.txt, which tells how ADCME should interact with the custom operator. It is a Julia Pair, which has the format\nsignature => (source_directory, library_name, signature, has_gradient)\nFor example\n\"ot_network\"=>(\"OTNetwork\", \"libOTNetwork\", \"ot_network\", true)\nCMakeLists.txt, which is used for compiling the library. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Users are free to arrange other source files or other third-party libraries. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Upon using those libraries in ADCME, users first download those libraries to deps directory via","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"install(\"https://github.com/ADCMEMarket/OTNetwork\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The official plugins are hosted on https://github.com/ADCMEMarket. To get access to the custom operators in ADCME, use","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"op = load_system_op(\"OTNetwork\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"https://on-demand.gputechconf.com/ai-conference-2019/T1-3Minseok%20LeeAdding%20custom%20CUDA%20C++%20Operations%20in%20Tensorflow%20for%20boosting%20BERT%20Inference.pdf)","category":"page"},{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-Julia-Custom-Operator-in-ADCMAE-1","page":"-","title":"Quick Reference for Implementing Julia Custom Operator in ADCMAE","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Header files","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"  #include \"julia.h\"\n  #include \"Python.h\"","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"For Python GIL workround","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n...\nPyGILState_Release(py_threadstate);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Get function from Julia main module ","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_get_function(jl_main_module, \"myfun\");","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ to Julia","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t *a = jl_box_float64(3.0);\njl_value_t *b = jl_box_float32(3.0f);\njl_value_t *c = jl_box_int32(3);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia to C++","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double ret_unboxed = jl_unbox_float64(ret);\nfloat  ret_unboxed = jl_unbox_float32(ret);\nint32  ret_unboxed = jl_unbox_int32(ret);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ Arrays to Julia Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\njl_array_t* x          = jl_alloc_array_1d(array_type, 10);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"or for existing arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *existingArray = (double*)malloc(sizeof(double)*10);\njl_array_t *x = jl_ptr_to_array_1d(array_type, existingArray, 10, 0);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia Arrays to C++ Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *xData = (double*)jl_array_data(x);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Call Julia Functions","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_array_t *y = (jl_array_t*)jl_call1(func, (jl_value_t*)x);\njl_value_t *jl_call(jl_function_t *f, jl_value_t **args, int32_t nargs)","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Gabage collection","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 2); // args can now hold 2 `jl_value_t*` objects\nargs[0] = some_value;\nargs[1] = some_other_value;\n// Do something with args (e.g. call jl_... functions)\nJL_GC_POP();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Reference: Embedding Julia","category":"page"},{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1","page":"-","title":"Quick Reference for Implementing C++ Custom Operators in ADCME","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Set output shape","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"c->set_output(0, c->Vector(n));\nc->set_output(0, c->Matrix(m, n));\nc->set_output(0, c->Scalar());","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Names","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":".Input and .Ouput : names must be in lower case, no _, only letters.","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"TensorFlow Input/Output to TensorFlow Tensors","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.vec<double>();\ngrad.scalar<double>();\ngrad.matrix<double>();\ngrad.flat<double>();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Obtain flat arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.flat<double>().data()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Scalars","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate scalars using TensorShape()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate Shapes","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Although you can use -1 for shape reference, you must allocate exact shapes in Compute","category":"page"},{"location":"extra/#Miscellaneous-Tools-1","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"There are many handy tools implemented in ADCME for analysis, benchmarking, input/output, etc. ","category":"page"},{"location":"extra/#Debugging-and-Printing-1","page":"Miscellaneous Tools","title":"Debugging and Printing","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Add the following line before Session and change tf.Session to see verbose printing (such as GPU/CPU information)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.debugging.set_log_device_placement(true)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.print can be used for printing tensor values. It must be binded with an executive operator.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"# a, b are tensors, and b is executive\nop = tf.print(a)\nb = bind(b, op)","category":"page"},{"location":"extra/#Debugging-Python-Codes-1","page":"Miscellaneous Tools","title":"Debugging Python Codes","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"If the error comes from Python (through PyCall), we can print out the Python trace with the following commands","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"import traceback\ntry:\n    # Your codes here \nexcept Exception:\n    print(traceback.format_exc())","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"This Python script can be inserted to Julia and use interpolation to invoke Julia functions (in the comment line).","category":"page"},{"location":"extra/#Profiling-1","page":"Miscellaneous Tools","title":"Profiling","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Profiling can be done with the help of run_profile and save_profile","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"a = normal(2000, 5000)\nb = normal(5000, 1000)\nres = a*b \nrun_profile(sess, res)\nsave_profile(\"test.json\")","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Open Chrome and navigate to chrome://tracing\nLoad the timeline file","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Below shows an example of profiling results.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"(Image: )","category":"page"},{"location":"extra/#Save-and-Load-Python-Object-1","page":"Miscellaneous Tools","title":"Save and Load Python Object","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"psave\npload","category":"page"},{"location":"extra/#ADCME.psave","page":"Miscellaneous Tools","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.pload","page":"Miscellaneous Tools","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-Diary-1","page":"Miscellaneous Tools","title":"Save and Load Diary","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"We can use TensorBoard to track a scalar value easily","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"d = Diary(\"test\")\np = placeholder(1.0, dtype=Float64)\nb = constant(1.0)+p\ns = scalar(b, \"variable\")\nfor i = 1:100\n    write(d, i, run(sess, s, Dict(p=>Float64(i))))\nend\nactivate(d)","category":"page"},{"location":"apps_ad/#Intelligent-Automatic-Differentiation-1","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"","category":"section"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"Kailai Xu, Dongzhuo Li, Eric Darve, and Jerry M. Harris. \"Learning Hidden Dynamics using Intelligent Automatic Differentiation\"","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"Dongzhuo Li, Kailai Xu, Jerry M. Harris, and Eric Darve. \"Time-lapse Full Waveform Inversion for Subsurface Flow Problems with Intelligent Automatic Differentiation\"","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"Project Website","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"We treat physical simulations as a chain of multiple differentiable operators, such as discrete Laplacian evaluation, a Poisson solver and a single implicit time stepping for nonlinear PDEs. They are like building blocks that can be assembled to make simulation tools for new physical models.","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"Those operators are differentiable and integrated in a computational graph so that the gradients can be computed automatically and efficiently via analyzing the dependency in the graph. Independent operators are parallelized executed. With the gradients we can perform gradient-based PDE-constrained optimization for inverse problems.","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"(Image: )","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"This view of numerical simulation enables us to develope very sophisticated tools for inverse modeling: we decouple the individual operators and implement a forward/backward for each of them; they are consolidated using ADCME to create a computational graph. The computational dependency is then parsed and gradients are automatically computed based on the dependency. For example, in this work, we coupled multiphysics and obtain the gradients of the objective function with respect to the hidden dynamics parameters (i.e., permeability). This can be quite time-consuming and error-prone if we are going to derive the gradients by hand, implement and debug. With ADCME, we \"chain\" all the operators and the gradients are obtained automatically. ","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"(Image: )","category":"page"},{"location":"apps_ad/#","page":"Intelligent Automatic Differentiation","title":"Intelligent Automatic Differentiation","text":"We call this technique intelligent automatic differentiation, since we can design our own operators for performance and those operators are submodules that can be flexibly replaced and reused. For more details, see FwiFlow.jl, a package focused on elastic full waveform inversion for subsurface flow problems.","category":"page"},{"location":"ode/#PDE/ODE-Solvers-1","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"","category":"section"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"info: Info\nThe PDE/ODE solver features are currently under heavy development. We aim to provide a complete set of built-in PDE/ODE solvers.","category":"page"},{"location":"ode/#Runge-Kutta-Method-1","page":"PDE/ODE Solvers","title":"Runge Kutta Method","text":"","category":"section"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"The Runge Kutta method is one of the workhorses for solving ODEs. The method is a higher order interpolation to the derivatives. The system of ODE has the form","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"fracdydt = f(y t theta)","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"where t denotes time, y denotes states and theta denotes parameters. ","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"The Runge-Kutta method is defined as","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"beginaligned\nk_1 = Delta t f(t_n y_n theta)\nk_2 = Delta t f(t_n+Delta t2 y_n + k_12 theta)\nk_3 = Delta t f(t_n+Delta t2 y_n + k_22 theta)\nk_4 = Delta t f(t_n+Delta t y_n + k_3 theta)\ny_n+1 = y_n + frack_16 +frack_23 +frack_33 +frack_46\nendaligned","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"ADCME provides a built-in Runge Kutta solver rk4 and ode45. Consider an example: the Lorentz equation","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"beginaligned\nfracdxdt = 10(y-x) \nfracdydt = x(27-z)-y \nfracdzdt = xy -frac83z\nendaligned","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"Let the initial condition be x_0 = 100, the following code snippets solves the Lorentz equation with ADCME","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"function f(t, y, θ)\n    [10*(y[2]-y[1]);y[1]*(27-y[3])-y[2];y[1]*y[2]-8/3*y[3]]\nend\nx0 = [1.;0.;0.]\nrk4(f, 30.0, 10000, x0)","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"(Image: )","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"We can also solve three body problem with the Runge-Kutta method. The full script is ","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"# \n# adapted from \n# https://github.com/pjpmarques/Julia-Modeling-the-World/\n# \nusing Revise\nusing ADCME\nusing PyPlot\nusing Printf\n\nfunction f(t, y, θ)\n    # Extract the position and velocity vectors from the g array\n    r0, v0 = y[1:2], y[3:4]\n    r1, v1 = y[5:6], y[7:8]\n    r2, v2 = y[9:10], y[11:12]\n    \n    # The derivatives of the position are simply the velocities\n    dr0 = v0\n    dr1 = v1\n    dr2 = v2\n    \n    # Now calculate the the derivatives of the velocities, which are the accelarations\n    # Start by calculating the distance vectors between the bodies (assumes m0, m1 and m2 are global variables)\n    # Slightly rewriten the expressions dv0, dv1 and dv2 comprared to the normal equations so we can reuse d0, d1 and d2\n    d0  = (r2 - r1) / ( norm(r2 - r1)^3.0 )\n    d1  = (r0 - r2) / ( norm(r0 - r2)^3.0 )\n    d2  = (r1 - r0) / ( norm(r1 - r0)^3.0 )    \n    \n    dv0 = m1*d2 - m2*d1\n    dv1 = m2*d0 - m0*d2\n    dv2 = m0*d1 - m1*d0\n    \n    # Reconstruct the derivative vector\n    [dr0; dv0; dr1; dv1; dr2; dv2]\nend\n\nfunction plot_trajectory(t1, t2)\n\n    t1i = round(Int,NT * t1/T) + 1\n    t2i = round(Int,NT * t2/T) + 1\n    \n    # Plot the initial and final positions\n    # In these vectors, the first coordinate will be X and the second Y\n    X = 1\n    Y = 2\n    \n    # figure(figsize=(6,6))\n    plot(r0[t1i,X], r0[t1i,Y], \"ro\")\n    plot(r0[t2i,X], r0[t2i,Y], \"rs\")\n    plot(r1[t1i,X], r1[t1i,Y], \"go\")\n    plot(r1[t2i,X], r1[t2i,Y], \"gs\")\n    plot(r2[t1i,X], r2[t1i,Y], \"bo\")\n    plot(r2[t2i,X], r2[t2i,Y], \"bs\")\n    \n    # Plot the trajectories\n    plot(r0[t1i:t2i,X], r0[t1i:t2i,Y], \"r-\")\n    plot(r1[t1i:t2i,X], r1[t1i:t2i,Y], \"g-\")\n    plot(r2[t1i:t2i,X], r2[t1i:t2i,Y], \"b-\")\n    \n    # Plot cente of mass\n    # plot(cx[t1i:t2i], cy[t1i:t2i], \"kx\")\n    \n    # Setup the axis and titles\n    xmin = minimum([r0[t1i:t2i,X]; r1[t1i:t2i,X]; r2[t1i:t2i,X]]) * 1.10\n    xmax = maximum([r0[t1i:t2i,X]; r1[t1i:t2i,X]; r2[t1i:t2i,X]]) * 1.10\n    ymin = minimum([r0[t1i:t2i,Y]; r1[t1i:t2i,Y]; r2[t1i:t2i,Y]]) * 1.10\n    ymax = maximum([r0[t1i:t2i,Y]; r1[t1i:t2i,Y]; r2[t1i:t2i,Y]]) * 1.10\n    \n    axis([xmin, xmax, ymin, ymax])\n    title(@sprintf \"3-body simulation for t=[%.1f .. %.1f]\" t1 t2)\nend;\n\nm0 = 5.0\nm1 = 4.0\nm2 = 3.0\n\n# Initial positions and velocities of each body (x0, y0, vx0, vy0) \ngi0 = [ 1.0; -1.0; 0.0; 0.0]\ngi1 = [ 1.0;  3.0; 0.0; 0.0]\ngi2 = [-2.0; -1.0; 0.0; 0.0]\n\n\nT  = 30.0\nNT  = 500*300\ng0  = [gi0; gi1; gi2]\n\nres_ = ode45(f, T, NT, g0)\n\nsess = Session(); init(sess)\nres = run(sess, res_)\n\nr0, v0, r1, v1, r2, v2 = res[:,1:2], res[:,3:4], res[:,5:6], res[:,7:8], res[:,9:10], res[:,11:12]\n\nfigure(figsize=[4,1])\nsubplot(131); plot_trajectory(0.0,10.0)\nsubplot(132); plot_trajectory(10.0,20.0)\nsubplot(133); plot_trajectory(20.0,30.0)","category":"page"},{"location":"ode/#","page":"PDE/ODE Solvers","title":"PDE/ODE Solvers","text":"(Image: )","category":"page"},{"location":"inverse_impl/#Inverse-Modeling-with-ADCME-1","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"","category":"section"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"In this section, we show how to solve the four types of inverse problems identified in Inverse Modeling. For simplicity, let the forward model be a 1D Poisson equation","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"beginaligned-nabla (Xnabla u(x)) = varphi(x)  xin (01) u(0)=u(1) = 0endaligned","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Here X is the unknown  which may be one of the four forms: parameter, function, functional or random variable. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Inverse problem Problem type Approach Reference\nnablacdot(cnabla u) = varphi(x) Parameter Adjoint State Method 1 2\nnablacdot(f(x)nabla u) = varphi(mathbfx) Function DNN 3\nnablacdot(f(u)nabla u) = varphi(x) Functional DNN Learning from indirect data 4\nnablacdot(varpinabla u) = varphi(x) Stochastic Inversion Adversarial Learning with GAN 5","category":"page"},{"location":"inverse_impl/#Parameter-Inverse-Problem-1","page":"Inverse Modeling with ADCME","title":"Parameter Inverse Problem","text":"","category":"section"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"When X is just a scalar/vector, we call this type of problem parameter inverse problem. We consider a manufactured solution: the exact X=1 and u(x)=x(1-x), so we have","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"varphi(x) = 2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Assume we can observe u(05)=025 and the initial guess for X_0=10. We use finite difference method to discretize the PDE and the interval 01 is divided uniformly to 0=x_0x_1ldotsx_n=1, with n=100, x_i+1-x_i = h=frac1n.","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"we can solve the problem with the following code snippet","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"using ADCME\nn = 100\nh = 1/n\nX0 = Variable(10.0)\nA = X0 * diagm(0=>2/h^2*ones(n-1), 1=>-1/h^2*ones(n-2), -1=>-1/h^2*ones(n-2)) # coefficient matrix for the finite difference\nφ = 2.0*ones(n-1) # right hand side\nu = A\\φ\nloss = (u[50] - 0.25)^2\n\nsess = Session(); init(sess)\nBFGS!(sess, loss)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"After around 7 iterations, the estimated X_0 converges to 1.0000000016917243. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"info: Info\nWe can actually solve the linear system A\\φ more efficiently by using SparseTensor. In this case, simply substitute A = X0 * diagm(0=>2/h^2*ones(n-1), 1=>-1/h^2*ones(n-2), -1=>-1/h^2*ones(n-2))with A = X0 * SparseTensor(diagm(0=>2/h^2*ones(n-1), 1=>-1/h^2*ones(n-2), -1=>-1/h^2*ones(n-2)))","category":"page"},{"location":"inverse_impl/#Function-Inverse-Problem-1","page":"Inverse Modeling with ADCME","title":"Function Inverse Problem","text":"","category":"section"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"When X is a function that does not depend on u, i.e., a function of location x, we call this type of problem function inverse problem. A common approach to this type of problem is to approximate the unknown function X with a parametrized form, such as piecewise linear functions, radial basis functions or Chebyshev polynomials; sometimes we can also discretize X and substitute X by a vector of its values at the discrete grid nodes. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"This tutorial is not aimed at the comparison of different methods. Instead, we show how we can use neural networks to represent X and train the neural network by coupling it with numerical schemes. The gradient calculation can be laborious with the traditional adjoint state methods but is trivial with automatic differentiation. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Let's assume the true X has the following form","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"X(x) = frac11+x^2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"The exact varphi is given by ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"varphi(x) = frac2 left(x^2 - x left(2 x - 1right) + 1right)left(x^2 + 1right)^2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"The idea is to use a neural network mathcalN(xw) with weights and biases w that maps the location xin mathbbR to a scalar value such that","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"mathcalN(x w)approx X(x)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"To find the optional w, we solve the Poisson equation with X(x)=mathcalN(xw), where the numerical scheme is ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"left( -fracX_i+X_i+12 right) u_i+1 + fracX_i-1+2X_i+X_i+12 u_i + left( -fracX_i+X_i-12 right) = varphi(x_i) h^2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Here X_i = mathcalN(x_i w). ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Assume we can observe the full solution u(x), we can compare it with the solution u(xw), and minimize the loss function ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"L(w) = sum_i=2^n-1 (u(x_iw)-u(x_i))^2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"using ADCME\nn = 100\nh = 1/n\nx = collect(LinRange(0, 1.0, n+1))\nX = ae(x, [20,20,20,1])^2  # to ensure that X is positive, we use NN^2 instead of NN\nA = spdiag(\n  n-1,\n  1=>-(X[2:end-2] + X[3:end-1])/2,\n  -1=>-(X[3:end-1] + X[2:end-2])/2,\n  0=>(2*X[2:end-1]+X[3:end]+X[1:end-2])/2\n)/h^2\nφ = @. 2*x*(1 - 2*x)/(x^2 + 1)^2 + 2 /(x^2 + 1)\nu = Array(A)\\φ[2:end-1] # for efficiency, we can use A\\φ[2:end-1] (sparse solver)\nu_obs = (@. x * (1-x))[2:end-1]\nloss = sum((u - u_obs)^2)\n\nsess = Session(); init(sess)\nBFGS!(sess, loss)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"We show the exact X(x) and the pointwise error in the following plots","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"leftmathcalN(x_iw)-X(x_i)right","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"(Image: errorX) (Image: exactX)\nPointwise Absolute Error Exact X(u)","category":"page"},{"location":"inverse_impl/#Functional-Inverse-Problem-1","page":"Inverse Modeling with ADCME","title":"Functional Inverse Problem","text":"","category":"section"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"In the functional inverse problem, X is a function that depends on u (or both x and u); it must not be confused with the functional inverse problem and it is much harder to solve (since the equation is nonlinear). For example, we may have","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"X(u) = frac11+100u^2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"The corresponding varphi is ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"frac2 left(100 x^2 left(x - 1right)^2 - 100 x left(x - 1right) left(2 x - 1right)^2 + 1right)left(100 x^2 left(x - 1right)^2 + 1right)^2","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"To solve the Poisson equation, we use the standard Newton-Raphson scheme, in which case, we need to compute the residual","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"R_i = X(u_i)fracu_i+1-u_i-12h + X(u_i)fracu_i+1+u_i-1-2u_ih^2 + varphi(x_i)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"and the corresponding Jacobian","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"fracpartial R_ipartial u_j = left beginmatrix  fracX(u_i)2h + fracX(u_i)h^2  j=i-1 X(u_i)fracu_i+1-u_i-12h + X(u_i)fracu_i+1+u_i-1-2u_ih^2 - frac2h^2X(u_i)  j=i  -fracX(u_i)2h + fracX(u_i)h^2  j=i+1 0  j-i1  endmatrix right","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Just like the function inverse problem, we also use a neural network to approximate X(u); the difference is that the input of the neural network is u instead of x. It is convenient to compute X(u) with automatic differentiation. If we had used piecewise linear functions, it is only possible to compute the gradients in the weak sense; but this is not a problem for neural networks as long as we use smooth activation functions such as tanh. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"ADCME also prepares a built-in Newton-Raphson solver newton_raphson for you. To use this function, you only need to provide the residual and Jacobian ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"function residual_and_jacobian(θ, u)\n  X = ae(u, [20,20,20,20,1], θ)+1.0 # to avoid X=0 at the first step \n  Xp = tf.gradients(X, u)[1]\n  Xpp = tf.gradients(Xp, u)[1]\n  up = [u[2:end];constant(zeros(1))]\n  un = [constant(zeros(1)); u[1:end-1]]\n  R = Xp * (up-un)/2h + X * (up+un-2u)/h^2 + φ\n  dRdu = Xpp * (up-un)/2h + Xp*(up+un-2u)/h^2 - 2/h^2*X \n  dRdun = Xp[2:end]/2h + X[2:end]/h^2\n  dRdup = -Xp[1:end-1]/2h + X[1:end-1]/h^2\n  J = spdiag(n-1, \n\t  -1=>dRdun,\n  \t  0=>dRdu,\n      1=>dRdup)\n  return R, Array(J)\nend","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Then we can solve the Poisson equation with ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"newton_raphson(residual_and_jacobian, u0, θ)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"One caveat here is that the Newton-Raphson operator is a nonlinear implicit operator that does not fall into the types of operators where automatic differentiation applies. Instead, a special procedure is needed. Luckily, ADCME provides an API that abstracts away this technical difficulty and users can call NonlinearConstrainedProblem directly to extract the gradients. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"using ADCME \n# definitions of residual_and_jacobian is omited here\nn = 100\nh = 1/n\nx = collect(LinRange(0, 1.0, n+1))\n\nφ = @. 2*(x^2*(x - 1)^2 - x*(x - 1)*(2*x - 1)^2 + (x^2*(x - 1)^2 + 1)^2 + 1)/(x^2*(x - 1)^2 + 1)^2\nφ = φ[2:end-1]\nθ = Variable(ae_init([1,20,20,20,20,1]))\nu0 = constant(zeros(n-1)) #initial guess\nfunction L(u)\n  u_obs = (@. x * (1-x))[2:end-1]\n\tloss = sum((u - u_obs)^2)\nend\nloss, solution, grad = NonlinearConstrainedProblem(residual_and_jacobian, L, θ, u0)\n\nsess = Session(); init(sess)\nBFGS!(sess, loss, grad, θ)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Note in this case, we only have one set of observations and the inverse problem may be ill-posed, i.e., the solution is not unique. Thus the best we can expect is that we can find one of the solutions mathcalN(xw) that can reproduce the observation we have. This is indeed the case we encounter in this example: the reproduced solution is nearly the same as the observation, but we found a completely different mathcalN(xw) compared with X(u). ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"(Image: nn)","category":"page"},{"location":"inverse_impl/#Stochastic-Inverse-Problem-1","page":"Inverse Modeling with ADCME","title":"Stochastic Inverse Problem","text":"","category":"section"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"The final type of inverse problem is called stochastic inverse problem. In this problem, X is a random variable with unknown distribution. Consequently, the solution u will also be a random variable. For example, we may have the following settings in practice","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"The measurement of u(05) may not be accurate. We might assume that u(05) sim mathcalN(hat u(05) sigma^2) where hat u(05) is one observation and sigma is the prescribed standard deviation of the measurement. Thus, we want to estimate the distribution of X which will produce the same distribution for u(05). This type of problem falls under the umbrella of uncertainty quantification. \nThe quantity X itself is subject to randomness in nature, but its distribution may be positively/negatively skewed (e.g., the stock price returns). We can measure several samples of u(05) and want to estimate the distribution of X based on the samples. This problem is also called the probabilistic inverse problem. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"We cannot simply minimize the distance between u(05) and u   (which are random variables) as usual; instead, we need a metric to measure the discrepancy between two distributions–u and u(05). The observables u(05) may be given in multiple forms","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"The probability density function. \nThe unnormalized log-likelihood function. \nDiscrete samples. ","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"We consider the third type in this tutorial. The idea is to construct a sampler for X with a neural network and find the optimal weights and biases by minimizing the discrepancy between actually observed samples  and produced ones. Here is how we train the neural network:","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"We first propose a candidate neural network that transforms a sample from mathcalN(0 I_d) to a sample from X. Then we randomly generate K samples z_i_i=1^K from mathcalN(0 I_d) and transform them to X_i w_i=1^K. We solve the Poisson equation K times to obtain u(05z_i w)_i=1^K. Meanwhile, we sample K items from the observations (e.g., with the bootstrap method) u_i(05)_i=1^K. We can use a probability metric D to measure the discrepancy between u(05z_i w)_i=1^K and u_i(05)_i=1^K. There are many choices for D, such as (they are not necessarily non-overlapped)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Wasserstein distance (from optimal transport)\nKL-divergence, JS-divergence, etc. \nDiscriminator neural networks (from generative adversarial nets)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"For example, we can consider the first approach, and invoke sinkhorn provided by ADCME","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"using ADCME\nusing Distributions\n\n# we add a mixture Gaussian noise to the observation\nm = MixtureModel(Normal[\n   Normal(0.3, 0.1),\n   Normal(0.0, 0.1)], [0.5, 0.5])\n\nfunction solver(a)\n  n = 100\n  h = 1/n\n  A = a[1] * diagm(0=>2/h^2*ones(n-1), 1=>-1/h^2*ones(n-2), -1=>-1/h^2*ones(n-2)) \n  φ = 2.0*ones(n-1) # right hand side\n  u = A\\φ\n  u[50]\nend\n\nbatch_size = 64\nx = placeholder(Float64, shape=[batch_size,10])\nz = placeholder(Float64, shape=[batch_size,1])\ndat = z + 0.25\nfdat  = reshape(map(solver, ae(x, [20,20,20,1])+1.0), batch_size, 1)\nloss = empirical_sinkhorn(fdat, dat, dist=(x,y)->dist(x,y,2), method=\"lp\")\nopt = AdamOptimizer(0.01, beta1=0.5).minimize(loss)\n\nsess = Session(); init(sess)\nfor i = 1:100000\n  run(sess, opt, feed_dict=Dict(\n        x=>randn(batch_size, 10),\n        z=>rand(m, batch_size,1)\n      ))\nend","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"Loss Function Iteration 5000 Iteration 15000 Iteration 25000\n(Image: loss) (Image: test5000) (Image: test15000) (Image: test25000)","category":"page"},{"location":"inverse_impl/#","page":"Inverse Modeling with ADCME","title":"Inverse Modeling with ADCME","text":"\u0014","category":"page"},{"location":"apps_ana/#Adversarial-Numerical-Analysis-1","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"","category":"section"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Kailai Xu, and Eric Darve. \"Adversarial Numerical Analysis for Inverse Problems\"","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Project Website","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Many scientific and engineering applications are formulated as inverse problems associated with stochastic models. In such cases the unknown quantities are distributions. The applicability of traditional methods is limited because of their demanding assumptions or prohibitive computational consumptions; for example, maximum likelihood methods require closed-form density functions, and Markov Chain Monte Carlo needs a large number of simulations. ","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Consider the forward model","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"x = F(w theta)","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Here w is a known stochastic process such as Gaussian processes, theta is an unknown parameter, distribution or stochastic processes. Consequently, the output of the model x is also a stochastic process. F can be a very complicated model such as a system of partial differential equations. Many models fall into this category; here  we solve an inverse modeling problem of boundary value Poisson equations","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"\\begin{cases}     -\\nabla \\cdot (a(x)\\nabla u(x)) = 1 & x\\in(0,1)\\\\\n    u(0) = u(1) = 0 & \\mbox{otherwise} \\end{cases}","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"a(x) = 1-09expleft( -frac(x-mu)^22sigma^2 right)","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"Here (mu, sigma) is subject to unknown distribution (theta in the forward model). w=emptyset and x is the solution to the equation, u. Assume we have observed a set of solutions u_i, and we want to estimate the distribution of (mu, sigma). Adversarial numerical analysis works as follows","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"The distribution (mu, sigma) is parametrized by a deep neural network G_eta.\nFor each instance of (mu, sigma) sampled from the neural network parametrized distribution, we can compute a solution u_mu sigma using the finite difference method. \nWe compute a metric between the distribution u_mu sigma and u_i with a discriminative neural network D_xi.\nMinimize the metric by adjusting the weights of G_eta and D_xi. ","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"The distribution of (mu, sigma) is given by G_eta. The following plots show the workflow of adversarial numerical analysis and a sample result for the Dirichlet distribution. ","category":"page"},{"location":"apps_ana/#","page":"Adversarial Numerical Analysis","title":"Adversarial Numerical Analysis","text":"(Image: )","category":"page"},{"location":"ot/#Optimal-Transport-1","page":"Optimal Transport","title":"Optimal Transport","text":"","category":"section"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Optimal transport (OT) can be used to measure the \"distance\" between two probability distribution. ","category":"page"},{"location":"ot/#Discrete-Wasserstein-Distance-1","page":"Optimal Transport","title":"Discrete Wasserstein Distance","text":"","category":"section"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"In this section, we introduce a novel approach for training a general model: SinkHorn Generative Networks (SGN). In this approach, a neural network is used to transform a sample from uniform distributions to a sample of targeted distribution. We train the neural network by minimizing the discrepancy between the targeted distribution and the desired distribution, which is described by optimal transport distance. Different from generative adversarial nets (GAN), we do not use a discriminator neural network to construct the discrepancy; instead, it is computed directly with efficient SinkHorn algorithm or net-flow solver. The minimization is conducted via a gradient-based optimizer, where the gradients are computed with reverse mode automatic differentiation. ","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"To begin with, we first construct the sample x of the targeted distribution and the sample s from the desired distribution and compute the loss function with sinkhorn","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"using Revise\nusing ADCME\nusing PyPlot\n\nreset_default_graph()\nK = 64\nz = placeholder(Float64, shape=[K, 10])\nx = squeeze(ae(z, [20,20,20,1]))\ns = placeholder(Float64, shape=[K])\nM = abs(reshape(x, -1, 1) - reshape(s, 1, -1))\nloss = sinkhorn(ones(K)/K, ones(K)/K, M, reg=0.1)","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Example 1 In the first example, we assume the desired distribution is the standard Gaussian. We minimize the loss function with AdamOptimizer","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"opt = AdamOptimizer().minimize(loss)\nsess = Session(); init(sess)\nfor i = 1:10000\n    _, l = run(sess, [opt, loss], z=>rand(K, 10), s=>randn(K))\n    @show i, l\nend","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"The result is shown below","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"V = []\nfor k = 1:100\n    push!(V,run(sess, x, z=>rand(K,10)))\nend\nV = vcat(V...)\nhist(V, bins=50, density=true)\nx0 = LinRange(-3.,3.,100)\nplot(x0, (@. 1/sqrt(2π)*exp(-x0^2/2)), label=\"Reference\")\nxlabel(\"x\")\nylabel(\"f(x)\")\nlegend()","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"(Image: )","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Example 2 In the first example, we assume the desired distribution is the positive part of the the standard Gaussian. ","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"opt = AdamOptimizer().minimize(loss)\nsess = Session(); init(sess)\nfor i = 1:10000\n    _, l = run(sess, [opt, loss], z=>rand(K, 10), s=>abs.(randn(K)))\n    @show i, l\nend","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"(Image: )","category":"page"},{"location":"ot/#Dynamic-Time-Wrapping-1","page":"Optimal Transport","title":"Dynamic Time Wrapping","text":"","category":"section"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Dynamic time wrapping is suitable for computing the distance of two time series. The idea is that we can shift the time series to obtain the \"best\" match while retaining the causality in time. This is best illustrated in the following figure  (Image: )","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"In ADCME, the distance is computed using dtw. As an example, given two time series","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"Sample = Float64[1,2,3,5,5,5,6]\nTest = Float64[1,1,2,2,3,5]","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"The distance can be computed by ","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"c, p = dtw(Sample, Test, true)","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"c is the distance and p is the path.","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"If we have 2000 time series A and 2000 time series B and we want to compute the total distance of the corresponding time series, we can use map function ","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"A = constant(rand(2000,1000))\nB = constant(rand(2000,1000))\ndistance = map(x->dtw(x[1],x[2],false)[1],[A,B], dtype=Float64)","category":"page"},{"location":"ot/#","page":"Optimal Transport","title":"Optimal Transport","text":"distances is a 2000 length vector and gives us the pairwise distance for all time series. ","category":"page"},{"location":"array/#Tensor-Operations-1","page":"Tensor Operations","title":"Tensor Operations","text":"","category":"section"},{"location":"array/#","page":"Tensor Operations","title":"Tensor Operations","text":"Description API\nConstant creation constant(rand(10))\nVariable creation Variable(rand(10))\nGet size size(x)\nGet size of dimension size(x,i)\nGet length length(x)\nResize reshape(x,5,3)\nVector indexing v[1:3],v[[1;3;4]],v[3:end],v[:]\nMatrix indexing m[3,:], m[:,3], m[1,3],m[[1;2;5],[2;3]]\nIndex relative to end v[end], m[1,end]\nExtract row (most efficient) m[2], m[2,:]\nExtract column m[:,3]\nConvert to dense diagonal matrix diagm(v)\nConvert to sparse diagonal matrix spdiag(v)\nExtract diagonals as vector diag(m)\nElementwise multiplication a.*b\nMatrix (vector) multiplication a*b\nMatrix transpose m'\nDot product sum(a*b)\nSolve A\\b\nInversion inv(m)\nAverage all elements mean(x)\nAverage along dimension mean(x, dims=1)\nMaximum/Minimum of all elements maximum(x), minimum(x)\nSqueeze all single dimensions squeeze(x)\nSqueeze along dimension squeeze(x, dims=1), squeeze(x, dims=[1;2])\nReduction (along dimension) norm(a), sum(a, dims=1)\nElementwise Multiplication a.*b\nElementwise Power a^2\nSVD svd(a)","category":"page"},{"location":"parallel/#Parallel-Computing-1","page":"Parallel Computing","title":"Parallel Computing","text":"","category":"section"},{"location":"parallel/#Manually-Place-Operators-on-Devices-1","page":"Parallel Computing","title":"Manually Place Operators on Devices","text":"","category":"section"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"ADCME backend TensorFlow treats each operator as the smallest computation unit. Users are allowed to manually assign the device locations for each operator. This is usually done with the @pywith tf.device(\"/cpu:0\") syntax. For example, if we want to create a variable a and compute sin(a) on GPU:0 we can write","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(\"/GPU:0\") begin\n    global a = Variable(1.0)\n    global b = sin(a)\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Custom Device Placement Functions","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"This syntax is useful and simple for placing operators on certain GPU devices without changing original codes. However, sometimes we want to place certain operators on certain devices. This can be done by implementing a custom assign_to_device function. As an example, we want to place all Variables on CPU:0 while placing all other operators on GPU:0, the function has the following form","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"PS_OPS = [\"Variable\", \"VariableV2\", \"AutoReloadVariable\"]\nfunction assign_to_device(device, ps_device=\"/device:CPU:0\")\n    function _assign(op)\n        node_def = pybuiltin(\"isinstance\")(op, tf.NodeDef) ? op : op.node_def\n        if node_def.op in PS_OPS\n            return ps_device\n        else\n            return device\n        end\n    end\n\n    return _assign\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Then we can write something like","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(assign_to_device(\"/device:GPU:0\")) begin\n    global a = Variable(1.0)\n    global b = sin(a)\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"We can check the location of a and b by inspecting their device attributes","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"julia> a.device\n\"/device:CPU:0\"\n\njulia> b.device\n\"/device:GPU:0\"","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Colocate Gradient Operators","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"When we call gradients, TensorFlow actually creates a set of new operators, one for each operator in the forward computation. By default, those operators are placed on the default device (GPU:0 if GPU is available; otherwise it's CPU:0). Sometimes we want to place the operators created by gradients on the same devices as the corresponding original operators. For example, if the operator b (sin) in the last example is on GPU:0, we hope the corresponding gradient computation (cos) is also on GPU:0. This can be done by specifying colocate keyword arguments in gradients","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(assign_to_device(\"/device:GPU:0\")) begin\n    global a = Variable(1.0)\n    global b = sin(a)\nend\n\n@pywith tf.device(\"/CPU:0\") begin\n    global c = cos(b)\nend\n\ng = gradients(c, a, colocate=true)","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"In the following figure, we show the effects of colocate of the above codes. The test code snippet is","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"g = gradients(c, a, colocate=true)\nsess = Session(); init(sess)\nrun_profile(sess, g+c)\nsave_profile(\"true.json\")\n\ng = gradients(c, a, colocate=false)\nsess = Session(); init(sess)\nrun_profile(sess, g+c)\nsave_profile(\"false.json\")","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"(Image: )","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"Batch Normalization Update Operators","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"If you use bn (batch normalization) on multi-GPUs, you must be careful to update the parameters in batch normalization on CPUs. This can be done by explicitly specify ","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"@pywith tf.device(\"/cpu:0\") begin\n  global update_ops = get_collection(tf.GraphKeys.UPDATE_OPS)\nend","category":"page"},{"location":"parallel/#","page":"Parallel Computing","title":"Parallel Computing","text":"and bind update_ops to an active operator (or explictly execute it in run(sess,...)).","category":"page"},{"location":"sparse/#Sparse-Linear-Algebra-1","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"ADCME augments TensorFlow APIs by adding sparse linear algebra support. In ADCME, sparse matrices are represented by SparseTensor. This data structure stores indices, rows and cols of the sparse matrices and keep track of relevant information such as whether it is diagonal for performance consideration. The default is row major (due to TensorFlow backend). ","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"When evaluating SparseTensor, the output will be SparseMatrixCSC, the native Julia representation of sparse matrices","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"A = run(sess, s) # A has type SparseMatrixCSC{Float64,Int64}","category":"page"},{"location":"sparse/#Sparse-Matrix-Construction-1","page":"Sparse Linear Algebra","title":"Sparse Matrix Construction","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"By passing columns (Int64), rows (Int64) and values (Float64) arrays","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"ii = [1;2;3;4]\njj = [1;2;3;4]\nvv = [1.0;1.0;1.0;1.0]\ns = SparseTensor(ii, jj, vv, 4, 4)","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"By passing a SparseMatrixCSC","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"using SparseArrays\ns = SparseTensor(sprand(10,10,0.3))","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"By passing a dense array (tensor or numerical array)","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"D = Array(sprand(10,10,0.3)) # a dense array\nd = constant(D)\ns = dense_to_sparse(d)","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"There are also special constructors. ","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Description Code\nDiagonal matrix with diagonal v spdiag(v)\nEmpty matrix with size m, n spzero(m, n)\nIdentity matrix with size m spdiag(m)","category":"page"},{"location":"sparse/#Matrix-Traits-1","page":"Sparse Linear Algebra","title":"Matrix Traits","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Size of the matrices\nsize(s) # (10,20)\nsize(s,1) # 10\nReturn row, col, val arrays (also known as COO arrays)\nii,jj,vv = find(s)","category":"page"},{"location":"sparse/#Arithmetic-Operations-1","page":"Sparse Linear Algebra","title":"Arithmetic Operations","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Add Subtract\ns = s1 + s2\ns = s1 - s2\n\nScalar Product\ns = 2.0 * s1\ns = s1 / 2.0\nSparse Product\ns = s1 * s2\nTransposition\ns = s1'","category":"page"},{"location":"sparse/#Sparse-Solvers-1","page":"Sparse Linear Algebra","title":"Sparse Solvers","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"Solve a linear system (s is a square matrix)\nsol = s\\rhs\nSolve a least square system (s is a tall matrix)\nsol = s\\rhs","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"note: Note\nThe least square solvers are implemented using Eigen sparse linear packages, and the gradients are also implemented. Thus, the following codes will work as expected (the gradients functions will correctly compute the gradients):ii = [1;2;3;4]\njj = [1;2;3;4]\nvv = constant([1.0;1.0;1.0;1.0])\nrhs = constant(rand(4))\ns = SparseTensor(ii, jj, vv, 4, 4)\nsol = s\\rhs\nrun(sess, sol)\nrun(sess, gradients(sum(sol), rhs))\nrun(sess, gradients(sum(sol), vv))","category":"page"},{"location":"sparse/#Assembling-Sparse-Matrix-1","page":"Sparse Linear Algebra","title":"Assembling Sparse Matrix","text":"","category":"section"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"In many applications, we want to accumulate row, col and val to assemble a sparse matrix in iterations. For this purpose, we provide the SparseAssembler utilities. ","category":"page"},{"location":"sparse/#","page":"Sparse Linear Algebra","title":"Sparse Linear Algebra","text":"SparseAssembler\naccumulate\nassemble","category":"page"},{"location":"sparse/#ADCME.SparseAssembler","page":"Sparse Linear Algebra","title":"ADCME.SparseAssembler","text":"SparseAssembler(handle::Union{PyObject, <:Integer}, n::Union{PyObject, <:Integer}, tol::Union{PyObject, <:Real}=0.0)\n\nCreates a SparseAssembler for accumulating row, col, val for sparse matrices. \n\nhandle: an integer handle for creating a sparse matrix. If the handle already exists, SparseAssembler return the existing sparse matrix handle. If you are creating different sparse matrices, the handles should be different. \nn: Number of rows of the sparse matrix. \ntol (optional): Tolerance. SparseAssembler will treats any values less than tol as zero. \n\nExample 1\n\nhandle = SparseAssembler(100, 5, 1e-8)\nop1 = accumulate(handle, 1, [1;2;3], [1.0;2.0;3.0])\nop2 = accumulate(handle, 2, [1;2;3], [1.0;2.0;3.0])\nJ = assemble(5, 5, [op1;op2])\n\nJ will be a SparseTensor object. \n\nExample 2\n\nhandle = SparseAssembler(0, 5)\nop1 = accumulate(handle, 1, [1;2;3], ones(3))\nop2 = accumulate(handle, 1, [3], [1.])\nop3 = accumulate(handle, 2, [1;3], ones(2))\nJ = assemble(5, 5, [op1;op2;op3]) # op1, op2, op3 are parallel\nArray(run(sess, J))≈[1.0  1.0  2.0  0.0  0.0\n                1.0  0.0  1.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0]\n\n\n\n\n\n","category":"function"},{"location":"sparse/#Base.accumulate","page":"Sparse Linear Algebra","title":"Base.accumulate","text":"accumulate(handle::PyObject, row::Union{PyObject, <:Integer}, cols::Union{PyObject, Array{<:Integer}}, vals::Union{PyObject, Array{<:Real}})\n\nAccumulates row-th row. It adds the value to the sparse matrix\n\nfor k = 1:length(cols)\n    A[row, cols[k]] += vals[k]\nend\n\nhandle is the handle created by SparseAssembler. \n\nSee SparseAssembler for an example.\n\nnote: Note\nThe function accumulate returns a op::PyObject. Only when op is executed, the nonzero values are populated into the sparse matrix. \n\n\n\n\n\n","category":"function"},{"location":"sparse/#ADCME.assemble","page":"Sparse Linear Algebra","title":"ADCME.assemble","text":"assemble(m::Union{PyObject, <:Integer}, n::Union{PyObject, <:Integer}, ops::PyObject)\n\nAssembles the sparse matrix from the ops created by accumulate. ops is either a single output from accumulate, or concated from several ops\n\nop1 = accumulate(handle, 1, [1;2;3], [1.0;2.0;3.0])\nop2 = accumulate(handle, 2, [1;2;3], [1.0;2.0;3.0])\nop = [op1;op2] # equivalent to `vcat([op1, op2]...)`\n\nm and n are rows and columns of the sparse matrix. \n\nSee SparseAssembler for an example.\n\n\n\n\n\n","category":"function"},{"location":"apps_constitutive_law/#Learning-Constitutive-Relations-from-Indirect-Observations-Using-Deep-Neural-Networks-1","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"","category":"section"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"Huang, Daniel Z. (co-first author), Kailai Xu (co-first author), Charbel Farhat, and Eric Darve. \"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks\"","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"Project Website","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"We present a new approach for predictive modeling and its uncertainty quantification for mechanical systems, where coarse-grained models such as constitutive relations are derived directly from observation data. We explore the use of neural networks to represent the unknowns functions (e.g., constitutive relations). Its counterparts, like piecewise linear functions and radial basis functions, are compared, and the strength of neural networks is explored. The training and predictive processes in this framework seamlessly combine the finite element method, automatic differentiation, and neural networks (or its counterparts). Under mild assumptions, we establish convergence guarantees. This framework also allows uncertainty quantification analysis in the form of intervals of confidence. Numerical examples on a multiscale fiber-reinforced plate problem and a nonlinear rubbery membrane problem from solid mechanics demonstrate the effectiveness of the framework.","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"The solid mechanics equation can be formulated as","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"mathcalP(u(mathbfx) mathcalM(u(mathbfx)dot u(mathbfx) mathbfx)) = mathcalF(u(mathbfx) mathbfx p)","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"where u is the displacement, mathbfx is the location, p is the external pressure, mathcalF is the external force, mathcalM(u(mathbfx)dot u(mathbfx) mathbfx) is the stress (mathcalM is also called the constitutive law), mathcalP(u(mathbfx) mathcalM(u(mathbfx)dot u(mathbfx) mathbfx)) is the internal force. For a new material or nonhomogeneous material, the constitutive relation mathcalM is not known and we want to estimate it. In laboratory, usually only u(mathbfx) can be measured but the stress cannot. The idea is to substitute the constitutive law relation–in this work, we assume mathcalM only depends on u(mathbfx) and the neural network is mathcalM_theta(u(mathbfx)), where theta is the unknown parameter. ","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"We train the neural network by solving the optimization problem","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"min_thetamathcalP(mathbfu mathcalM_theta(mathbfu)) - mathcalF(mathbfu mathbfx p) ^2_2","category":"page"},{"location":"apps_constitutive_law/#","page":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","title":"Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks","text":"(Image: image-20191031200808697)","category":"page"},{"location":"apps_levy/#Calibrating-Multivariate-Lévy-Processes-with-Neural-Networks-1","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"","category":"section"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Kailai Xu and Eric Darve. \"Calibrating Multivariate Lévy Processes with Neural Networks\" ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Project Website","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Calibrating a Lévy process usually requires characterizing its jump distribution. Traditionally this problem can be solved with nonparametric estimation using the empirical characteristic functions (ECF), assuming certain regularity, and results to date are mostly in 1D. For multivariate Lévy processes and less smooth Lévy densities, the problem becomes challenging as ECFs decay slowly and have large uncertainty because of limited observations. We solve this problem by approximating the Lévy density with a parametrized functional form; the characteristic function is then estimated using numerical integration. In our benchmarks, we used deep neural networks and found that they are robust and can capture sharp transitions in the Lévy density. They perform favorably compared to piecewise linear functions and radial basis functions. The methods and techniques developed here apply to many other problems that involve nonparametric estimation of functions embedded in a system model.","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The Lévy process can be described by the Lévy-Khintchine formula","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"phi(xi) = mathbbEe^mathrmi langle xi mathbfX_t rangle =explefttleft( mathrmi langle mathbfb xi rangle - frac12langle xi mathbfAxirangle  +int_mathbbR^d left( e^mathrmi langle xi mathbfxrangle - 1 - mathrmi langle xi mathbfxrangle mathbf1_mathbfxleq 1right)nu(dmathbfx)right) right","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Here the multivariate Lévy process is described by three parameters: a positive semi-definite matrix mathbfA = SigmaSigma^T in mathbbR^dtimes d, where Sigmain mathbbR^dtimes d; a vector mathbfbin mathbbR^d; and a measure nuin mathbbR^dbackslashmathbf0. ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Given a sample path mathbfX_iDelta t, i=123ldots, we focus on estimating mathbfb, mathbfA and nu. In this work, we focus on the functional inverse problem–estimate nu–and assume mathbfb=0mathbfA=0. The idea is","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The Lévy density is approximated by a parametric functional form–-such as piecewise linear functions–-with parameters theta,","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"    nu(mathbfx) approx nu_theta(mathbfx)","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The characteristic function is approximated by numerical integration ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"phi(xi)approx    phi_theta(xi) = expleft Delta t sum_i=1^n_q left(e^mathrmi langlexi mathbfx_i rangle-1-mathrmilanglexi mathbfx_i ranglemathbf1_mathbfx_ileq 1  right)nu_theta(mathbfx_i) w_i right","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"where (mathbfx_i w_i)_i=1^n_q are quadrature nodes and weights.","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"The empirical characteristic functions are computed given observations mathbfX_iDelta t_i=0^n","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"hatphi_n(xi) = frac1nsum_i=1^n exp(mathrmilangle xi mathbfX_iDelta t-mathbfX_(i-1)Delta trangle )  xi in mathbbR^d","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"Solve the following optimization problem with a gradient based method. Here xi_i _i=1^m are collocation points depending on the data. ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"min_thetafrac1m sum_i=1^m hatphi_n(xi_i)-phi_theta(xi_i)  ^2","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"We show the schematic description of the method and some results on calibrating a discontinuous Lévy density function nu. ","category":"page"},{"location":"apps_levy/#","page":"Calibrating Multivariate Lévy Processes with Neural Networks","title":"Calibrating Multivariate Lévy Processes with Neural Networks","text":"(Image: image-20191031200808697)","category":"page"},{"location":"api/#API-Reference-1","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Core-Functions-1","page":"API Reference","title":"Core Functions","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"ADCME.jl\", \"core.jl\", \"run.jl\"]","category":"page"},{"location":"api/#ADCME.add_collection-Tuple{String,PyObject}","page":"API Reference","title":"ADCME.add_collection","text":"add_collection(name::String, v::PyObject)\n\nAdds v to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.add_collection-Tuple{String,Vararg{PyObject,N} where N}","page":"API Reference","title":"ADCME.add_collection","text":"add_collection(name::String, vs::PyObject...)\n\nAdds operators vs to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.control_dependencies-Tuple{Any,Union{Tuple, PyObject, Array{PyObject,N} where N}}","page":"API Reference","title":"ADCME.control_dependencies","text":"control_dependencies(f, ops::Union{Array{PyObject}, PyObject})\n\nExecutes all operations in ops before any operations created inside the block. \n\nop1 = tf.print(\"print op1\")\nop3 = tf.print(\"print op3\")\ncontrol_dependencies(op1) do\n    global op2 = tf.print(\"print op2\")\nend\nrun(sess, [op2,op3])\n\nIn this example, op1 must be executed before op2. But there is no guarantee when op3 will be executed.  There are several possible outputs of the program such as\n\nprint op3\nprint op1\nprint op2\n\nor \n\nprint op1\nprint op3\nprint op2\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.get_collection","page":"API Reference","title":"ADCME.get_collection","text":"get_collection(name::Union{String, Missing})\n\nReturns the collection with name name. If name is missing, returns all the trainable variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.has_gpu-Tuple{}","page":"API Reference","title":"ADCME.has_gpu","text":"has_gpu()\n\nChecks if GPU is available.\n\nnote: Note\nADCME will use GPU automatically if GPU is available. To disable GPU, set the environment variable ENV[\"CUDA_VISIBLE_DEVICES\"]=\"\" before importing ADCME \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.if_else-Tuple{Union{Bool, PyObject, Array},Any,Any,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.if_else","text":"if_else(condition::Union{PyObject,Array,Bool}, fn1, fn2, args...;kwargs...)\n\nIf condition is a scalar boolean, it outputs fn1 or fn2 (a function with no input argument or a tensor) based on whether condition is true or false.\nIf condition is a boolean array, if returns condition .* fn1 + (1 - condition) .* fn2\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.independent-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.independent","text":"independent(o::PyObject, args...; kwargs...)\n\nReturns o but when computing the gradients, the top gradients will not be back-propagated into dependent variables of o.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.reset_default_graph-Tuple{}","page":"API Reference","title":"ADCME.reset_default_graph","text":"reset_default_graph()\n\nResets the graph by removing all the operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Tuple{String}","page":"API Reference","title":"ADCME.tensor","text":"tensor(s::String)\n\nReturns the tensor with name s. See tensorname.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensorname-Tuple{PyObject}","page":"API Reference","title":"ADCME.tensorname","text":"tensorname(o::PyObject)\n\nReturns the name of the tensor. See tensor.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.while_loop-Tuple{Union{Function, PyObject},Function,Union{PyObject, Array{Any,N} where N, Array{PyObject,N} where N}}","page":"API Reference","title":"ADCME.while_loop","text":"while_loop(condition::Union{PyObject,Function}, body::Function, loop_vars::Union{PyObject, Array{Any}, Array{PyObject}};\n    parallel_iterations::Int64=10, kwargs...)\n\nLoops over loop_vars while condition is true. This operator only creates one extra node to mark the loops in the computational graph.\n\nExample\n\nThe following script computes \n\nsum_i=1^10 i\n\nfunction condition(i, ta)\n    i <= 10\nend\nfunction body(i, ta)\n    u = read(ta, i-1)\n    ta = write(ta, i, u+1)\n    i+1, ta\nend\nta = TensorArray(10)\nta = write(ta, 1, constant(1.0))\ni = constant(2, dtype=Int32)\n_, out = while_loop(condition, body, [i, ta])\nsummation = stack(out)[10]\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.run_profile-Tuple","page":"API Reference","title":"ADCME.run_profile","text":"run_profile(args...;kwargs...)\n\nRuns the session with tracing information.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save_profile","page":"API Reference","title":"ADCME.save_profile","text":"save_profile(filename::String=\"default_timeline.json\")\n\nSave the timeline information to file filename. \n\nOpen Chrome and navigate to chrome://tracing\nLoad the timeline file\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.bind-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"Base.bind","text":"bind(op::PyObject, ops...)\n\nAdding operations ops to the dependencies of op. The function is useful when we want to execute ops but ops is not  in the dependency of the final output. For example, if we want to print i each time i is evaluated\n\ni = constant(1.0)\nop = tf.print(i)\ni = bind(i, op)\n\n\n\n\n\n","category":"method"},{"location":"api/#Variables-1","page":"API Reference","title":"Variables","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"variable.jl\"]","category":"page"},{"location":"api/#ADCME.TensorArray","page":"API Reference","title":"ADCME.TensorArray","text":"TensorArray(size_::Int64=0, args...;kwargs...)\n\nConstructs a tensor array for while_loop.  \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.Variable-Tuple{Any}","page":"API Reference","title":"ADCME.Variable","text":"Variable(initial_value;kwargs...)\n\nConstructs a ref tensor from value. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.cell-Tuple{Array,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.cell","text":"cell(arr::Array, args...;kwargs...)\n\nConstruct a cell tensor. \n\nExample\n\njulia> r = cell([[1.],[2.,3.]])\njulia> run(sess, r[1])\n1-element Array{Float32,1}:\n 1.0\njulia> run(sess, r[2])\n2-element Array{Float32,1}:\n 2.0\n 3.0\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.constant-Tuple{Any}","page":"API Reference","title":"ADCME.constant","text":"constant(value; kwargs...)\n\nConstructs a non-trainable tensor from value.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.convert_to_tensor-Union{Tuple{Union{Missing, Nothing, Number, PyObject, Array{T,N} where N}}, Tuple{T}} where T<:Number","page":"API Reference","title":"ADCME.convert_to_tensor","text":"convert_to_tensor(o::Union{PyObject, Number, Array{T}, Missing, Nothing}; dtype::Union{Type, Missing}=missing) where T<:Number\nconvert_to_tensor(os::Array, dtypes::Array)\n\nConverts the input o to tensor. If o is already a tensor and dtype (if provided) is the same as that of o, the operator does nothing. Otherwise, convert_to_tensor converts the numerical array to a constant tensor or casts the data type. convert_to_tensor also accepts multiple tensors. \n\nExample\n\nconvert_to_tensor([1.0, constant(rand(2)), rand(10)], [Float32, Float64, Float32])\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.gradient_checkpointing","page":"API Reference","title":"ADCME.gradient_checkpointing","text":"gradient_checkpointing(type::String=\"speed\")\n\nUses checkpointing scheme for gradients. \n\n'speed':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,   so checkpointing them maximizes the running speed   (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n'memory': try to minimize the memory usage   (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n'collection': look for a tensorflow collection named 'checkpoints', which holds the tensors to checkpoint\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.gradients-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.gradients","text":"gradients(ys::PyObject, xs::PyObject; kwargs...)\n\nComputes the gradients of ys w.r.t xs. \n\nIf ys is a scalar, gradients returns the gradients with the same shape as xs.\nIf ys is a vector, gradients returns the Jacobian fracpartial ypartial x\n\nnote: Note\nThe second usage is not suggested since ADCME adopts reverse mode automatic differentiation.  Although in the case ys is a vector and xs is a scalar, gradients cleverly uses forward mode automatic differentiation, it requires that the second order gradients are implemented for relevant operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.hessian-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.hessian","text":"hessian computes the hessian of a scalar function f with respect to vector inputs xs\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Union{Tuple{Array{T,1}}, Tuple{T}} where T","page":"API Reference","title":"ADCME.tensor","text":"tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Union{Tuple{Array{T,2}}, Tuple{T}} where T","page":"API Reference","title":"ADCME.tensor","text":"tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T\n\nConvert a generic array v to a tensor. For example, \n\nv = [0.0 constant(1.0) 2.0\n    constant(2.0) 0.0 1.0]\nu = tensor(v)\n\nu will be a 2times 3 tensor. \n\nnote: Note\nThis function is expensive. Use with caution.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.read-Tuple{PyObject,Union{Integer, PyObject}}","page":"API Reference","title":"Base.read","text":"read(ta::PyObject, i::Union{PyObject,Integer})\n\nReads data from TensorArray at index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.write-Tuple{PyObject,Union{Integer, PyObject},PyObject}","page":"API Reference","title":"Base.write","text":"write(ta::PyObject, i::Union{PyObject,Integer}, obj)\n\nWrites data obj to TensorArray at index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#Random-Variables-1","page":"API Reference","title":"Random Variables","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"random.jl\"]","category":"page"},{"location":"api/#ADCME.categorical-Tuple{Union{Integer, PyObject}}","page":"API Reference","title":"ADCME.categorical","text":"categorical(n::Union{PyObject, Integer}; kwargs...)\n\nkwargs has a keyword argument logits, a 2-D Tensor with shape [batch_size, num_classes].   Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.choice-Tuple{Union{PyObject, Array},Union{Integer, PyObject}}","page":"API Reference","title":"ADCME.choice","text":"choice(inputs::Union{PyObject, Array}, n_samples::Union{PyObject, Integer};replace::Bool=false)\n\nChoose n_samples samples from inputs with/without replacement. \n\n\n\n\n\n","category":"method"},{"location":"api/#Sparse-Matrix-1","page":"API Reference","title":"Sparse Matrix","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"sparse.jl\"]","category":"page"},{"location":"api/#ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}","page":"API Reference","title":"ADCME.SparseTensor","text":"SparseTensor(A::SparseMatrixCSC)\nSparseTensor(A::Array{Float64, 2})\n\nCreates a SparseTensor from numerical arrays. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseTensor-Union{Tuple{S}, Tuple{T}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S},Union{Nothing, PyObject, S}}} where S<:Integer where T<:Integer","page":"API Reference","title":"ADCME.SparseTensor","text":"SparseTensor(I::Union{PyObject,Array{T,1}}, J::Union{PyObject,Array{T,1}}, V::Union{Array{Float64,1}, PyObject}, m::Union{S, PyObject, Nothing}=nothing, n::Union{S, PyObject, Nothing}=nothing) where {T<:Integer, S<:Integer}\n\nConstructs a sparse tensor.  Examples:\n\nii = [1;2;3;4]\njj = [1;2;3;4]\nvv = [1.0;1.0;1.0;1.0]\ns = SparseTensor(ii, jj, vv, 4, 4)\ns = SparseTensor(sprand(10,10,0.3))\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseAssembler","page":"API Reference","title":"ADCME.SparseAssembler","text":"SparseAssembler(handle::Union{PyObject, <:Integer}, n::Union{PyObject, <:Integer}, tol::Union{PyObject, <:Real}=0.0)\n\nCreates a SparseAssembler for accumulating row, col, val for sparse matrices. \n\nhandle: an integer handle for creating a sparse matrix. If the handle already exists, SparseAssembler return the existing sparse matrix handle. If you are creating different sparse matrices, the handles should be different. \nn: Number of rows of the sparse matrix. \ntol (optional): Tolerance. SparseAssembler will treats any values less than tol as zero. \n\nExample 1\n\nhandle = SparseAssembler(100, 5, 1e-8)\nop1 = accumulate(handle, 1, [1;2;3], [1.0;2.0;3.0])\nop2 = accumulate(handle, 2, [1;2;3], [1.0;2.0;3.0])\nJ = assemble(5, 5, [op1;op2])\n\nJ will be a SparseTensor object. \n\nExample 2\n\nhandle = SparseAssembler(0, 5)\nop1 = accumulate(handle, 1, [1;2;3], ones(3))\nop2 = accumulate(handle, 1, [3], [1.])\nop3 = accumulate(handle, 2, [1;3], ones(2))\nJ = assemble(5, 5, [op1;op2;op3]) # op1, op2, op3 are parallel\nArray(run(sess, J))≈[1.0  1.0  2.0  0.0  0.0\n                1.0  0.0  1.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0]\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.assemble-Tuple{Union{PyObject, #s195} where #s195<:Integer,Union{PyObject, #s139} where #s139<:Integer,PyObject}","page":"API Reference","title":"ADCME.assemble","text":"assemble(m::Union{PyObject, <:Integer}, n::Union{PyObject, <:Integer}, ops::PyObject)\n\nAssembles the sparse matrix from the ops created by accumulate. ops is either a single output from accumulate, or concated from several ops\n\nop1 = accumulate(handle, 1, [1;2;3], [1.0;2.0;3.0])\nop2 = accumulate(handle, 2, [1;2;3], [1.0;2.0;3.0])\nop = [op1;op2] # equivalent to `vcat([op1, op2]...)`\n\nm and n are rows and columns of the sparse matrix. \n\nSee SparseAssembler for an example.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.find-Tuple{SparseTensor}","page":"API Reference","title":"ADCME.find","text":"find(s::SparseTensor)\n\nReturns the row, column and values for sparse tensor s.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.scatter_add-Union{Tuple{T}, Tuple{S}, Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}},Union{Colon, UnitRange{T}, Array{S,1}, Integer, PyObject},Union{Colon, UnitRange{T}, Array{T,1}, Integer, PyObject},Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}}}} where T<:Real where S<:Real","page":"API Reference","title":"ADCME.scatter_add","text":"scatter_update(A::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}},\ni1::Union{Integer, Colon, UnitRange{T}, PyObject,Array{S,1}},\ni2::Union{Integer, Colon, UnitRange{T}, PyObject,Array{T,1}},\nB::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}})  where {S<:Real,T<:Real}\n\nAdds B to a subblock of a sparse matrix A. Equivalently, \n\nA[i1, i2] += B\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.scatter_update-Union{Tuple{T}, Tuple{S}, Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}},Union{Colon, UnitRange{T}, Array{S,1}, Integer, PyObject},Union{Colon, UnitRange{T}, Array{T,1}, Integer, PyObject},Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}}}} where T<:Real where S<:Real","page":"API Reference","title":"ADCME.scatter_update","text":"scatter_update(A::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}},\ni1::Union{Integer, Colon, UnitRange{T}, PyObject,Array{S,1}},\ni2::Union{Integer, Colon, UnitRange{T}, PyObject,Array{T,1}},\nB::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}})  where {S<:Real,T<:Real}\n\nUpdates a subblock of a sparse matrix by B. Equivalently, \n\nA[i1, i2] = B\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{Int64}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(n::Int64)\n\nConstructs a sparse identity matrix of size ntimes n.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{Integer,Vararg{Pair,N} where N}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(m::Integer, pair::Pair...)\n\nConstructs a square mtimes m SparseTensor from pairs of the form \n\noffset => array \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{PyObject}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(o::PyObject)\n\nConstructs a sparse diagonal matrix where the diagonal entries are o\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spzero","page":"API Reference","title":"ADCME.spzero","text":"spzero(m::Int64, n::Union{Missing, Int64}=missing)\n\nConstructs a empty sparse matrix of size mtimes n. n=m if n is missing\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.accumulate-Tuple{PyObject,Union{PyObject, #s195} where #s195<:Integer,Union{PyObject, Array{#s139,N} where N where #s139<:Integer},Union{PyObject, Array{#s138,N} where N where #s138<:Real}}","page":"API Reference","title":"Base.accumulate","text":"accumulate(handle::PyObject, row::Union{PyObject, <:Integer}, cols::Union{PyObject, Array{<:Integer}}, vals::Union{PyObject, Array{<:Real}})\n\nAccumulates row-th row. It adds the value to the sparse matrix\n\nfor k = 1:length(cols)\n    A[row, cols[k]] += vals[k]\nend\n\nhandle is the handle created by SparseAssembler. \n\nSee SparseAssembler for an example.\n\nnote: Note\nThe function accumulate returns a op::PyObject. Only when op is executed, the nonzero values are populated into the sparse matrix. \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.:\\","page":"API Reference","title":"Base.:\\","text":"\\(s::SparseTensor, o::PyObject, method::String=\"SparseLU\")\n\nSolves the linear equation  s x = o\n\nMethod\n\nFor square matrices s, one of the following methods is available\n\nSparseLU\nSparseQR\nSimplicialLDLT\nSimplicialLLT\n\n\n\n\n\n","category":"function"},{"location":"api/#Operations-1","page":"API Reference","title":"Operations","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"ops.jl\"]","category":"page"},{"location":"api/#ADCME.argsort-Tuple{PyObject}","page":"API Reference","title":"ADCME.argsort","text":"argsort(o::PyObject; \nstable::Bool = false, rev::Bool=false, dims::Integer=-1, name::Union{Nothing,String}=nothing)\n\nReturns the indices of a tensor that give its sorted order along an axis.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.batch_matmul-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.batch_matmul","text":"batch_matmul(o1::PyObject, o2::PyObject)\n\nComputes o1[i,:,:] * o2[i, :] or o1[i,:,:] * o2[i, :, :] for each index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.clip-Tuple{Union{Array{Any,N} where N, Array{PyObject,N} where N},Any,Any,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.clip","text":"clip(o::Union{Array{Any}, Array{PyObject}}, vmin, vmax, args...;kwargs...)\n\nClips the values of o to the range [vmin, vmax]\n\nExample\n\na = constant(3.0)\na = clip(a, 1.0, 2.0)\nb = constant(rand(3))\nb = clip(b, 0.5, 1.0)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.cvec-Tuple{PyObject}","page":"API Reference","title":"ADCME.cvec","text":"rvec(o::PyObject; kwargs...)\n\nVectorizes the tensor o to a column vector, assuming column major.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.pmap-Tuple{Function,Union{PyObject, Array{PyObject,N} where N}}","page":"API Reference","title":"ADCME.pmap","text":"pmap(fn::Function, o::Union{Array{PyObject}, PyObject})\n\nParallel for loop. There should be no data dependency between different iterations.\n\nExample\n\nx = constant(ones(10))\ny1 = pmap(x->2.0*x, x)\ny2 = pmap(x->x[1]+x[2], [x,x])\ny3 = pmap(1:10, x) do z\n    i = z[1]\n    xi = z[2]\n    xi + cast(Float64, i)\nend\nrun(sess, y1)\nrun(sess, y2)\nrun(sess, y3)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.rvec-Tuple{PyObject}","page":"API Reference","title":"ADCME.rvec","text":"rvec(o::PyObject; kwargs...)\n\nVectorizes the tensor o to a row vector, assuming row major.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.set_shape-Union{Tuple{N}, Tuple{PyObject,Union{Tuple{Vararg{Integer,N}}, Array{#s196,N} where N where #s196<:Integer}}} where N","page":"API Reference","title":"ADCME.set_shape","text":"set_shape(o::PyObject, s::Union{Array{<:Integer}, Tuple{Vararg{<:Integer, N}}}) where N\nset_shape(o::PyObject, s::Integer...)\n\nSets the shape of o to s. s must be the actual shape of o. This function is used to convert a  tensor with unknown dimensions to a tensor with concrete dimensions. \n\nExample\n\na = placeholder(Float64, shape=[nothing, 10])\nb = set_shape(a, 3, 10)\nrun(sess, b, a=>rand(3,10)) # OK \nrun(sess, b, a=>rand(5,10)) # Error\nrun(sess, b, a=>rand(10,3)) # Error\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.topk","page":"API Reference","title":"ADCME.topk","text":"topk(o::PyObject, k::Union{PyObject,Integer}=1;\n    sorted::Bool=true, name::Union{Nothing,String}=nothing)\n\nFinds values and indices of the k largest entries for the last dimension. If sorted=true the resulting k elements will be sorted by the values in descending order.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.vector-Union{Tuple{T}, Tuple{Union{PyObject, StepRange, UnitRange, Array{T,N} where N},Union{PyObject, Array{Float64,N} where N},Union{Int64, PyObject}}} where T<:Integer","page":"API Reference","title":"ADCME.vector","text":"vector(i::Union{Array{T}, PyObject, UnitRange, StepRange}, v::Union{Array{Float64},PyObject},s::Union{Int64,PyObject})\n\nReturns a vector V with length s such that\n\nV[i] = v\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.vec-Tuple{PyObject}","page":"API Reference","title":"Base.vec","text":"vec(o::PyObject;kwargs...)\n\nVectorizes the tensor o assuming column major. \n\n\n\n\n\n","category":"method"},{"location":"api/#LinearAlgebra.svd-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"LinearAlgebra.svd","text":"svd(o::PyObject, args...; kwargs...)\n\nReturns a TFSVD structure which holds the following data structures\n\nS::PyObject\nU::PyObject\nV::PyObject\nVt::PyObject\n\nWe have the equality o = USV\n\nExample\n\nA = rand(10,20)\nr = svd(constant(A))\nA2 = r.U*diagm(r.S)*r.Vt # The value of `A2` should be equal to `A`\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.map-Tuple{Function,Union{PyObject, Array{PyObject,N} where N}}","page":"API Reference","title":"Base.map","text":"map(fn::Function, o::Union{Array{PyObject},PyObject};\nkwargs...)\n\nApplies fn to each element of o. \n\no∈Array{PyObject} : returns [fn(x) for x in o]\no∈PyObject : splits o according to the first dimension and then applies fn. \n\nExample\n\na = constant(rand(10,5))\nb = map(x->sum(x), a) # equivalent to `sum(a, dims=2)`\n\nnote: Note\nIf fn is a multivariate function, we need to specify the output type using dtype keyword. For example, a = constant(ones(10))\nb = constant(ones(10))\nfn = x->x[1]+x[2]\nc = map(fn, [a, b], dtype=Float64)\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.reshape-Union{Tuple{N}, Tuple{PyObject,Union{Tuple{Vararg{Integer,N}}, Array{#s196,N} where N where #s196<:Integer}}} where N","page":"API Reference","title":"Base.reshape","text":"reshape(o::PyObject, s::Union{Array{<:Integer}, Tuple{Vararg{<:Integer, N}}}) where N \nreshape(o::PyObject, s::Integer; kwargs...)\nreshape(o::PyObject, m::Integer, n::Integer; kwargs...)\nreshape(o::PyObject, ::Colon, n::Integer)\nreshape(o::PyObject, n::Integer, ::Colon)\n\nReshapes the tensor. reshape is compatible with \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.sort-Tuple{PyObject}","page":"API Reference","title":"Base.sort","text":"Base.:sort(o::PyObject; \nrev::Bool=false, dims::Integer=-1, name::Union{Nothing,String}=nothing)\n\nSort a multidimensional array o along the given dimension. \n\nrev: true for DESCENDING and false (default) for ASCENDING\ndims: -1 for last dimension. \n\n\n\n\n\n","category":"method"},{"location":"api/#IO-1","page":"API Reference","title":"IO","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"io.jl\"]","category":"page"},{"location":"api/#ADCME.Diary","page":"API Reference","title":"ADCME.Diary","text":"Diary(suffix::Union{String, Nothing}=nothing)\n\nCreates a diary at a temporary directory path. It returns a writer and the corresponding directory path\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.activate","page":"API Reference","title":"ADCME.activate","text":"activate(sw::Diary, port::Int64=6006)\n\nRunning Diary at http://localhost:port.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load","page":"API Reference","title":"ADCME.load","text":"load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nLoads the values of variables to the session sess from the file file. If vars is nothing, it loads values to all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load-Tuple{Diary,String}","page":"API Reference","title":"ADCME.load","text":"load(sw::Diary, dirp::String)\n\nLoads Diary from dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.pload-Tuple{String}","page":"API Reference","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.psave-Tuple{PyObject,String}","page":"API Reference","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save","page":"API Reference","title":"ADCME.save","text":"save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nSaves the values of vars in the session sess. The result is written into file as a dictionary. If vars is nothing, it saves all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.save-Tuple{Diary,String}","page":"API Reference","title":"ADCME.save","text":"save(sw::Diary, dirp::String)\n\nSaves Diary to dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.scalar","page":"API Reference","title":"ADCME.scalar","text":"scalar(o::PyObject, name::String)\n\nReturns a scalar summary object.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.write-Tuple{Diary,Int64,Union{String, Array{String,N} where N}}","page":"API Reference","title":"Base.write","text":"write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})\n\nWrites to Diary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Optimization-1","page":"API Reference","title":"Optimization","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"optim.jl\"]","category":"page"},{"location":"api/#ADCME.BFGS!","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, max_iter::Int64=15000; \nvars::Array{PyObject}=PyObject[], callback::Union{Function, Nothing}=nothing, kwargs...)\n\nBFGS! is a simplified interface for BFGS optimizer. See also ScipyOptimizerInterface. callback is a callback function with signature \n\ncallback(vs::Array, iter::Int64, loss::Float64)\n\nvars is an array consisting of tensors and its values will be the input to vs.\n\nexample\n\na = Variable(1.0)\nloss = (a - 10.0)^2\nBFGS!(sess, loss)\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(value_and_gradients_function::Function, initial_position::Union{PyObject, Array{Float64}}, max_iter::Int64=50, args...;kwargs...)\n\nApplies the BFGS optimizer to value_and_gradients_function\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!-Union{Tuple{T}, Tuple{PyObject,PyObject,Union{Nothing, PyObject, Array{T,N} where N},Union{PyObject, Array{PyObject,N} where N}}} where T<:Union{Nothing, PyObject}","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, grads::Union{Array{T},Nothing,PyObject}, \n    vars::Union{Array{PyObject},PyObject}; kwargs...) where T<:Union{Nothing, PyObject}\n\nRunning BFGS algorithm min_textttvars textttloss(textttvars) The gradients grads must be provided. Typically, grads[i] = gradients(loss, vars[i]).  grads[i] can exist on different devices (GPU or CPU). \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.CustomOptimizer-Tuple{Function}","page":"API Reference","title":"ADCME.CustomOptimizer","text":"CustomOptimizer(opt::Function, name::String)\n\ncreates a custom optimizer with struct name name. For example, we can integrate Optim.jl with ADCME by  constructing a new optimizer\n\nCustomOptimizer(\"Con\") do f, df, c, dc, x0, nineq, neq, x_L, x_U\n    opt = Opt(:LD_MMA, length(x0))\n    bd = zeros(length(x0)); bd[end-1:end] = [-Inf, 0.0]\n    opt.lower_bounds = bd\n    opt.xtol_rel = 1e-4\n    opt.min_objective = (x,g)->(g[:]= df(x); return f(x)[1])\n    inequality_constraint!(opt, (x,g)->( g[:]= dc(x);c(x)[1]), 1e-8)\n    (minf,minx,ret) = NLopt.optimize(opt, x0)\n    minx\nend\n\nThen we can create an optimizer with \n\nopt = Con(loss, inequalities=[c1], equalities=[c2])\n\nTo trigger the optimization, use\n\nopt.minimize(sess)\n\nor \n\nminimize(opt, sess)\n\nNote thanks to the global variable scope of Julia, step_callback, optimizer_kwargs can actually  be passed from Julia environment directly.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyObject},Union{PyObject, Array{Float64,N} where N}}} where T<:Real","page":"API Reference","title":"ADCME.NonlinearConstrainedProblem","text":"NonlinearConstrainedProblem(f::Function, L::Function, θ::PyObject, u0::Union{PyObject, Array{Float64}}; options::Union{Dict{String, T}, Missing}=missing) where T<:Integer\n\nComputes the gradients fracpartial Lpartial theta\n\nmin  L(u) quad mathrmst  F(theta u) = 0\n\nu0 is the initial guess for the numerical solution u, see newton_raphson.\n\nCaveats: Assume r, A = f(θ, u) and θ are the unknown parameters, gradients(r, θ) must be defined (backprop works properly)\n\nReturns: It returns a tuple (L: loss, C: constraints, and Graidents)\n\nleft(L(u) u fracpartial Lpartial θright)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerInterface-Tuple{Any}","page":"API Reference","title":"ADCME.ScipyOptimizerInterface","text":"ScipyOptimizerInterface(loss; method=\"L-BFGS-B\", options=Dict(\"maxiter\"=> 15000, \"ftol\"=>1e-12, \"gtol\"=>1e-12), kwargs...)\n\nA simple interface for Scipy Optimizer. See also ScipyOptimizerMinimize and BFGS!.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerMinimize-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.ScipyOptimizerMinimize","text":"ScipyOptimizerMinimize(sess::PyObject, opt::PyObject; kwargs...)\n\nMinimizes a scalar Tensor. Variables subject to optimization are updated in-place at the end of optimization.\n\nNote that this method does not just return a minimization Op, unlike minimize; instead it actually performs minimization by executing commands to control a Session https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface. See also ScipyOptimizerInterface and BFGS!.\n\nfeed_dict: A feed dict to be passed to calls to session.run.\nfetches: A list of Tensors to fetch and supply to loss_callback as positional arguments.\nstep_callback: A function to be called at each optimization step; arguments are the current values of all optimization variables flattened into a single vector.\nloss_callback: A function to be called every time the loss and gradients are computed, with evaluated fetches supplied as positional arguments.\nrun_kwargs: kwargs to pass to session.run.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.newton_raphson-Union{Tuple{T}, Tuple{Function,Union{PyObject, Array}}, Tuple{Function,Union{PyObject, Array},Union{Missing, PyObject, Array{#s196,N} where N where #s196<:Real}}} where T<:Real","page":"API Reference","title":"ADCME.newton_raphson","text":"newton_raphson(f::Function, u::Union{Array,PyObject}, θ::Union{Missing,PyObject}; options::Union{Dict{String, T}, Missing}=missing)\n\nNewton Raphson solver for solving a nonlinear equation.  f has the signature \n\nf(θ::Union{Missing,PyObject}, u::PyObject)->(r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is off)\nf(θ::Union{Missing,PyObject}, u::PyObject)->(fval::PyObject, r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is on)\n\nwhere r is the residual and A is the Jacobian matrix; in the case where linesearch is on, the function value fval must also be supplied. θ are external parameters. u0 is the initial guess for u options:\n\nmax_iter: maximum number of iterations (default=100)\nverbose: whether details are printed (default=false)\nrtol: relative tolerance for termination (default=1e-12)\ntol: absolute tolerance for termination (default=1e-12)\nLM: a float number, Levenberg-Marquardt modification x^k+1 = x^k - (J^k + mu^k)^-1g^k (default=0.0)\nlinesearch: whether linesearch is used (default=false)\n\nCurrently, the backtracing algorithm is implemented. The parameters for linesearch are also supplied via options\n\nls_c1: stop criterion, f(x^k)  f(0) + alpha c_1  f(0)\nls_ρ_hi: the new step size alpha_1leq rho_hialpha_0 \nls_ρ_lo: the new step size alpha_1geq rho_loalpha_0 \nls_iterations: maximum number of iterations for linesearch\nls_maxstep: maximum allowable steps\nls_αinitial: initial guess for the step size alpha\n\n\n\n\n\n","category":"method"},{"location":"api/#Neural-Networks-1","page":"API Reference","title":"Neural Networks","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"layers.jl\"]","category":"page"},{"location":"api/#ADCME.ae","page":"API Reference","title":"ADCME.ae","text":"ae(x::PyObject, output_dims::Array{Int64}, scope::String = \"default\";\n    activation::Union{Function,String} = \"tanh\")\n\nCreates a neural network with intermediate numbers of neurons output_dims.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.ae-Tuple{Union{PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{PyObject, Array{Float64,N} where N}}","page":"API Reference","title":"ADCME.ae","text":"ae(x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64}, θ::Union{Array{Float64}, PyObject};\nactivation::Union{Function,String} = \"tanh\")\n\nCreates a neural network with intermediate numbers of neurons output_dims. The weights are given by θ\n\nExample 1: Explicitly construct weights and biases\n\nx = constant(rand(10,2))\nn = ae_num([2,20,20,20,2])\nθ = Variable(randn(n)*0.001)\ny = ae(x, [20,20,20,2], θ)\n\nExample 2: Implicitly construct weights and biases\n\nθ = ae_init([10,20,20,20,2]) \nx = constant(rand(10,10))\ny = ae(x, [20,20,20,2], θ)\n\nSee also ae_num, ae_init.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_init-Tuple{Array{Int64,N} where N}","page":"API Reference","title":"ADCME.ae_init","text":"ae_init(output_dims::Array{Int64}; T::Type=Float64, method::String=\"xavier\")\n\nReturn the initial weights and bias values by TensorFlow as a vector. Three types of  random initializers are provided\n\nxavier (default). It is useful for tanh fully connected neural network. \n\nW^l_i sim sqrtfrac1n_l-1\n\nxavier_avg. A variant of xavier\n\nW^l_i sim sqrtfrac2n_l + n_l-1\n\nhe. This is the activation aware initialization of weights and helps mitigate the problem\n\nof vanishing/exploding gradients. \n\nW^l_i sim sqrtfrac2n_l-1\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_num-Tuple{Array{Int64,N} where N}","page":"API Reference","title":"ADCME.ae_num","text":"ae_num(output_dims::Array{Int64})\n\nEstimates the number of weights and biases for the neural network. Note the first dimension should be the feature dimension (this is different from ae since in ae the feature dimension can be inferred), and the last dimension should be the output dimension. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_to_code-Tuple{String,String}","page":"API Reference","title":"ADCME.ae_to_code","text":"ae_to_code(file::String, scope::String; activation::String = \"tanh\")\n\nReturn the code string from the feed-forward neural network data in file. Usually we can immediately evaluate  the code string into Julia session by \n\neval(Meta.parse(s))\n\nIf activation is not specified, tanh is the default. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.bn-Tuple","page":"API Reference","title":"ADCME.bn","text":"bn(args...;center = true, scale=true, kwargs...)\n\nbn accepts a keyword parameter is_training. \n\nExample\n\nbn(inputs, name=\"batch_norm\", is_training=true)\n\nnote: Note\nbn should be used with control_dependencyupdate_ops = get_collection(UPDATE_OPS)\ncontrol_dependencies(update_ops) do \n    global train_step = AdamOptimizer().minimize(loss)\nend \n\n\n\n\n\n","category":"method"},{"location":"api/#Generative-Neural-Nets-1","page":"API Reference","title":"Generative Neural Nets","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"gan.jl\"]","category":"page"},{"location":"api/#ADCME.GAN","page":"API Reference","title":"ADCME.GAN","text":"GAN(dat::PyObject, \n    generator::Function, \n    discriminator::Function,\n    loss::Union{String, Function, Missing}=missing; \n    latent_dim::Union{Missing, Int64}=missing, \n    batch_size::Union{Missing, Int64}=missing)\n\nCreates a GAN instance. \n\ndat in mathbbR^ntimes d is the training data for the GAN, where n is the number of training data, and d is the dimension per training data.\ngeneratormathbbR^d rightarrow mathbbR^d is the generator function, d is the hidden dimension.\ndiscriminatormathbbR^d rightarrow mathbbR is the discriminator function. \nloss is the loss function. See klgan, rklgan, wgan, lsgan for examples.\nlatent_dim (default=d, the same as output dimension) is the latent dimension.\nbatch_size (default=32) is the batch size in training.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.jsgan-Tuple{GAN}","page":"API Reference","title":"ADCME.jsgan","text":"jsgan(gan::GAN)\n\nComputes the vanilla GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.klgan-Tuple{GAN}","page":"API Reference","title":"ADCME.klgan","text":"klgan(gan::GAN)\n\nComputes the KL-divergence GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.lsgan-Tuple{GAN}","page":"API Reference","title":"ADCME.lsgan","text":"lsgan(gan::GAN)\n\nComputes the least square GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.predict-Tuple{GAN,Union{PyObject, Array}}","page":"API Reference","title":"ADCME.predict","text":"predict(gan::GAN, input::Union{PyObject, Array})\n\nPredicts the GAN gan output given input input. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.rklgan-Tuple{GAN}","page":"API Reference","title":"ADCME.rklgan","text":"rklgan(gan::GAN)\n\nComputes the reverse KL-divergence GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.sample-Tuple{GAN,Int64}","page":"API Reference","title":"ADCME.sample","text":"sample(gan::GAN, n::Int64)\n\nSamples n instances from gan.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.wgan-Tuple{GAN}","page":"API Reference","title":"ADCME.wgan","text":"wgan(gan::GAN)\n\nComputes the Wasserstein GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.build!-Tuple{GAN}","page":"API Reference","title":"ADCME.build!","text":"build!(gan::GAN)\n\nBuilds the GAN instances. This function returns gan for convenience.\n\n\n\n\n\n","category":"method"},{"location":"api/#Tools-1","page":"API Reference","title":"Tools","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"extra.jl\"]","category":"page"},{"location":"api/#ADCME.compile_op-Tuple{String}","page":"API Reference","title":"ADCME.compile_op","text":"compile_op(oplibpath::String; check::Bool=false)\n\nCompile the library operator by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.customop","page":"API Reference","title":"ADCME.customop","text":"customop(simple::Bool=false)\n\nCreate a new custom operator. If simple=true, the custom operator only supports CPU and does not have gradients. \n\nExample\n\njulia> customop() # create an editable `customop.txt` file\n[ Info: Edit custom_op.txt for custom operators\njulia> customop() # after editing `customop.txt`, call it again to generate interface files.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.install-Tuple{String}","page":"API Reference","title":"ADCME.install","text":"install(s::String; force::Bool = false)\n\nInstall a custom operator via URL. s can be\n\nA URL. ADCME will download the directory through git\nA string. ADCME will search for the associated package on https://github.com/ADCMEMarket\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op-Tuple{String,String}","page":"API Reference","title":"ADCME.load_op","text":"load_op(oplibpath::String, opname::String)\n\nLoads the operator opname from library oplibpath.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op_and_grad-Tuple{String,String}","page":"API Reference","title":"ADCME.load_op_and_grad","text":"load_op_and_grad(oplibpath::String, opname::String; multiple::Bool=false)\n\nLoads the operator opname from library oplibpath; gradients are also imported.  If multiple is true, the operator is assumed to have multiple outputs. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_system_op","page":"API Reference","title":"ADCME.load_system_op","text":"load_system_op(s::String, oplib::String, grad::Bool=true)\n\nLoads custom operator from CustomOps directory (shipped with ADCME instead of TensorFlow) For example \n\ns = \"SparseOperator\"\noplib = \"libSO\"\ngrad = true\n\nthis will direct Julia to find library CustomOps/SparseOperator/libSO.dylib on MACOSX\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.test_jacobian-Tuple{Function,Array{Float64,N} where N}","page":"API Reference","title":"ADCME.test_jacobian","text":"test_jacobian(f::Function, x0::Array{Float64}; scale::Float64 = 1.0)\n\nTesting the gradients of a vector function f: y, J = f(x) where y is a vector output and J is the Jacobian.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.xavier_init","page":"API Reference","title":"ADCME.xavier_init","text":"xavier_init(size, dtype=Float64)\n\nReturns a matrix of size size and its values are from Xavier initialization. \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.compile-Tuple{String}","page":"API Reference","title":"ADCME.compile","text":"compile(s::String)\n\nCompiles the library s by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#Misc-1","page":"API Reference","title":"Misc","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"misc.jl\"]","category":"page"},{"location":"resource_manager/#Resource-Manager-1","page":"Resource Manager","title":"Resource Manager","text":"","category":"section"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"Sometimes we want to store data for different operations to share, or maintain a stateful kernel (data are shared across different invocations). One way to achieve this goal in the concurrency environment is to use  ResourceMgr in C++ custom operators. ","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"A typical usage of ResourceMgr is as follows","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"Define your own resource, which should inherent from ResourceBase and DebugString must be defined (it is an abstract method in ResourceBase).","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"#include \"tensorflow/core/framework/resource_mgr.h\"\nstruct MyVar: public ResourceBase{\n  string DebugString() const { return \"MyVar\"; };\n  mutex mu;\n  int32 val;\n};","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"Access the system ResourceMgr through ","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"auto rm = context->resource_manager();","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"Define your resource creation and manipulation method (make sure at any time there is only one single instance given the same container name and resource name).","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"MyVar* my_var;\nStatus s = rm->LookupOrCreate<MyVar>(\"my_container\", \"my_name\", &my_var, [&](MyVar** ret){\n    printf(\"Create a new container\\n\");\n    *ret = new MyVar;\n    (*ret)->val = *u_tensor;\n    return Status::OK();\n});\nDCHECK_EQ(s, Status::OK());\nmy_var->val += 1;\nmy_var->Unref();","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"When using the ResourceMgr, keep in mind that whenever you execute a new path in the computational graph, the system will create a new ResourceMgr. Therefore, to run operators that manipulate ResourceMgr in parallel, the trigger operator (which is fed to run(sess, ...)) must be attached those manipulation dependencies. ","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"See the following scripts for an example","category":"page"},{"location":"resource_manager/#","page":"Resource Manager","title":"Resource Manager","text":"CMakeLists.txt, TestResourceManager.cpp, gradtest.jl","category":"page"},{"location":"while_loop/#While-Loops-1","page":"While Loops","title":"While Loops","text":"","category":"section"},{"location":"while_loop/#Motivation-1","page":"While Loops","title":"Motivation","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"In engineering, we usually need to do for loops, e.g., time stepping, finite element matrix assembling, etc. In pseudocode, we have","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"for i = 1:1000000\n  global x\n\tx = do_some_simulation(x)\nend","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"To do automatic differentiation in ADCME, direct implemnetation in the above way incurs creation of 1000000 subgraphs, which requires large memories and long dependency parsing time. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"TensorFlow provides us a clever way to do loops, where only one graph is created for the whole loops. The basic idea is to create a while_loop graph based on five primitives, and the corresponding graph for backpropagation is constructed thereafter. ","category":"page"},{"location":"while_loop/#A-Basic-Example-1","page":"While Loops","title":"A Basic Example","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"As a simple example, we consider assemble the external load vector for linear finite elements in 1D. Assume that the load distribution is f(x)=1-x^2, xin01. The goal is to compute a vector mathbfv with v_i=int_0^1 f(x)phi_i(x)dx, where phi_i(x) is the i-th linear element. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The pseudocode for this problem is shown in the following","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"F = zeros(ne+1) // ne is the total number of elements\nfor e = 1:ne\n\tadd load contribution to F[e] and F[e+1]\nend","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"(Image: )","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"However, if ne is very large, writing explicit loops is unwise since it will create ne subgraphs. while_loop can be very helpful in this case (the script can also be found in https://github.com/kailaix/ADCME.jl/tree/master/examples/whileloop/whileloop_simple.jl)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"using ADCME\n\nne = 100\nh = 1/ne\nf = x->1-x^2\nfunction cond0(i, F_arr)\n    i<=ne+1\nend\nfunction body(i, F_arr)\n    fmid = f(cast(i-2, Float64)*h+h/2)\n    F = vector([i-1;i], [fmid*h/2;fmid*h/2], ne+1)\n    F_arr = write(F_arr, i, F)\n    i+1, F_arr\nend\n\nF_arr = TensorArray(ne+1)\nF_arr = write(F_arr, 1, constant(zeros(ne+1))) # inform `F_arr` of the data type by writing at index 1\ni = constant(2, dtype=Int32)\n_, out = while_loop(cond0, body, [i,F_arr]; parallel_iterations=10)\nF = sum(stack(out), dims=1)\nsess = Session(); init(sess)\nF0 = run(sess, F)","category":"page"},{"location":"while_loop/#Finite-Element-Analysis-1","page":"While Loops","title":"Finite Element Analysis","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"In this section, we demonstrate how to assemble a finite element matrix based on while_loop for a 2D Poisson problem. We consider the following problem","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"beginaligned\nnabla cdot ( Dnabla u(mathbfx) ) = f(mathbfx) mathbfxin Omega\nu(mathbfx) = 0  mathbfxin partial Omega\nendaligned","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Here Omega is the unit disk. We consider a simple case, where","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"beginaligned\nD=mathbfI\nf(mathbfx)=-4\nendaligned","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Then the exact solution will be ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"u(mathbfx) = 1-x^2-y^2","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The weak formulation is","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"langle nabla v(mathbfx) Dnabla u(mathbfx) rangle = langle f(mathbfx)v(mathbfx) rangle","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We  split Omega into triangles mathcalT and use piecewise linear basis functions. Typically, we would iterate over all elements and compute the local stiffness matrix for each element. However, this could result in a large loop if we use a fine mesh. Instead, we can use while_loop to complete the task. In ADCME, the syntax for while_loop is ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"while_loop(condition, body, loop_vars)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"here condition and body take loop_vars as inputs. The former outputs a bool tensor indicating whether to terminate the loop while the latter outputs the updated loop_vars. TensorArry is used to store variables that change during the loops. The codes for assembling FEM is","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"function assemble_FEM(Ds, Fs, nodes, elem)\n    NT = size(elem,1)\n    cond0 = (i,tai,taj,tav, tak, taf) -> i<=NT\n    elem = constant(elem)\n    nodes = constant(nodes)\n    function body(i, tai, taj, tav, tak, taf)\n        el = elem[i]\n        x1, y1 = nodes[el[1]][1], nodes[el[1]][2]\n        x2, y2 = nodes[el[2]][1], nodes[el[2]][2]\n        x3, y3 = nodes[el[3]][1], nodes[el[3]][2]\n        T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)\n        D = Ds[i]; F = Fs[i]*T/3\n        v = T*stack([D*((-x2 + x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y2 - y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2)])\n        tav = write(tav, i, v)\n        ii = vec([elem[i] elem[i] elem[i]]')\n        jj = [elem[i]; elem[i]; elem[i]]\n        tai = write(tai, i, ii)\n        taj = write(taj, i, jj)\n        tak = write(tak, i, elem[i])\n        taf = write(taf, i, stack([F,F,F]))\n        return i+1, tai, taj, tav, tak, taf\n    end\n    tai = TensorArray(NT, dtype=Int32)\n    taj = TensorArray(NT, dtype=Int32)\n    tak = TensorArray(NT, dtype=Int32)\n    tav = TensorArray(NT)\n    taf = TensorArray(NT)\n    i = constant(1, dtype=Int32)\n    i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)\n    _, tai, taj, tav, tak, taf = while_loop(cond0, body, [i, tai, taj, tav, tak, taf]; parallel_iterations=10)\n    vec(stack(tai)[1:NT]'), vec(stack(taj)[1:NT]'), vec(stack(tav)[1:NT]'),\n                        vec(stack(tak)[1:NT]'), vec(stack(taf)[1:NT]')\nend","category":"page"},{"location":"while_loop/#Explanation-1","page":"While Loops","title":"Explanation","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We now explain the codes. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We assume that nodes is a n_vtimes 2 tensor holding all n_v coordinates of the nodes, elem is a n_etimes 3  tensor holding all n_e triangle vertex index triples. We create five TensorArray to hold the row indices, column indices and values for the stiffness matrix, and row indices and values for the right hand side (Here NT denotes n_e):","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"tai = TensorArray(NT, dtype=Int32)\ntaj = TensorArray(NT, dtype=Int32)\ntak = TensorArray(NT, dtype=Int32)\ntav = TensorArray(NT)\ntaf = TensorArray(NT)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Within each loop (body), we extract the coordinates of each vertex coordinate","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"el = elem[i]\nx1, y1 = nodes[el[1]][1], nodes[el[1]][2]\nx2, y2 = nodes[el[2]][1], nodes[el[2]][2]\nx3, y3 = nodes[el[3]][1], nodes[el[3]][2]","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"and compute the area of ith triangle","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The local stiffness matrix is computed and vectorized (v). It is computed symbolically.  To store the computed value into TensorArray, we call the write API (there is also read API, which reads a value from TensorArray)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"tav = write(tav, i, v)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Note we have called ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"before we call while_loop. This is because we need to initialize the TensorArrays (i.e., telling them the size and type of elements in the arrays). We must guarantee that the sizes and types of the elements in the arrays are consistent in while_loop. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Finally, we stack the TensorArray into a tensor and vectorized it according to the row major. This serves as the output of assemble_FEM. The complete script for solving this problem is here and the following plot shows the numerical result and corresponding reference solution. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"(Image: Result for the Poisson Problem)","category":"page"},{"location":"while_loop/#Gradients-through-while_loop-1","page":"While Loops","title":"Gradients through while_loop","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"To inspect the gradients through the loops, we can run ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"println(run(sess, gradients(sum(u), Ds))) # a sparse tensor","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"This outputs a sparse tensor instead of a full tensor. To obtain the full tensor, we could call tf.convert_to_tensor","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"println(run(sess, tf.convert_to_tensor(gradients(sum(u), Ds)))) # full tensor","category":"page"},{"location":"inverse_modeling/#Inverse-Modeling-1","page":"Inverse Modeling","title":"Inverse Modeling","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Inverse modeling (IM) identifies a certain set of parameters or functions with which the outputs of the forward model matches the desired result or measurement. IM can usually be solved by formulating it as an optimization problem. But the major difference is that IM aims at getting information not accessible to forward model, instead of obtaining an optimal value of a fixed objective function and set of constraints. In practice, the objective function and constraints can be adjusted and prior information of the unknown parameters or functions can be imposed in the form of regularizers to better reflect the physical laws. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"The inverse modeling problem can be mathematically formulated as finding an unknown parameter X given input theta = hat theta and output u = hat u of a forward model","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"u = F(theta X)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Here theta and u can be a sample from a stochastic process. The scope of inverse problems that can be tackled with ADCME is","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"The forward model must be differentiable,  i.e., fracpartial Fpartial X and fracpartial Fpartial theta exist. However, we do not require those gradients to be implemented by users; they can be computed with automatic differentiation in ADCME.\nThe forward model must be a white-box and implemented with ADCME. ADCME is not for inverse modeling of black-box models. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"One iterative process for estimating X works as follows: we start from an initial guess X = hat X_0, assuming it is correct, and compute the predicted output hat u_0 with the forward modeling codes implemented in ADCME. Then, we measure the discrepancy between the predicted output hat u_0 and the actual observation hat u and apply the regular gradient-based optimization method to find the optimal X that minimizes this discrepancy. The gradients are computed with automatic differentiation, adjoint state methods or both. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"This conceptually simple approach can solve various types of inverse problems: either theta, u are stochastic or deterministic and the unknown X can  be a value, function and even functionals. As an example, assume the forward model is Poisson equation nabla cdot (Xnabla u(mathbfx)) = 0 with appropriate boundary condition, u(mathbfx) is the output (mathbfx is the coordinate) , the following is four kinds of potential classes of problems solvable with ADCME","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Inverse problem Problem type Approach Reference\nnablacdot(cnabla u) = 0 Parameter Adjoint State Method 1 2\nnablacdot(f(mathbfx)nabla u) = 0 Function* DNN 3\nnablacdot(f(u)nabla u) = 0 Functional** DNN Learning from indirect data 4\nnablacdot(varpinabla u) = 0 Stochastic Inversion Adversarial Learning with GAN 5","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"To see how those problems can be solved with ADCME in practice, see this tutorial. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"info: Info\n(*) Arguments of f are independent of u(**) At least one arguments of f is dependent on u. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: )","category":"page"},{"location":"inverse_modeling/#Automatic-Differentiation-1","page":"Inverse Modeling","title":"Automatic Differentiation","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"One powerful tool in inverse modeling is automatic differentiation (AD). Automatic differentiation is a general way to compute gradients based on the chain rule. By tracing the forward-pass computation, the gradient at the final step can propagate back to every operator and every parameter in a computational graph. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"As an example, a neural network model mainly consists of a sequence of linear transforms and non-linear activation functions. The goal of the training process is to minimize the error between its prediction and the label of ground truth. Automatic differentiation is used to calculate the gradients of every variable by back-propagating the gradients from the loss function to the trainable parameters, i.e., the weights and biases of neural networks. The gradients are then used in a gradient-based optimizer such as gradient descent methods to update the parameters. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"For another example, the physical forward simulation is similar to the neural network model in that they are both sequences of linear/non-linear transforms. One popular method in physical simulation, the FDTD (Finite-Difference Time-Domain) method, applies a finite difference operator to a consecutive time steps to solve time-dependent partial differential equations (PDEs). In seismic problems, we can specify parameters such as earthquake source functions and earth media properties to simulate the received seismic signals. In seismic inversion problems, those parameters are unknown and we can invert the underlining source characteristic and media property by minimizing the difference between the simulated seismic signals and the observed ones. In the framework of automatic differentiation, the gradients of the difference can be computed automatically and thus used in a gradient-based optimizer. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: )","category":"page"},{"location":"inverse_modeling/#AD-Implementation-in-ADCME-1","page":"Inverse Modeling","title":"AD Implementation in ADCME","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"ADCME uses TensorFlow as the backend for automatic differentiation. However, one major difference of ADCME compared with TensorFlow is that it provides a friendly syntax for scientific computing (essentially the same syntax as native Julia). This substantially reduces development time. In addition, ADCME augments TensorFlow libraries by adding missing features that are useful for scientific computing, such as sparse matrix solve, sparse least square, sparse assembling, etc. Additionally, Julia interfaces make it possible for directly implementing efficient numerical computation parts of the simulation (requires no automatic differentiation), for interacting with other languages (MATLAB, C/C++, R, etc.) and for built-in Julia parallelism. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"As an example, we show how a convoluted acoustic wave equation simulation with PML boundary condition can be translated to Julia codes with AD feature very neatly. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: )","category":"page"},{"location":"inverse_modeling/#Forward-Operator-Types-1","page":"Inverse Modeling","title":"Forward Operator Types","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"(Image: Operators)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called input, x, and turns out a parameter called ouput, y=f(x). For reverse mode automatic differentiation, besides evaluating f(x), we need also to compute fracpartial Jpartial x given fracpartial Jpartial y where J is a functional of y. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Note  the operator y=f(x) may be implicit in the sense that f is not given directly. In general, we can write the relationship between x and y as F(xy)=0. The operator is well-defined if for given x, there exists one and only one y such that F(xy)=0. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"For automatic differentiation, besides the well-definedness of F, we also require that we can compute fracpartial Jpartial x given fracpartial Jpartial y. It is easy to see that","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = -fracpartial Jpartial yF_y^-1F_x","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Therefore, we call an operator F is well-posed if F_y^-1 exists. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"All operators can be classified into four types based on the linearity and explicitness.","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Linear and explicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"This type of operators has the form ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"y = Ax","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"where A is a matrix. In this case, ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"F(xy) = Ax-y","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"and therefore ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = fracpartial Jpartial yA","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In Tensorflow, such an operator can be implemented as (assuming A is )","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"import tensorflow as tf\n@tf.custom_gradient\ndef F(x):\n      u = tf.linalg.matvec(A, x)\n      def grad(dy):\n          return tf.linalg.matvec(tf.transpose(A), dy)\n      return u, grad","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Nonlinear and explicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this case, we have ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"y = F(x)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"where F is explicitly given. We have","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"F(xy) = F(x)-yRightarrow fracpartial Jpartial x = fracpartial Jpartial y F_x(x)","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"One challenge here is we need to implement the matrix vector production fracpartial Jpartial y F_x(x) for grad. ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Linear and implicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this case ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Ay = x","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"We have F(xy) = x-Ay and ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = fracpartial Jpartial yA^-1","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Nonlinear and implicit","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"In this case F(xy)=0 and the corresponding gradient is ","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"fracpartial Jpartial x = -fracpartial Jpartial yF_y^-1F_x","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"This case is the most challenging of the four but widely seen in scientific computing code. In many numerical simulation code, F_y is usually sparse and therefore it is rewarding to exploit the sparse structure for computation acceleration in practice.","category":"page"},{"location":"inverse_modeling/#Related-Algorithms-1","page":"Inverse Modeling","title":"Related Algorithms","text":"","category":"section"},{"location":"inverse_modeling/#EM-Algorithm-1","page":"Inverse Modeling","title":"EM Algorithm","text":"","category":"section"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"We now split the parameter theta=(theta_1 theta_2), i.e., the inverse model is","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"y = F(x (theta_1 theta_2))","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"The expectation maximization algorithm alternates between the steps of guessing a probability distribution of a certain unknown theta_1 (E-Step) given current theta_2  and then re-estimating the other unknown theta_2 (M-Step) assuming the guess theta_1 is true. The name \"expectation\" comes from the fact that usually the optimal guess is the expectation over a probability distribution. The algorithm is as follows","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"beginaligned\ntheta_2^(0) = argmin_theta_2 D_1(hat y F(hat x theta_1^(0) theta_2))\ntheta_1^(1) = argmin_theta_1 D_2(hat y F(hat x theta_1 theta_2^(0)))\ntheta_2^(1) = argmin_theta_2 D_1(hat y F(hat x theta_1^(1) theta_2))\ntheta_1^(2) = argmin_theta_1 D_1(hat y F(hat x theta_1 theta_2^(1)))\nldots\nendaligned","category":"page"},{"location":"inverse_modeling/#","page":"Inverse Modeling","title":"Inverse Modeling","text":"Recall that hat x and hat y represent observations (can be stochastic). D_1 and D_2 are discrepancy metrics, such as KL divergence. We immediately recognize EM algorithm as a coordinate descent method when D_1=D_2. ","category":"page"},{"location":"newton_raphson/#Newton-Raphson-1","page":"Newton Raphson","title":"Newton Raphson","text":"","category":"section"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"Newton-Raphson algorithm is widely used in scientific computing. In ADCME, the function for the algorithm is newton_raphson. And the signature is","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"newton_raphson","category":"page"},{"location":"newton_raphson/#ADCME.newton_raphson","page":"Newton Raphson","title":"ADCME.newton_raphson","text":"newton_raphson(f::Function, u::Union{Array,PyObject}, θ::Union{Missing,PyObject}; options::Union{Dict{String, T}, Missing}=missing)\n\nNewton Raphson solver for solving a nonlinear equation.  f has the signature \n\nf(θ::Union{Missing,PyObject}, u::PyObject)->(r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is off)\nf(θ::Union{Missing,PyObject}, u::PyObject)->(fval::PyObject, r::PyObject, A::Union{PyObject,SparseTensor}) (if linesearch is on)\n\nwhere r is the residual and A is the Jacobian matrix; in the case where linesearch is on, the function value fval must also be supplied. θ are external parameters. u0 is the initial guess for u options:\n\nmax_iter: maximum number of iterations (default=100)\nverbose: whether details are printed (default=false)\nrtol: relative tolerance for termination (default=1e-12)\ntol: absolute tolerance for termination (default=1e-12)\nLM: a float number, Levenberg-Marquardt modification x^k+1 = x^k - (J^k + mu^k)^-1g^k (default=0.0)\nlinesearch: whether linesearch is used (default=false)\n\nCurrently, the backtracing algorithm is implemented. The parameters for linesearch are also supplied via options\n\nls_c1: stop criterion, f(x^k)  f(0) + alpha c_1  f(0)\nls_ρ_hi: the new step size alpha_1leq rho_hialpha_0 \nls_ρ_lo: the new step size alpha_1geq rho_loalpha_0 \nls_iterations: maximum number of iterations for linesearch\nls_maxstep: maximum allowable steps\nls_αinitial: initial guess for the step size alpha\n\n\n\n\n\n","category":"function"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"As an example, assume we want to solve ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"u_i^2 - 1 = 0 i=12ldots 10","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"We first need to construct a function ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"function f(θ, u)\n    return u^2 - 1, 2*spdiag(u)\nend","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"Here 2textttspdiag(u) is the Jacobian matrix for the equation. Then we construct a Newton Raphson solver via","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr = newton_raphson(f, constant(rand(10)))","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr is a NRResult struct which is runnable and can be materialized by ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr = run(sess, nr)","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"The signature for NRResult is ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"struct NRResult\n    x::Union{PyObject, Array{Float64}} # final solution\n    res::Union{PyObject, Array{Float64, 1}} # residual\n    u::Union{PyObject, Array{Float64, 2}} # solution history\n    converged::Union{PyObject, Bool} # whether it converges\n    iter::Union{PyObject, Int64} # number of iterations\nend","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"uin mathbbR^ptimes n where p is the solution dimension and n is the number of iterations. ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"note: Note\nSometimes we want to construct f via some external variables theta, e.g., when theta is a trainable variable and embeded in the Newton-Raphson solver, we can pass this parameter to newton_raphson via the third parameternr = newton_raphson(f, constant(rand(10)),θ)","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"note: Note\nnewton_raphson also accepts a keyword argument options through which we can specify special options for the optimization. For examplenr = newton_raphson(f, constant(rand(10)), missing, \n            options=Dict(\"verbose\"=>true, \"tol\"=>1e-12))This might be useful for debugging.","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"In the case we want to apply a linesearch step in our Newton-Raphson solver, we can turn on the linesearch option in options. However, in this case, we must provide the function value for f (assuming we are solving a minimization problem).  ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"function f(θ, u)\n    return sum(1/3*u^3-u), u^2 - 1, 2*spdiag(u)\nend","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"The corresponding driver code is","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"nr = newton_raphson(f, constant(rand(10)), missing, \n                options=Dict(\"verbose\"=>false, \"tol\"=>1e-12, \"linesearch\"=>true, \"ls_αinitial\"=>1.0))","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"Finally we consider an advanced usage of the code, where we want to create a custom operator that solves","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"y^3-x=0","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"We compute the forward using Newton-Raphson and the backward with the implicit function theorem.","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"using Random\nfunction myop_(x)\n    function f(θ, y)\n        y^3 - x, spdiag(3y^2)\n    end\n    nr = newton_raphson(f, constant(ones(length(x))), options=Dict(\"verbose\"=>true))\n    y = nr.x\n    function myop_grad(dy, y)\n        dy/3y^2\n    end\n    # move variables to python space\n    s = randstring(8)\npy\"\"\"\ny_$$s = $y\ngrad_$$s = $myop_grad\n\"\"\"\n    # workaround \n    g = py\"\"\"lambda dy: grad_$$s(dy, y_$$s)\"\"\"\n    return y, g\nend\ntf_myop = tf.custom_gradient(myop_)","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"note: Note\nHere py\"\"\"lambda dy: grad_$$s(dy, y_$$s)\"\"\" is related to a workaround for converting Julia function to Python function.  Also we need to explicitly put Julia object to Python. ","category":"page"},{"location":"newton_raphson/#","page":"Newton Raphson","title":"Newton Raphson","text":"x = constant(8ones(5))\ny = tf_myop(x)\nprintln(run(sess, y))\n\nl = sum(y)\nrun(sess, gradients(l, x))","category":"page"},{"location":"#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"ADCME is suitable for conducting inverse modeling in scientific computing. The purpose of the package is to: (1) provide differentiable programming framework for scientific computing based on TensorFlow automatic differentiation (AD) backend; (2) adapt syntax to facilitate implementing scientific computing, particularly for numerical PDE discretization schemes; (3) supply missing functionalities in the backend (TensorFlow) that are important for engineering, such as sparse linear algebra, constrained optimization, etc. Applications include","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"full wavelength inversion\nreduced order modeling in solid mechanics\nlearning hidden geophysical dynamics\nphysics based machine learning\nparameter estimation in stochastic processes","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The package inherents the scalability and efficiency from the well-optimized backend TensorFlow. Meanwhile, it provides access to incooperate existing C/C++ codes via the custom operators. For example, some functionalities for sparse matrices are implemented in this way and serve as extendable \"plugins\" for ADCME. ","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Read more about the methodology and philosophy about ADCME: slides.","category":"page"},{"location":"#Getting-Started-1","page":"Overview","title":"Getting Started","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"To install ADCME, simply type the following commands in Julia REPL","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"julia> using Pkg; Pkg.add(\"ADCME\")","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"To enable GPU support for custom operators (if you do not need to compile custom operators, you do not need this step), make sure nvcc command is available on your machine, then","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"ENV[\"GPU\"] = 1\nPkg.build(\"ADCME\")","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"We consider a simple inverse modeling problem: consider the following partial differential equation","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"-bu(x)+u(x)=f(x)quad xin01 u(0)=u(1)=0","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"where ","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"f(x) = 8 + 4x - 4x^2","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Assume that we have observed u(05)=1, we want to estimate b. The true value in this case should be b=1. We can discretize the system using finite difference method, and the resultant linear system will be","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"(bA+I)mathbfu = mathbff","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"where","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"A = beginbmatrix\n        frac2h^2  -frac1h^2  dots  0\n         -frac1h^2  frac2h^2  dots  0\n         dots \n         0  0  dots  frac2h^2\n    endbmatrix quad mathbfu = beginbmatrix\n        u_2\n        u_3\n        vdots\n        u_n\n    endbmatrix quad mathbff = beginbmatrix\n        f(x_2)\n        f(x_3)\n        vdots\n        f(x_n)\n    endbmatrix","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The idea for implementing the inverse modeling method in ADCME is that we make the unknown b a Variable and then solve the forward problem pretending b is known. The following code snippet shows the implementation","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using LinearAlgebra\nusing ADCME\n\nn = 101 # number of grid nodes in [0,1]\nh = 1/(n-1)\nx = LinRange(0,1,n)[2:end-1]\n\nb = Variable(10.0) # we use Variable keyword to mark the unknowns\nA = diagm(0=>2/h^2*ones(n-2), -1=>-1/h^2*ones(n-3), 1=>-1/h^2*ones(n-3)) \nB = b*A + I  # I stands for the identity matrix\nf = @. 4*(2 + x - x^2) \nu = B\\f # solve the equation using built-in linear solver\nue = u[div(n+1,2)] # extract values at x=0.5\n\nloss = (ue-1.0)^2 \n\n# Optimization\nsess = Session(); init(sess) \nBFGS!(sess, loss)\n\nprintln(\"Estimated b = \", run(sess, b))","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The expected output is","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Estimated b = 0.9995582304494237","category":"page"},{"location":"julia_customop/#Julia-Custom-Operators-1","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"warning: Warning\nCurrently, embedding Julia suffers from multithreading issues: calling Julia from a non-Julia thread is not supported in ADCME. When TensorFlow kernel codes are executed concurrently, it is difficult to invoke the Julia functions. See issue.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"In scientific and engineering applications, the operators provided by TensorFlow are not sufficient for high performance computing. In addition, constraining oneself to TensorFlow environment sacrifices the powerful scientific computing ecosystems provided by other languages such as Julia and Python. For example, one might want to code a finite volume method for a sophisticated fluid dynamics problem; it is hard to have the flexible syntax to achieve this goal, obtain performance boost from existing fast solvers such as AMG, and benefit from many other third-party packages within TensorFlow. This motivates us to find a way to \"plugin\" custom operators to TensorFlow.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We have already introduced how to incooperate C++ custom operators.  For many researchers, they usually prototype the solvers in a high level language such as MATLAB, Julia or Python. To enjoy the parallelism and automatic differentiation feature of TensorFlow, they need to port them into C/C++. However, this is also cumbersome sometimes, espeically the original solvers depend on many packages in the high-level language. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We solve this problem by incorporating Julia functions directly into TensorFlow. That is, for any Julia functions, we can immediately convert it to a TensorFlow operator. At runtime, when this operator is executed, the corresponding Julia function is executed. That implies we have the Julia speed. Most importantly, the function is perfectly compitable with the native Julia environment; third-party packages, global variables, nested functions, etc. all work smoothly. Since Julia has the ability to call other languages in a quite elegant and simple manner, such as C/C++, Python, R, Java, this means it is possible to incoporate packages/codes from any supported languages into TensorFlow ecosystem. We need to point out that in TensorFlow, tf.numpy_function can be used to convert a Python function to a TensorFlow operator. However, in the runtime, the speed for this operator falls back to Python (or numpy operation for related parts). This is a drawback. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The key for implementing the mechanism is embedding Julia in C++. Still we need to create a C++ dynamic library for TensorFlow. However, the library is only an interface for invoking Julia code. At runtime, jl_get_function is called to search for the related function in the main module. C++ arrays, which include all the relavant data, are passed to this function through jl_call. It requires routine convertion from C++ arrays to Julia array interfaces jl_array_t*. However, those bookkeeping tasks are programatic and possibly will be automated in the future. Afterwards,Julia returns the result to C++ and thereafter the data are passed to the next operator. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"There are two caveats in the implementation. The first is that due to GIL of Python, we must take care of the thread lock while interfacing with Julia. This was done by putting a guard around th eJulia interface","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n// code here \nPyGILState_Release(py_threadstate);","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The second is the memory mangement of Julia arrays. This was done by defining gabage collection markers explicitly","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\nargs[0] = ...\nargs[1] = ...\n# do something\nJL_GC_POP();","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"This technique is remarkable and puts together one of the best langages in scientific computing and that in machine learning. The work that can be built on ADCME is enormous and significantly reduce the development time. ","category":"page"},{"location":"julia_customop/#Example-1","page":"Julia Custom Operators","title":"Example","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Here we present a simple example. Suppose we want to compute the Jacobian of a two layer neural network fracpartial ypartial x","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"y = W_2tanh(W_1x+b_1)+b_2","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"where x b_1 b_2 yin mathbbR^10, W_1 W_2in mathbbR^100. In TensorFlow, this can be done by computing the gradients fracpartial y_ipartial x for each i. In Julia, we can use ForwardDiff to do it automatically. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"function twolayer(J, x, w1, w2, b1, b2)\n    f = x -> begin\n        w1 = reshape(w1, 10, 10)\n        w2 = reshape(w2, 10, 10)\n        z = w2*tanh.(w1*x+b1)+b2\n    end\n    J[:] = ForwardDiff.jacobian(f, x)[:]\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"To make a custom operator, we first generate a wrapper","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"using ADCME\nmkdir(\"TwoLayer\")\ncd(\"TwoLayer\")\ncustomop()","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We modify custom_op.txt","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"TwoLayer\ndouble x(?)\ndouble w1(?)\ndouble b1(?)\ndouble w2(?)\ndouble b2(?)\ndouble y(?) -> output","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"and run ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"customop()","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Three files are generatedCMakeLists.txt, TwoLayer.cpp and gradtest.jl. Now create a new file TwoLayer.h","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"#include \"julia.h\"\n#include \"Python.h\"\n\nvoid forward(double *y, const double *x, const double *w1, const double *w2, const double *b1, const double *b2, int n){\n    PyGILState_STATE py_threadstate;\n    py_threadstate = PyGILState_Ensure();\n    jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\n    jl_value_t **args;\n    JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\n    args[0] = (jl_value_t*)jl_ptr_to_array_1d(array_type, y, n*n, 0);\n    args[1] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(x), n, 0);\n    args[2] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w1), n*n, 0);\n    args[3] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w2), n*n, 0);\n    args[4] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b1), n, 0);\n    args[5] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b2), n, 0);\n    auto fun = jl_get_function(jl_main_module, \"twolayer\");\n  \tif (fun==NULL) jl_errorf(\"Function not found in Main module.\");\n    else jl_call(fun, args, 6);\n    JL_GC_POP();\n    if (jl_exception_occurred())\n        printf(\"%s \\n\", jl_typeof_str(jl_exception_occurred()));\n    PyGILState_Release(py_threadstate);\n}","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Most of the codes have been explanined except jl_ptr_to_array_1d. This function generates a Julia array wrapper from C++ arrays. The last argument 0 indicates that Julia is not responsible for gabage collection. TwoLayer.cpp should also be modified according to https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Finally, we can test in gradtest.jl ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"two_layer = load_op(\"build/libTwoLayer\", \"two_layer\")\n\n\nw1 = rand(100)\nw2 = rand(100)\nb1 = rand(10)\nb2 = rand(10)\nx = rand(10)\nJ = rand(100)\ntwolayer(J, x, w1, w2, b1, b2)\n\ny = two_layer(constant(x), constant(w1), constant(b1), constant(w2), constant(b2))\nsess = Session(); init(sess)\nJ0 = run(sess, y)\n@show norm(J-J0)","category":"page"},{"location":"julia_customop/#Embedded-in-Modules-1","page":"Julia Custom Operators","title":"Embedded in Modules","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"If the custom operator is intended to be used in a precompiled module, we can load the dynamic library at initialization","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"global my_op \nfunction __init__()\n\tglobal my_op = load_op(\"$(@__DIR__)/path/to/libMyOp\", \"my_op\")\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The corresponding Julia function called by my_op must be exported in the module (such that it is in the Main module when invoked). One such example is given in MyModule","category":"page"},{"location":"julia_customop/#Reference-Sheet-1","page":"Julia Custom Operators","title":"Reference Sheet","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"For implementation reference, see Reference Sheet","category":"page"},{"location":"tutorial/#ADCME-Tutorial-1","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The Gateway to Inverse Modeling with Physics Based Machine Learning","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Overview","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"ADCME is an open-source Julia package for inverse modeling in scientific computing using automatic differentiation. The backend of ADCME is the high performance deep learning framework, TensorFlow, which provides parallel computing and automatic differentiation features based on computational graph, but  ADCME augments TensorFlow by functionalities–-like sparse linear algebra–-essential for scientific computing. ADCME leverages the Julia environment for maximum efficiency of computing. Additionally, the syntax of ADCME is designed from the beginning to be compatible with the Julia syntax, which is friendly for scientific computing. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Prerequisites","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The tutorial does not assume readers with experience in deep learning. However, basic knowledge of scientific computing in Julia is required. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Tutorial Series","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"What is ADCME? Computational Graph, Automatic Differentiation & TensorFlow","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"How to install ADCME?","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"ADCME Basics: Tensor, Type, Operator, Session & Kernel","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Mathematical Minimization with ADCME","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Sparse Linear Algebra in ADCME","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Numerical Scheme in ADCME: Finite Difference Example","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Numerical Scheme in ADCME: Finite Element Example","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Inverse Modeling in ADCME","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Neural Network Tutorial: Combining NN with Numerical Schemes ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Advanced: Automatic Differentiation for Linear Implicit Operations ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Advanced: Automatic Differentiation for Nonlinear Implicit Operators","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Advanced: Custom Operators ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Advanced: Debugging ","category":"page"},{"location":"tutorial/#What-is-ADCME?-Computational-Graph,-Automatic-Differentiation-and-TensorFlow-1","page":"ADCME Tutorial","title":"What is ADCME? Computational Graph, Automatic Differentiation & TensorFlow","text":"","category":"section"},{"location":"tutorial/#Computational-Graph-1","page":"ADCME Tutorial","title":"Computational Graph","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"A computational graph is a functional description of the required computation. In the computationall graph, an edge represents a value, such as a scalar, a vector, a matrix or a tensor. A node represents a function whose input arguments are the the incoming edges and output values are are the outcoming edges. Based on the number of input arguments, a function can be nullary, unary, binary, ..., and n-ary; based on the number of output arguments, a function can be single-valued or multiple-valued. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Computational graphs are directed and acyclic. The acyclicity implies the forward propagation computation is well-defined: we loop over edges in topological order and evaluates the outcoming edges for each node. To make the discussion more concrete, we illustrate the computational graph for ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"z = sin(x_1+x_2) + x_2^2 x_3","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"There are in general two programmatic ways to construct computational graphs: static and dynamic declaration. In the static declaration, the computational graph is first constructed symbolically, i.e., no actual numerical arithmetic are executed. Then a bunch of data is fed to the graph for the actual computation. An advantage of static declarations is that they allow for graph optimization such as removing unused branches. Additionally, the dependencies can be analyzed for parallel execution of independent components. Another approach is the dynamic declaration, where the computational graph is constructed on-the-fly as the forward computation is executed. The dynamic declaration interleaves construction and evaluation of the graph, making software development more intuitive. ","category":"page"},{"location":"tutorial/#Automatic-Differentiation-1","page":"ADCME Tutorial","title":"Automatic Differentiation","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(This section can be skipped in the first reading.)","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"An important application of computational graphs is automatic differentiation (AD). In general, there are three modes of AD: reverse-mode, forward-mode, and mixed mode. In this tutorial, we focus on the reverse-mode, which computes the gradients with respect to independent variables by backward propagation, i.e., loop over the edges in reverse topological order starting with a final goal edge. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"To explain how reverse-mode AD works, let's consider constructing a computational graph with independent variables ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"x_1 x_2 ldots x_n","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"and the forward propagation produces a single output x_N, Nn. The gradients fracpartial x_N(x_1 x_2 ldots x_n)partial x_i i=1, 2, ldots, n are queried. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The idea is that this algorithm can be decomposed into a sequence of functions f_i (i=n+1 n+2 ldots N) that can be easily differentiated analytically, such as addition, multiplication, or basic functions like exponential, logarithm and trigonometric functions. Mathematically, we can formulate it as","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"beginaligned\n    x_n+1 = f_n+1(mathbfx_pi(n+1))\n    x_n+2 = f_n+2(mathbfx_pi(n+2))\n    ldots\n    x_N = f_N(mathbfx_pi(N))\nendaligned","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"where mathbfx = x_i_i=1^N and pi(i) are the parents of x_i, s.t., pi(i) in 12ldotsi-1.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The idea to compute partial x_N  partial x_i is to start from i = N, and establish recurrences to calculate derivatives with respect to x_i in terms of derivatives with respect to x_j, j i. To define these recurrences rigorously, we need to define different functions that differ by the choice of independent variables.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The starting point is to define x_i considering all previous x_j, j  i, as independent variables. Then:","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"x_i(x_1 x_2 ldots x_i-1) = f_i(mathbfx_pi(i))","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Next, we observe that x_i-1 is a function of previous x_j, j  i-1, and so on; so that we can recursively define x_i in terms of fewer independent variables, say in terms of x_1, ..., x_k, with k  i-1. This is done recursively using the following definition:","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"x_i(x_1 x_2 ldots x_j) = x_i(x_1 x_2 ldots x_j f_j+1(mathbfx_pi(j+1))) quad n  j+1  i","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Observe that the function of the left-hand side has j arguments, while the function on the right has j+1 arguments. This equation is used to \"reduce\" the number of arguments in x_i.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"With these definitions, we can define recurrences for our partial derivatives which form the basis of the back-propagation algorithm. The partial derivatives for","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"x_N(x_1 x_2 ldots x_N-1)","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"are readily available since we can differentiate","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"f_N(mathbfx_pi(N))","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"directly. The problem is therefore to calculate partial derivatives for functions of the type x_N(x_1 x_2 ldots x_i) with iN-1. This is done using the following recurrence:","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"fracpartial x_N(x_1 x_2 ldots x_i)partial x_i = sum_jiin pi(j)\n    fracpartial x_N(x_1 x_2 ldots x_j)partial x_j\n    fracpartial x_j(x_1 x_2 ldots x_j-1)partial x_i","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"with n  i N-1. Since i in pi(j), we have i  j. So we are defining derivatives with respect to x_i in terms of derivatives with respect to x_j with j  i. The last term","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"fracpartial x_j(x_1 x_2 ldots x_j-1)partial x_k","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"is readily available since:","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"x_j(x_1 x_2 ldots x_j-1) = f_j(mathbfx_pi(j))","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The computational cost of this recurrence is proportional to the number of edges in the computational graph (excluding the nodes 1 through n), assuming that the cost of differentiating f_k is O(1). The last step is defining","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"fracpartial x_N(x_1 x_2 ldots x_n)partial x_i = sum_jiin pi(j)\n    fracpartial x_N(x_1 x_2 ldots x_j)partial x_j\n    fracpartial x_j(x_1 x_2 ldots x_j-1)partial x_i","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"with 1 le i le n. Since n  j, the first term","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"fracpartial x_N(x_1 x_2 ldots x_j)partial x_j","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"has already been computed in earlier steps of the algorithm. The computational cost is equal to the number of edges connected to one of the nodes in 1 dots n.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"We can see that the complexity of the back-propagation is bounded by that of the forward step, up to a constant factor. Reverse mode differentiation is very useful in the penalty method, where the loss function is a scalar, and no other constraints are present. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"As a concrete example, we consider the example of evaluating fracdz(x_1x_2x_3)dx_i, where z = sin(x_1+x_2) + x_2^2x_3. The gradients are  backward propagated exactly in the reverse order of the forward propagation. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#TensorFlow-1","page":"ADCME Tutorial","title":"TensorFlow","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Google's TensorFlow provides a convenient way to specify the computational graph statically. TensorFlow  has automatic differentiation features and its performance is optimized for large-scale computing. ADCME is built on TensorFlow by overloading numerical operators and augmenting TensorFlow with essential scientific computing functionalities. We contrast the TensorFlow implementation with the ADCME implementation of computing the objective function and its gradient in the following example.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"y(x) = (AA^T+xI)^-1b-c^2  z = y(x)","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"where Ain mathbbR^ntimes n is a random matrix, xbc are scalars, and n=10.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"TensorFlow Implementation","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"import tensorflow as tf\nimport numpy as np \nA = tf.constant(np.random.rand(10,10), dtype=tf.float64)\nx = tf.constant(1.0, dtype=tf.float64)\nb = tf.constant(np.random.rand(10), dtype=tf.float64)\nc = tf.constant(np.random.rand(10), dtype=tf.float64)\nB = tf.matmul(A, tf.transpose(A)) + x * tf.constant(np.identity(10))\ny = tf.reduce_sum((tf.squeeze(tf.matrix_solve(B, tf.reshape(b, (-1,1))))-c)**2)\nz = tf.gradients(y, x)[0]\nsess = tf.Session()\nsess.run([y, z])","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Julia Implementation","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"using ADCME, LinearAlgebra\nA = constant(rand(10,10))\nx = constant(1.0)\nb = rand(10)\nc = rand(10)\ny = sum(((A*A'+x*diagm(0=>ones(10)))\\b - c)^2)\nz = gradients(y, x)\nsess = Session()\nrun(sess, [y,z])","category":"page"},{"location":"tutorial/#Summary-1","page":"ADCME Tutorial","title":"Summary","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The computational graph and automatic differentiation are the core concepts underlying ADCME. TensorFlow works as the workhorse for optimion and execution of the computational graph in a high performance environment. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"To construct a computational graph for a Julia program, ADCME overloads most numerical operators like +, -, *, / and matrix multiplication in Julia by the corresponding TensorFlow operators. Therefore, you will find many similar workflows and concepts as TensorFlow, such as constant, Variable, session, etc. However, not all operators relevant to scientific computing in Julia have its counterparts in TensorFlow. To that end, custom kernels are implemented to supplement TensorFlow, such as sparse linear algebra related functions. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"ADCME aims at providing a easy-to-use, flexible,  and high performance interface to do data processing, implement numerical schemes, and conduct mathematical optimization. It is built not only for academic interest but also for real-life large-scale simulations. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Like TensorFlow, ADCME works in sessions, in which each session consumes a computational graph. Usually the workflow is split into three steps:","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Define independent variables. constant for tensors that do not require gradients and Variable for those requiring gradients. \na = constant(0.0)\nConstruct the computational graph by defining the computation\nL = (a-1)^2\nCreate a session and run the computational graph\nsess = Session()\nrun(sess, L)","category":"page"},{"location":"tutorial/#ADCME-Basics:-Tensor,-Type,-Operator,-Session-and-Kernel-1","page":"ADCME Tutorial","title":"ADCME Basics: Tensor, Type, Operator, Session & Kernel","text":"","category":"section"},{"location":"tutorial/#Tensors-and-Operators-1","page":"ADCME Tutorial","title":"Tensors and Operators","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Tensor is a data structure for storing structured data, such as a scalar, a vector, a matrix or a high dimensional tensor. The name of the ADCME backend, TensorFlow, is also derived from its core framework, Tensor. Tensors can be viewed as symbolic versions of Julia's Array. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"A tensor is a collection of n-dimensional arrays. ADCME represents tensors using a PyObject handle to the TensorFlow Tensor data structure. A tensor has three important properties","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"name: Each Tensor admits a unique name. \nshape: For scalars, the shape is always an empty tuple (); for n-dimensional vectors, the shape is (n,); for matrices or higher order tensors, the shape has the form (n1, n2, ...)\ndtype: The type of the tensors. There is a one-to-one correspondence between most TensorFlow types and Julia types (e.g., Int64, Int32, Float64, Float32, String, and Bool). Therefore, we have overloaded the type name so users have a unified interface. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"An important difference is that tensor object stores data in the row-major while Julia's default for Array is column major. The difference may affect performance if not carefully dealt with, but more often than not, the difference is not relevant if you do not convert data between Julia and Python often. Here is a representation of ADCME tensor","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"There are 4 ways to create tensors. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"constant. As the name suggests, constant creates an immutable tensor from Julia Arrays. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"constant(1.0)\nconstant(rand(10))\nconstant(rand(10,10))","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Variable. In contrast to constant, Variable creates tensors that are mutable. The mutability allows us to update the tensor values, e.g., in an optimization procedure. It is very important to understand the difference between constant and Variable: simply put, in inverse modeling, tensors that are defined as Variable should be the quantity you want to invert, while constant is a way to provide known data.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Variable(1.0)\nVariable(rand(10))\nVariable(rand(10,10))","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"placeholder. placeholder is a convenient way to specify a tensor whose values are to be provided in the runtime. One use case is that you want to try out different values for this tensor and scrutinize the simulation result. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"placeholder(Float64, shape=[10,10])\nplaceholder(rand(10)) # default value is `rand(10)`","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"SparseTensor. SparseTensor is a special data structure to store a sparse matrix. Although it is not very emphasized in machine learning, sparse linear algebra is one of the cores to scientific computing. Thus possessing a strong sparse linear algebra support is the key to success inverse modeling with physics based machine learning. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"using SparseArrays\nSparseTensor(sprand(10,10,0.3))\nSparseTensor([1,2,3],[2,2,2],[0.1,0.3,0.5],3,3) # specify row, col, value, number of rows, number of columns","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Now we know how to create tensors, the next step is to perform mathematical operations on those tensors.","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"Operator can be viewed as a function that takes multiple tensors and outputs multiple tensors. In the computational graph, operators are represented by nodes while tensors are represented by edges. Most mathematical operators, such as +, -, * and /, and matrix operators, such as matrix-matrix multiplication, indexing and linear system solve, also work on tensors. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"a = constant(rand(10,10))\nb = constant(rand(10))\na + 1.0 # add 1 to every entry in `a`\na * b # matrix vector production\na * a # matrix matrix production\na .* a # element wise production\ninv(a) # matrix inversion","category":"page"},{"location":"tutorial/#Session-1","page":"ADCME Tutorial","title":"Session","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"With the aforementioned syntax to create and transform tensors, we have created a computational graph. However, at this point, all the operations are symbolic, i.e., the operators have not been executed yet. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"To trigger the actual computing, the TensorFlow mechanism is to create a session, which drives the graph based optimization (like detecting dependencies) and executes all the operations.  ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"a = constant(rand(10,10))\nb = constant(rand(10))\nc = a * b\nsess = Session()\nrun(sess, c) # syntax for triggering the execution of the graph","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"If your computational graph contains Variables, which can be listed via get_collection, then you must initialize your graph before any run command, in which the Variables are populated with initial values","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"init(sess)","category":"page"},{"location":"tutorial/#Kernel-1","page":"ADCME Tutorial","title":"Kernel","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"The kernels provide the low level C++ implementation for the operators. ADCME augments users with missing features in TensorFlow that are crucial for scientific computing and tailors the syntax for numerical schemes. Those kernels, depending on their implementation, can be used in CPU, GPU, TPU or heterogenious computing environments. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"All the intensive computations are  done either in Julia or C++, and therefore we can achieve very high performance if the logic is done appropriately. For performance critical part, users may resort to custom kernels using custom_op, which allows you to incooperate custom designed C++ codes. ","category":"page"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Summary-2","page":"ADCME Tutorial","title":"Summary","text":"","category":"section"},{"location":"tutorial/#","page":"ADCME Tutorial","title":"ADCME Tutorial","text":"ADCME performances operations on tensors. The actual computations are pushed back to low level C++ kernels via operators. A session is need to drive the executation of the computation. It will be easier for you to analyze computational cost and optimize your codes with this computation model in mind. ","category":"page"}]
}
