var documenterSearchIndex = {"docs":
[{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-Julia-Custom-Operator-in-ADCMAE-1","page":"-","title":"Quick Reference for Implementing Julia Custom Operator in ADCMAE","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Header files","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"  #include \"julia.h\"\n  #include \"Python.h\"","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"For Python GIL workround","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n...\nPyGILState_Release(py_threadstate);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Get function from Julia main module ","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_get_function(jl_main_module, \"myfun\");","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ to Julia","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t *a = jl_box_float64(3.0);\njl_value_t *b = jl_box_float32(3.0f);\njl_value_t *c = jl_box_int32(3);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia to C++","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double ret_unboxed = jl_unbox_float64(ret);\nfloat  ret_unboxed = jl_unbox_float32(ret);\nint32  ret_unboxed = jl_unbox_int32(ret);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"C++ Arrays to Julia Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\njl_array_t* x          = jl_alloc_array_1d(array_type, 10);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"or for existing arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *existingArray = (double*)malloc(sizeof(double)*10);\njl_array_t *x = jl_ptr_to_array_1d(array_type, existingArray, 10, 0);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Julia Arrays to C++ Arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"double *xData = (double*)jl_array_data(x);","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Call Julia Functions","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_array_t *y = (jl_array_t*)jl_call1(func, (jl_value_t*)x);\njl_value_t *jl_call(jl_function_t *f, jl_value_t **args, int32_t nargs)","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Gabage collection","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 2); // args can now hold 2 `jl_value_t*` objects\nargs[0] = some_value;\nargs[1] = some_other_value;\n// Do something with args (e.g. call jl_... functions)\nJL_GC_POP();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Reference: Embedding Julia","category":"page"},{"location":"customop_reference_sheet/#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1","page":"-","title":"Quick Reference for Implementing C++ Custom Operators in ADCME","text":"","category":"section"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Set output shape","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"c->set_output(0, c->Vector(n));\nc->set_output(0, c->Matrix(m, n));\nc->set_output(0, c->Scalar());","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Names","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":".Input and .Ouput : names must be in lower case, no _, only letters.","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"TensorFlow Input/Output to TensorFlow Tensors","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.vec<double>();\ngrad.scalar<double>();\ngrad.matrix<double>();\ngrad.flat<double>();","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Obtain flat arrays","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"grad.flat<double>().data()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Scalars","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate scalars using TensorShape()","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Allocate Shapes","category":"page"},{"location":"customop_reference_sheet/#","page":"-","title":"-","text":"Although you can use -1 for shape reference, you must allocate exact shapes in Compute","category":"page"},{"location":"extra/#Miscellaneous-Tools-1","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"There are many handy tools implemented in ADCME for analysis, benchmarking, input/output, etc. ","category":"page"},{"location":"extra/#Debugging-and-printing-1","page":"Miscellaneous Tools","title":"Debugging and printing","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Add the following line before Session and change tf.Session to see verbose printing (such as GPU/CPU information)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.debugging.set_log_device_placement(true)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"tf.print can be used for printing tensor values. It must be binded with an executive operator.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"# a, b are tensors, and b is executive\nop = tf.print(a)\nb = bind(b, op)","category":"page"},{"location":"extra/#Profiling-1","page":"Miscellaneous Tools","title":"Profiling","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Profiling can be done with the help of run_profile and save_profile","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"a = normal(2000, 5000)\nb = normal(5000, 1000)\nres = a*b \nrun_profile(sess, res)\nsave_profile(\"test.json\")","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Open Chrome and navigate to chrome://tracing\nLoad the timeline file","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Below shows an example of profiling results.","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"(Image: )","category":"page"},{"location":"extra/#Save-and-Load-Python-Object-1","page":"Miscellaneous Tools","title":"Save and Load Python Object","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"psave\npload","category":"page"},{"location":"extra/#ADCME.psave","page":"Miscellaneous Tools","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.pload","page":"Miscellaneous Tools","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-TensorFlow-Session-1","page":"Miscellaneous Tools","title":"Save and Load TensorFlow Session","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"load\nsave","category":"page"},{"location":"extra/#ADCME.load","page":"Miscellaneous Tools","title":"ADCME.load","text":"load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nLoads the values of variables to the session sess from the file file. If vars is nothing, it loads values to all the trainable variables. See also save, load\n\n\n\n\n\nload(sw::Diary, dirp::String)\n\nLoads Diary from dirp.\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.save","page":"Miscellaneous Tools","title":"ADCME.save","text":"save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nSaves the values of vars in the session sess. The result is written into file as a dictionary. If vars is nothing, it saves all the trainable variables. See also save, load\n\n\n\n\n\nsave(sw::Diary, dirp::String)\n\nSaves Diary to dirp.\n\n\n\n\n\n","category":"function"},{"location":"extra/#Save-and-Load-Diary-1","page":"Miscellaneous Tools","title":"Save and Load Diary","text":"","category":"section"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"We can use TensorBoard to track a scalar value easily","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"d = Diary(\"test\")\np = placeholder(1.0, dtype=Float64)\nb = constant(1.0)+p\ns = scalar(b, \"variable\")\nfor i = 1:100\n    write(d, i, run(sess, s, Dict(p=>Float64(i))))\nend\nactivate(d)","category":"page"},{"location":"extra/#","page":"Miscellaneous Tools","title":"Miscellaneous Tools","text":"Diary\nscalar\nactivate\nload\nsave\nwrite","category":"page"},{"location":"extra/#ADCME.Diary","page":"Miscellaneous Tools","title":"ADCME.Diary","text":"Diary(suffix::Union{String, Nothing}=nothing)\n\nCreates a diary at a temporary directory path. It returns a writer and the corresponding directory path\n\n\n\n\n\n","category":"type"},{"location":"extra/#ADCME.scalar","page":"Miscellaneous Tools","title":"ADCME.scalar","text":"scalar(o::PyObject, name::String)\n\nReturns a scalar summary object.\n\n\n\n\n\n","category":"function"},{"location":"extra/#ADCME.activate","page":"Miscellaneous Tools","title":"ADCME.activate","text":"activate(sw::Diary, port::Int64=6006)\n\nRunning Diary at http://localhost:port.\n\n\n\n\n\n","category":"function"},{"location":"extra/#Base.write","page":"Miscellaneous Tools","title":"Base.write","text":"write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})\n\nWrites to Diary.\n\n\n\n\n\nwrite(ta::PyObject, i::Union{PyObject,Integer}, obj)\n\nWrites data obj to TensorArray at index i.\n\n\n\n\n\n","category":"function"},{"location":"api/#API-Reference-1","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Core-Functions-1","page":"API Reference","title":"Core Functions","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"ADCME.jl\", \"core.jl\", \"run.jl\"]","category":"page"},{"location":"api/#ADCME.add_collection-Tuple{String,PyObject}","page":"API Reference","title":"ADCME.add_collection","text":"add_collection(name::String, v::PyObject)\n\nAdds v to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.add_collection-Tuple{String,Vararg{PyObject,N} where N}","page":"API Reference","title":"ADCME.add_collection","text":"add_collection(name::String, vs::PyObject...)\n\nAdds operators vs to the collection with name name. If name does not exist, a new one is created.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.get_collection","page":"API Reference","title":"ADCME.get_collection","text":"get_collection(name::Union{String, Missing})\n\nReturns the collection with name name. If name is missing, returns all the trainable variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.has_gpu-Tuple{}","page":"API Reference","title":"ADCME.has_gpu","text":"has_gpu()\n\nChecks if GPU is available.\n\nnote: Note\nADCME will use GPU automatically if GPU is available. To disable GPU, set the environment variable ENV[\"CUDA_VISIBLE_DEVICES\"]=\"\" before importing ADCME \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.if_else-Tuple{Union{Bool, PyObject, Array},Any,Any,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.if_else","text":"if_else(condition::Union{PyObject,Array,Bool}, fn1, fn2, args...;kwargs...)\n\nIf condition is a scalar boolean, it outputs fn1 or fn2 (a function with no input argument or a tensor) based on whether condition is true or false.\nIf condition is a boolean array, if returns condition .* fn1 + (1 - condition) .* fn2\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.reset_default_graph-Tuple{}","page":"API Reference","title":"ADCME.reset_default_graph","text":"reset_default_graph()\n\nResets the graph by removing all the operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.stop_gradient-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"ADCME.stop_gradient","text":"stop_gradient(o::PyObject, args...;kwargs...)\n\nDisconnects o from gradients backpropagation. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Tuple{String}","page":"API Reference","title":"ADCME.tensor","text":"tensor(s::String)\n\nReturns the tensor with name s. See tensorname.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensorname-Tuple{PyObject}","page":"API Reference","title":"ADCME.tensorname","text":"tensorname(o::PyObject)\n\nReturns the name of the tensor. See tensor.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.run_profile-Tuple","page":"API Reference","title":"ADCME.run_profile","text":"run_profile(args...;kwargs...)\n\nRuns the session with tracing information.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save_profile","page":"API Reference","title":"ADCME.save_profile","text":"save_profile(filename::String=\"default_timeline.json\")\n\nSave the timeline information to file filename. \n\nOpen Chrome and navigate to chrome://tracing\nLoad the timeline file\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.bind-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"Base.bind","text":"bind(op::PyObject, ops...)\n\nAdding operations ops to the dependencies of op. The function is useful when we want to execute ops but ops is not  in the dependency of the final output. For example, if we want to print i each time i is evaluated\n\ni = constant(1.0)\nop = tf.print(i)\ni = bind(i, op)\n\n\n\n\n\n","category":"method"},{"location":"api/#Variables-1","page":"API Reference","title":"Variables","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"variable.jl\"]","category":"page"},{"location":"api/#ADCME.TensorArray","page":"API Reference","title":"ADCME.TensorArray","text":"TensorArray(size_::Int64=0, args...;kwargs...)\n\nConstructs a tensor array for while_loop.  \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.Variable-Tuple{Any}","page":"API Reference","title":"ADCME.Variable","text":"Variable(initial_value;kwargs...)\n\nConstructs a ref tensor from value. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.constant-Tuple{Any}","page":"API Reference","title":"ADCME.constant","text":"constant(value; kwargs...)\n\nConstructs a non-trainable tensor from value.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.convert_to_tensor-Union{Tuple{Union{Missing, Nothing, Number, PyObject, Array{T,N} where N}}, Tuple{T}} where T<:Number","page":"API Reference","title":"ADCME.convert_to_tensor","text":"convert_to_tensor(o::Union{PyObject, Number, Array{T}, Missing, Nothing}; dtype::Union{Type, Missing}=missing) where T<:Number\n\nConverts the input o to tensor. If o is already a tensor and dtype (if provided) is the same as that of o, the operator does nothing. Otherwise, convert_to_tensor converts the numerical array to a constant tensor or casts the data type.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.gradient_checkpointing","page":"API Reference","title":"ADCME.gradient_checkpointing","text":"gradient_checkpointing(type::String=\"speed\")\n\nUses checkpointing scheme for gradients. \n\n'speed':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,   so checkpointing them maximizes the running speed   (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n'memory': try to minimize the memory usage   (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n'collection': look for a tensorflow collection named 'checkpoints', which holds the tensors to checkpoint\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.gradients-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.gradients","text":"gradients(ys::PyObject, xs::PyObject; kwargs...)\n\nComputes the gradients of ys w.r.t xs. \n\nIf ys is a scalar, gradients returns the gradients with the same shape as xs.\nIf ys is a vector, gradients returns the Jacobian fracpartial ypartial x\n\n!!! \n\nThe second usage is not suggested since `ADCME` adopts reverse mode automatic differentiation. \nAlthough in the case `ys` is a vector and `xs` is a scalar, `gradients` cleverly uses forward mode automatic differentiation,\nit requires that the second order gradients are implemented for relevant operators. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.hessian-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.hessian","text":"hessian computes the hessian of a scalar function f with respect to vector inputs xs\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Union{Tuple{Array{T,1}}, Tuple{T}} where T","page":"API Reference","title":"ADCME.tensor","text":"tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.tensor-Union{Tuple{Array{T,2}}, Tuple{T}} where T","page":"API Reference","title":"ADCME.tensor","text":"tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T\n\nConvert a generic array v to a tensor. For example, \n\nv = [0.0 constant(1.0) 2.0\n    constant(2.0) 0.0 1.0]\nu = tensor(v)\n\nu will be a 2times 3 tensor. \n\nnote: Note\nThis function is expensive. Use with caution.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.read-Tuple{PyObject,Union{Integer, PyObject}}","page":"API Reference","title":"Base.read","text":"read(ta::PyObject, i::Union{PyObject,Integer})\n\nReads data from TensorArray at index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.write-Tuple{PyObject,Union{Integer, PyObject},PyObject}","page":"API Reference","title":"Base.write","text":"write(ta::PyObject, i::Union{PyObject,Integer}, obj)\n\nWrites data obj to TensorArray at index i.\n\n\n\n\n\n","category":"method"},{"location":"api/#Random-Variables-1","page":"API Reference","title":"Random Variables","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"random.jl\"]","category":"page"},{"location":"api/#ADCME.categorical-Tuple{Union{Integer, PyObject}}","page":"API Reference","title":"ADCME.categorical","text":"categorical(n::Union{PyObject, Integer}; kwargs...)\n\nkwargs has a keyword argument logits, a 2-D Tensor with shape [batch_size, num_classes].   Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.choice-Tuple{Union{PyObject, Array},Union{Integer, PyObject}}","page":"API Reference","title":"ADCME.choice","text":"choice(inputs::Union{PyObject, Array}, n_samples::Union{PyObject, Integer};replace::Bool=false)\n\nChoose n_samples samples from inputs with/without replacement. \n\n\n\n\n\n","category":"method"},{"location":"api/#Sparse-Matrix-1","page":"API Reference","title":"Sparse Matrix","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"sparse.jl\"]","category":"page"},{"location":"api/#ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}","page":"API Reference","title":"ADCME.SparseTensor","text":"SparseTensor(A::SparseMatrixCSC)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseTensor-Union{Tuple{S}, Tuple{T}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S}}, Tuple{Union{Array{T,1}, PyObject},Union{Array{T,1}, PyObject},Union{Array{Float64,1}, PyObject},Union{Nothing, PyObject, S},Union{Nothing, PyObject, S}}} where S<:Integer where T<:Integer","page":"API Reference","title":"ADCME.SparseTensor","text":"SparseTensor(I::Union{PyObject,Array{T,1}}, J::Union{PyObject,Array{T,1}}, V::Union{Array{Float64,1}, PyObject}, m::Union{S, PyObject, Nothing}=nothing, n::Union{S, PyObject, Nothing}=nothing) where {T<:Integer, S<:Integer}\n\nConstructs a sparse tensor.  Examples:\n\nii = [1;2;3;4]\njj = [1;2;3;4]\nvv = [1.0;1.0;1.0;1.0]\ns = SparseTensor(ii, jj, vv, 4, 4)\ns = SparseTensor(sprand(10,10,0.3))\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.SparseAssembler-Tuple{}","page":"API Reference","title":"ADCME.SparseAssembler","text":"accumulator, creater, initializer = SparseAssembler()\n\nReturns 3 functions that can be used for assembling sparse matrices concurrently.\n\ninitializer must be called before the working session\naccumulator accumulates column indices and values \ncreator accepts no input and outputs row indices, column indices and values for the sparse matrix\n\nExample:\n\naccumulator, creater, initializer = SparseAssembler()\ninitializer(5)\nop1 = accumulator(1, [1;2;3], ones(3))\nop2 = accumulator(1, [3], [1.])\nop3 = accumulator(2, [1;3], ones(2))\nrun(sess, [op1,op2,op3])\nii,jj,vv = creater()\ni,j,v = run(sess, [ii,jj,vv])\nA = sparse(i,j,v,5,5)\n@assert Array(A)≈[1.0  1.0  2.0  0.0  0.0\n                1.0  0.0  1.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0  0.0]\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.find-Tuple{SparseTensor}","page":"API Reference","title":"ADCME.find","text":"find(s::SparseTensor)\n\nReturns the row, column and values for sparse tensor s.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{Int64}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(n::Int64)\n\nConstructs a sparse identity matrix of size ntimes n.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spdiag-Tuple{PyObject}","page":"API Reference","title":"ADCME.spdiag","text":"spdiag(o::PyObject)\n\nConstructs a sparse diagonal matrix where the diagonal entries are o\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.spzero","page":"API Reference","title":"ADCME.spzero","text":"spzero(m::Int64, n::Union{Missing, Int64}=missing)\n\nConstructs a empty sparse matrix of size mtimes n. n=m if n is missing\n\n\n\n\n\n","category":"function"},{"location":"api/#Operations-1","page":"API Reference","title":"Operations","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"ops.jl\"]","category":"page"},{"location":"api/#ADCME.vector-Union{Tuple{T}, Tuple{Union{PyObject, StepRange, UnitRange, Array{T,N} where N},Union{PyObject, Array{Float64,N} where N},Union{Int64, PyObject}}} where T<:Integer","page":"API Reference","title":"ADCME.vector","text":"vector(i::Union{Array{T}, PyObject, UnitRange, StepRange}, v::Union{Array{Float64},PyObject},s::Union{Int64,PyObject})\n\nReturns a vector V with length s such that\n\nV[i] = v\n\n\n\n\n\n","category":"method"},{"location":"api/#LinearAlgebra.svd-Tuple{PyObject,Vararg{Any,N} where N}","page":"API Reference","title":"LinearAlgebra.svd","text":"svd(o::PyObject, args...; kwargs...)\n\nReturns a TFSVD structure which holds the following data structures\n\nS::PyObject\nU::PyObject\nV::PyObject\nVt::PyObject\n\nWe have the equality o = USV\n\n\n\n\n\n","category":"method"},{"location":"api/#IO-1","page":"API Reference","title":"IO","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"io.jl\"]","category":"page"},{"location":"api/#ADCME.Diary","page":"API Reference","title":"ADCME.Diary","text":"Diary(suffix::Union{String, Nothing}=nothing)\n\nCreates a diary at a temporary directory path. It returns a writer and the corresponding directory path\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.activate","page":"API Reference","title":"ADCME.activate","text":"activate(sw::Diary, port::Int64=6006)\n\nRunning Diary at http://localhost:port.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load","page":"API Reference","title":"ADCME.load","text":"load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nLoads the values of variables to the session sess from the file file. If vars is nothing, it loads values to all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.load-Tuple{Diary,String}","page":"API Reference","title":"ADCME.load","text":"load(sw::Diary, dirp::String)\n\nLoads Diary from dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.pload-Tuple{String}","page":"API Reference","title":"ADCME.pload","text":"pload(file::String)\n\nLoads a Python objection from file. See also psave\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.psave-Tuple{PyObject,String}","page":"API Reference","title":"ADCME.psave","text":"psave(o::PyObject, file::String)\n\nSaves a Python objection o to file. See also pload\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.save","page":"API Reference","title":"ADCME.save","text":"save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)\n\nSaves the values of vars in the session sess. The result is written into file as a dictionary. If vars is nothing, it saves all the trainable variables. See also save, load\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.save-Tuple{Diary,String}","page":"API Reference","title":"ADCME.save","text":"save(sw::Diary, dirp::String)\n\nSaves Diary to dirp.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.scalar","page":"API Reference","title":"ADCME.scalar","text":"scalar(o::PyObject, name::String)\n\nReturns a scalar summary object.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.write-Tuple{Diary,Int64,Union{String, Array{String,N} where N}}","page":"API Reference","title":"Base.write","text":"write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})\n\nWrites to Diary.\n\n\n\n\n\n","category":"method"},{"location":"api/#Optimization-1","page":"API Reference","title":"Optimization","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"optim.jl\"]","category":"page"},{"location":"api/#ADCME.BFGS!","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(value_and_gradients_function::Function, initial_position::Union{PyObject, Array{Float64}}, max_iter::Int64=50, args...;kwargs...)\n\nApplies the BFGS optimizer to value_and_gradients_function\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, max_iter::Int64=15000; kwargs...)\n\nBFGS! is a simplified interface for BFGS optimizer. See also ScipyOptimizerInterface.\n\nexample\n\na = Variable(1.0)\nloss = (a - 10.0)^2\nBFGS!(sess, loss)\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.BFGS!-Union{Tuple{T}, Tuple{PyObject,PyObject,Union{Nothing, PyObject, Array{T,N} where N},Union{PyObject, Array{PyObject,N} where N}}} where T<:Union{Nothing, PyObject}","page":"API Reference","title":"ADCME.BFGS!","text":"BFGS!(sess::PyObject, loss::PyObject, grads::Union{Array{T},Nothing,PyObject}, \n    vars::Union{Array{PyObject},PyObject}; kwargs...) where T<:Union{Nothing, PyObject}\n\nRunning BFGS algorithm min_textttvars textttloss(textttvars) The gradients grads must be provided. Typically, grads[i] = gradients(loss, vars[i]).  grads[i] can exist on different devices (GPU or CPU). \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.CustomOptimizer-Tuple{Function}","page":"API Reference","title":"ADCME.CustomOptimizer","text":"CustomOptimizer(opt::Function, name::String)\n\ncreates a custom optimizer with struct name name. For example, we can integrate Optim.jl with ADCME by  constructing a new optimizer\n\nCustomOptimizer(\"Con\") do f, df, c, dc, x0, nineq, neq, x_L, x_U\n    opt = Opt(:LD_MMA, length(x0))\n    bd = zeros(length(x0)); bd[end-1:end] = [-Inf, 0.0]\n    opt.lower_bounds = bd\n    opt.xtol_rel = 1e-4\n    opt.min_objective = (x,g)->(g[:]= df(x); return f(x)[1])\n    inequality_constraint!(opt, (x,g)->( g[:]= dc(x);c(x)[1]), 1e-8)\n    (minf,minx,ret) = NLopt.optimize(opt, x0)\n    minx\nend\n\nThen we can create an optimizer with \n\nopt = Con(loss, inequalities=[c1], equalities=[c2])\n\nTo trigger the optimization, use\n\nopt.minimize(sess)\n\nor \n\nminimize(opt, sess)\n\nNote thanks to the global variable scope of Julia, step_callback, optimizer_kwargs can actually  be passed from Julia environment directly.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyObject},Union{PyObject, Array{Float64,N} where N}}} where T<:Real","page":"API Reference","title":"ADCME.NonlinearConstrainedProblem","text":"NonlinearConstrainedProblem(f::Function, L::Function, θ::PyObject, u0::Union{PyObject, Array{Float64}}; options::Union{Dict{String, T}, Missing}=missing) where T<:Integer\n\nComputes the gradients fracpartial Lpartial theta\n\nbeginalign\nmin   L(u) \nmathrmst   F(theta u) = 0\nendalign\n\nu0 is the initial guess for the numerical solution u, see newton_raphson.\n\nCaveats: Assume r, A = f(θ, u) and θ are the unknown parameters, gradients(r, θ) must be defined (backprop works properly)\n\nReturns: It returns a tuple (L: loss, C: constraints, and Graidents)\n\nleft(L(u) u fracpartial Lpartial θright)\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerInterface-Tuple{Any}","page":"API Reference","title":"ADCME.ScipyOptimizerInterface","text":"ScipyOptimizerInterface(loss; method=\"L-BFGS-B\", options=Dict(\"maxiter\"=> 15000, \"ftol\"=>1e-12, \"gtol\"=>1e-12), kwargs...)\n\nA simple interface for Scipy Optimizer. See also ScipyOptimizerMinimize and BFGS!.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ScipyOptimizerMinimize-Tuple{PyObject,PyObject}","page":"API Reference","title":"ADCME.ScipyOptimizerMinimize","text":"ScipyOptimizerMinimize(sess::PyObject, opt::PyObject; kwargs...)\n\nMinimizes a scalar Tensor. Variables subject to optimization are updated in-place at the end of optimization.\n\nNote that this method does not just return a minimization Op, unlike minimize; instead it actually performs minimization by executing commands to control a Session https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface. See also ScipyOptimizerInterface and BFGS!.\n\nfeed_dict: A feed dict to be passed to calls to session.run.\nfetches: A list of Tensors to fetch and supply to loss_callback as positional arguments.\nstep_callback: A function to be called at each optimization step; arguments are the current values of all optimization variables flattened into a single vector.\nloss_callback: A function to be called every time the loss and gradients are computed, with evaluated fetches supplied as positional arguments.\nrun_kwargs: kwargs to pass to session.run.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.newton_raphson-Union{Tuple{T}, Tuple{Function,Union{PyObject, Array}}, Tuple{Function,Union{PyObject, Array},Union{Missing, PyObject, Array{#s110,N} where N where #s110<:Real}}} where T<:Real","page":"API Reference","title":"ADCME.newton_raphson","text":"newton_raphson(f::Function, u::Union{Array,PyObject}, θ::Union{Missing,PyObject}; options::Union{Dict{String, T}, Missing}=missing)\n\nNewton Raphson solver for solving a nonlinear equation.  f has the signature f(u::PyObject, θ::Union{Missing,PyObject})->(r::PyObject, A::Union{PyObject,SparseTensor}) where r is the residual and A is the Jacobian matrix.  θ are external parameters. u0 is the initial guess for u options:\n\n\"max_iter\": maximum number of iterations (default=100)\n\"verbose\": whether details are printed (default=false)\n\"rtol\": relative tolerance for termination (default=1e-12)\n\"tol\": absolute tolerance for termination (default=1e-12)\n\n\n\n\n\n","category":"method"},{"location":"api/#Neural-Networks-1","page":"API Reference","title":"Neural Networks","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"layers.jl\"]","category":"page"},{"location":"api/#ADCME.ae","page":"API Reference","title":"ADCME.ae","text":"ae(x::PyObject, output_dims::Array{Int64}, scope::String = \"default\")\n\nCreates a neural network with intermediate numbers of neurons output_dims.\n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.ae-Tuple{Union{PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{PyObject, Array{Float64,N} where N}}","page":"API Reference","title":"ADCME.ae","text":"ae(x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64}, θ::Union{Array{Float64}, PyObject})\n\nCreates a neural network with intermediate numbers of neurons output_dims. The weights are given by θ\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_init-Tuple{PyObject,Union{PyObject, Array{Float64,N} where N},Array{Int64,N} where N}","page":"API Reference","title":"ADCME.ae_init","text":"ae_init(sess::PyObject, x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64})\n\nReturn the initial weights and bias values by TensorFlow as a vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.ae_to_code-Tuple{String,String}","page":"API Reference","title":"ADCME.ae_to_code","text":"ae_to_code(file::String, scope::String)\n\nReturn the code string from the feed-forward neural network data in file.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.bn-Tuple","page":"API Reference","title":"ADCME.bn","text":"bn(args...;center = true, scale=true, kwargs...)\n\nexample\n\nbn(inputs, name=\"batch_norm\", is_training=true)\n\nnote: Note\nbn should be used with control_dependencyupdate_ops = get_collection(UPDATE_OPS)\ncontrol_dependencies(update_ops) do \n    global train_step = AdamOptimizer().minimize(loss)\nend \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.num_ae-Tuple{Array{Int64,N} where N}","page":"API Reference","title":"ADCME.num_ae","text":"num_ae(output_dims::Array{Int64})\n\nEstimates the number of weights for the neural network.\n\n\n\n\n\n","category":"method"},{"location":"api/#Generative-Neural-Nets-1","page":"API Reference","title":"Generative Neural Nets","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"gan.jl\"]","category":"page"},{"location":"api/#ADCME.GAN","page":"API Reference","title":"ADCME.GAN","text":"GAN(dat::PyObject, \n    generator::Function, \n    gan::GAN,\n    loss::Union{String, Function, Missing}=missing; \n    latent_dim::Union{Missing, Int64}=missing, \n    batch_size::Union{Missing, Int64}=missing)\n\nCreates a GAN instance. \n\ndat in mathbbR^ntimes d is the training data for the GAN, where n is the number of training data, and d is the dimension per training data.\ngeneratormathbbR^d rightarrow mathbbR^d is the generator function, d is the hidden dimension.\ndiscriminatormathbbR^d rightarrow mathbbR is the discriminator function. \nloss is the loss function. See klgan, rklgan, wgan, lsgan for examples.\nlatent_dim (default=d) is the latent dimension.\nbatch_size (default=32) is the batch size in training.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADCME.jsgan-Tuple{GAN}","page":"API Reference","title":"ADCME.jsgan","text":"jsgan(gan::GAN)\n\nComputes the vanilla GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.klgan-Tuple{GAN}","page":"API Reference","title":"ADCME.klgan","text":"klgan(gan::GAN)\n\nComputes the KL-divergence GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.lsgan-Tuple{GAN}","page":"API Reference","title":"ADCME.lsgan","text":"lsgan(gan::GAN)\n\nComputes the least square GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.predict-Tuple{GAN,Union{PyObject, Array}}","page":"API Reference","title":"ADCME.predict","text":"predict(gan::GAN, input::Union{PyObject, Array})\n\nPredicts the GAN gan output given input input. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.rklgan-Tuple{GAN}","page":"API Reference","title":"ADCME.rklgan","text":"rklgan(gan::GAN)\n\nComputes the reverse KL-divergence GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.sample-Tuple{GAN,Int64}","page":"API Reference","title":"ADCME.sample","text":"sample(gan::GAN, n::Int64)\n\nSamples n instances from gan.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.wgan-Tuple{GAN}","page":"API Reference","title":"ADCME.wgan","text":"wgan(gan::GAN)\n\nComputes the Wasserstein GAN loss function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.build!-Tuple{GAN}","page":"API Reference","title":"ADCME.build!","text":"build!(gan::GAN)\n\nBuilds the GAN instances. This function returns gan for convenience.\n\n\n\n\n\n","category":"method"},{"location":"api/#Tools-1","page":"API Reference","title":"Tools","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"extra.jl\"]","category":"page"},{"location":"api/#ADCME.compile_op-Tuple{String}","page":"API Reference","title":"ADCME.compile_op","text":"compile_op(oplibpath::String; check::Bool=false)\n\nCompile the library operator by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.customop-Tuple{}","page":"API Reference","title":"ADCME.customop","text":"customop()\n\nCreate a new custom operator.\n\nexample\n\njulia> customop() # create an editable `customop.txt` file\n[ Info: Edit custom_op.txt for custom operators\njulia> customop() # after editing `customop.txt`, call it again to generate interface files.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op-Tuple{String,String}","page":"API Reference","title":"ADCME.load_op","text":"load_op(oplibpath::String, opname::String)\n\nLoads the operator opname from library oplibpath.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_op_and_grad-Tuple{String,String}","page":"API Reference","title":"ADCME.load_op_and_grad","text":"load_op_and_grad(oplibpath::String, opname::String; multiple::Bool=false)\n\nLoads the operator opname from library oplibpath; gradients are also imported.  If multiple is true, the operator is assumed to have multiple outputs. \n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.xavier_init","page":"API Reference","title":"ADCME.xavier_init","text":"xavier_init(size, dtype=Float64)\n\nReturns a matrix of size size and its values are from Xavier initialization. \n\n\n\n\n\n","category":"function"},{"location":"api/#ADCME.compile-Tuple{String}","page":"API Reference","title":"ADCME.compile","text":"compile(s::String)\n\nCompiles the library s by force.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADCME.load_system_op","page":"API Reference","title":"ADCME.load_system_op","text":"load_system_op(s::String, oplib::String, grad::Bool=true)\n\nLoads custom operator from CustomOps directory (shipped with ADCME instead of TensorFlow) For example \n\ns = \"SparseOperator\"\noplib = \"libSO\"\ngrad = true\n\nthis will direct Julia to find library CustomOps/SparseOperator/libSO.dylib on MACOSX\n\n\n\n\n\n","category":"function"},{"location":"api/#Datasets-1","page":"API Reference","title":"Datasets","text":"","category":"section"},{"location":"api/#","page":"API Reference","title":"API Reference","text":"Modules = [ADCME]\nPages   = [\"dataset.jl\"]","category":"page"},{"location":"while_loop/#While-Loops-1","page":"While Loops","title":"While Loops","text":"","category":"section"},{"location":"while_loop/#Motivation-1","page":"While Loops","title":"Motivation","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"In engineering, we usually need to do for loops, e.g., time stepping, finite element matrix assembling, etc. In pseudocode, we have","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"for i = 1:1000000\n  global x\n\tx = do_some_simulation(x)\nend","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"To do automatic differentiation in ADCME, direct implemnetation in the above way incurs creation of 1000000 subgraphs, which requires large memories and long dependency parsing time. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"TensorFlow provides us a clever way to do loops, where only one graph is created for the whole loops. The basic idea is to create a while_loop graph based on five primitives, and the corresponding graph for backpropagation is constructed thereafter. ","category":"page"},{"location":"while_loop/#A-Simple-Example-1","page":"While Loops","title":"A Simple Example","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"As a simple example, we consider assemble the external load vector for linear finite elements in 1D. Assume that the load distribution is f(x)=1-x^2, xin01. The goal is to compute a vector mathbfv with v_i=int_0^1 f(x)phi_i(x)dx, where phi_i(x) is the i-th linear element. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The pseudocode for this problem is shown in the following","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"F = zeros(ne+1) // ne is the total number of elements\nfor e = 1:ne\n\tadd load contribution to F[e] and F[e+1]\nend","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"(Image: )","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"However, if ne is very large, writing explicit loops is unwise since it will create ne subgraphs. while_loop can be very helpful in this case (the script can also be found in https://github.com/kailaix/ADCME.jl/tree/master/examples/whileloop/whileloop_simple.jl)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"using ADCME\n\nne = 100\nh = 1/ne\nf = x->1-x^2\nfunction cond0(i, F_arr)\n    i<=ne+1\nend\nfunction body(i, F_arr)\n    fmid = f(cast(i-2, Float64)*h+h/2)\n    F = vector([i-1;i], [fmid*h/2;fmid*h/2], ne+1)\n    F_arr = write(F_arr, i, F)\n    i+1, F_arr\nend\n\nF_arr = TensorArray(ne+1)\nF_arr = write(F_arr, 1, constant(zeros(ne+1))) # inform `F_arr` of the data type by writing at index 1\ni = constant(2, dtype=Int32)\n_, out = while_loop(cond0, body, [i,F_arr]; parallel_iterations=10)\nF = sum(stack(out), dims=1)\nsess = Session(); init(sess)\nF0 = run(sess, F)","category":"page"},{"location":"while_loop/#A-practical-application-1","page":"While Loops","title":"A practical application","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"In this section, we demonstrate how to assemble a finite element matrix based on while_loop for a 2D Poisson problem. We consider the following problem beginaligned nabla cdot ( Dnabla u(mathbfx) ) = f(mathbfx) mathbfxin Omega\nu(mathbfx) = 0  mathbfxin partial Omega endaligned Here Omega is the unit disk. We consider a simple case, where beginaligned D=mathbfI\nf(mathbfx)=-4 endaligned Then the exact solution will be  u(mathbfx) = 1-x^2-y^2 The weak formulation is langle nabla v(mathbfx) Dnabla u(mathbfx) rangle = langle f(mathbfx)v(mathbfx) rangle We  split Omega into triangles mathcalT and use piecewise linear basis functions. Typically, we would iterate over all elements and compute the local stiffness matrix for each element. However, this could result in a large loop if we use a fine mesh. Instead, we can use while_loop to complete the task. In ADCME, the syntax for while_loop is ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"while_loop(condition, body, loop_vars)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"here condition and body take loop_vars as inputs. The former outputs a bool tensor indicating whether to terminate the loop while the latter outputs the updated loop_vars. TensorArry is used to store variables that change during the loops. The codes for assembling FEM is","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"function assemble_FEM(Ds, Fs, nodes, elem)\n    NT = size(elem,1)\n    cond0 = (i,tai,taj,tav, tak, taf) -> i<=NT\n    elem = constant(elem)\n    nodes = constant(nodes)\n    function body(i, tai, taj, tav, tak, taf)\n        el = elem[i]\n        x1, y1 = nodes[el[1]][1], nodes[el[1]][2]\n        x2, y2 = nodes[el[2]][1], nodes[el[2]][2]\n        x3, y3 = nodes[el[3]][1], nodes[el[3]][2]\n        T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)\n        D = Ds[i]; F = Fs[i]*T/3\n        v = T*stack([D*((-x2 + x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y2 - y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2)])\n        tav = write(tav, i, v)\n        ii = vec([elem[i] elem[i] elem[i]]')\n        jj = [elem[i]; elem[i]; elem[i]]\n        tai = write(tai, i, ii)\n        taj = write(taj, i, jj)\n        tak = write(tak, i, elem[i])\n        taf = write(taf, i, stack([F,F,F]))\n        return i+1, tai, taj, tav, tak, taf\n    end\n    tai = TensorArray(NT, dtype=Int32)\n    taj = TensorArray(NT, dtype=Int32)\n    tak = TensorArray(NT, dtype=Int32)\n    tav = TensorArray(NT)\n    taf = TensorArray(NT)\n    i = constant(1, dtype=Int32)\n    i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)\n    _, tai, taj, tav, tak, taf = while_loop(cond0, body, [i, tai, taj, tav, tak, taf]; parallel_iterations=10)\n    vec(stack(tai)[1:NT]'), vec(stack(taj)[1:NT]'), vec(stack(tav)[1:NT]'),\n                        vec(stack(tak)[1:NT]'), vec(stack(taf)[1:NT]')\nend","category":"page"},{"location":"while_loop/#Code-detail-explained-1","page":"While Loops","title":"Code detail explained","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We now explain the codes. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"We assume that nodes is a n_vtimes 2 tensor holding all n_v coordinates of the nodes, elem is a n_etimes 3  tensor holding all n_e triangle vertex index triples. We create five TensorArray to hold the row indices, column indices and values for the stiffness matrix, and row indices and values for the right hand side (Here NT denotes n_e):","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"tai = TensorArray(NT, dtype=Int32)\ntaj = TensorArray(NT, dtype=Int32)\ntak = TensorArray(NT, dtype=Int32)\ntav = TensorArray(NT)\ntaf = TensorArray(NT)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Within each loop (body), we extract the coordinates of each vertex coordinate","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"el = elem[i]\nx1, y1 = nodes[el[1]][1], nodes[el[1]][2]\nx2, y2 = nodes[el[2]][1], nodes[el[2]][2]\nx3, y3 = nodes[el[3]][1], nodes[el[3]][2]","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"and compute the area of ith triangle","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"The local stiffness matrix is computed and vectorized (v). It is computed symbolically.  To store the computed value into TensorArray, we call the write API (there is also read API, which reads a value from TensorArray)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"tav = write(tav, i, v)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Note we have called ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"before we call while_loop. This is because we need to initialize the TensorArrays (i.e., telling them the size and type of elements in the arrays). We must guarantee that the sizes and types of the elements in the arrays are consistent in while_loop. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"Finally, we stack the TensorArray into a tensor and vectorized it according to the row major. This serves as the output of assemble_FEM. The complete script for solving this problem is here and the following plot shows the numerical result and corresponding reference solution. ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"(Image: Result for the Poisson Problem)","category":"page"},{"location":"while_loop/#Gradients-that-backpropagate-through-loops-1","page":"While Loops","title":"Gradients that backpropagate through loops","text":"","category":"section"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"To inspect the gradients through the loops, we can run ","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"println(run(sess, gradients(sum(u), Ds))) # a sparse tensor","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"This outputs a sparse tensor instead of a full tensor. To obtain the full tensor, we could call tf.convert_to_tensor","category":"page"},{"location":"while_loop/#","page":"While Loops","title":"While Loops","text":"println(run(sess, tf.convert_to_tensor(gradients(sum(u), Ds)))) # full tensor","category":"page"},{"location":"customop/#Custom-Operators-1","page":"Custom Operators","title":"Custom Operators","text":"","category":"section"},{"location":"customop/#Basic-Usage-1","page":"Custom Operators","title":"Basic Usage","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Custom operators are ways to add missing features in ADCME. Typically users do not have to worry about custom operators. However, in the following situation custom opreators might be very useful","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Direct implementation in ADCME is inefficient (bottleneck). \nThere are legacy codes users want to reuse, such as GPU-accelerated codes. \nSpecial acceleration techniques such as checkpointing scheme. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In the following, we present an example of implementing the sparse solver custom operator for Ax=b.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Input: row vector ii, column vectorjj and value vector vv for the sparse coefficient matrix; row vector kk and value vector ff, matrix dimension d","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Output: solution vector u","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 1: Create and modify the template file","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The following command helps create the wrapper","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"customop()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There will be a custom_op.txt in the current directory. Modify the template file ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"MySparseSolver\nint32 ii(?)\nint32 jj(?)\ndouble vv(?)\nint32 kk(?)\ndouble ff(?)\nint32 d()\ndouble u(?) -> output","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The first line is the name of the operator. It should always be in the camel case. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The 2nd to the 7th lines specify the input arguments, the signature is type+variable name+shape. For the shape, () corresponds to a scalar, (?) to a vector and (?,?) to a matrix. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The last line is the output, denoted by -> output. Note there must be a space before and after ->. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The following types are accepted: int32, int64, double, float, string, bool. The name of the arguments must all be in lower cases. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 2: Implement core codes","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Run customop() again and there will be CMakeLists.txt, gradtest.jl, MySparseSolver.cpp appearing in the current directory. MySparseSolver.cpp is the main wrapper for the codes and gradtest.jl is used for testing the operator and its gradients. CMakeLists.txt is the file for compilation. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Create a new file MySparseSolver.h and implement both the forward simulation and backward simulation (gradients)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"#include <eigen3/Eigen/Sparse>\n#include <eigen3/Eigen/SparseLU>\n#include <vector>\n#include <iostream>\nusing namespace std;\ntypedef Eigen::SparseMatrix<double> SpMat; // declares a column-major sparse matrix type of double\ntypedef Eigen::Triplet<double> T;\n\nSpMat A;\n\nvoid forward(double *u, const int *ii, const int *jj, const double *vv, int nv, const int *kk, const double *ff,int nf,  int d){\n    vector<T> triplets;\n    Eigen::VectorXd rhs(d); rhs.setZero();\n    for(int i=0;i<nv;i++){\n      triplets.push_back(T(ii[i]-1,jj[i]-1,vv[i]));\n    }\n    for(int i=0;i<nf;i++){\n      rhs[kk[i]-1] += ff[i];\n    }\n    A.resize(d, d);\n    A.setFromTriplets(triplets.begin(), triplets.end());\n    auto C = Eigen::MatrixXd(A);\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(A);\n    solver.factorize(A);\n    auto x = solver.solve(rhs);\n    for(int i=0;i<d;i++) u[i] = x[i];\n}\n\nvoid backward(double *grad_vv, const double *grad_u, const int *ii, const int *jj, const double *u, int nv, int d){\n    Eigen::VectorXd g(d);\n    for(int i=0;i<d;i++) g[i] = grad_u[i];\n    auto B = A.transpose();\n    Eigen::SparseLU<SpMat> solver;\n    solver.analyzePattern(B);\n    solver.factorize(B);\n    auto x = solver.solve(g);\n    // cout << x << endl;\n    for(int i=0;i<nv;i++) grad_vv[i] = 0.0;\n    for(int i=0;i<nv;i++){\n      grad_vv[i] -= x[ii[i]-1]*u[jj[i]-1];\n    }\n}","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nIn this implementation we have used Eigen library for solving sparse matrix. Other choices are also possible, such as algebraic multigrid methods. Note here for convenience we have created a global variable SpMat A;. This is not recommend if you want to run the code concurrently. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 3: Compile","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"It is recommended that you use the cmake, make and gcc provided by ADCME.  | Variable      | Description                           | | ––––––- | ––––––––––––––––––- | | ADCME.CXX   | C++ Compiler                          | | ADCME.CC    | C Compiler                            | | ADCME.TFLIB | libtensorflow_framework.so location | | ADCME.CMAKE | Cmake binary location                 | | ADCME.MAKE  | Make binary location                  |","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Make a build directory in bash.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"mkdir build\ncd build","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Configure CMake files.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"julia> using ADCME\njulia> ADCME.cmake()","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Build. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"make -j","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"note: Note\nIf the system make command is not compatible, try the pre-installed ADCME make located at ADCME.MAKE. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Based on your operation system, you will create libMySparseSolver.{so,dylib,dll}. This will be the dynamic library to link in TensorFlow. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Step 4: Test","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Finally, you could use gradtest.jl to test the operator and its gradients (specify appropriate data in gradtest.jl first). If you implement the gradients correctly, you will be able to obtain first order convergence for finite difference and second order convergence for automatic differentiation. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"(Image: custom_op)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"If the process fails, it is most probability the GCC compiler is not compatible with which was used to compile libtensorflow_framework.{so,dylib}. In the Linux system, you can check the compiler using ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"readelf -p .comment libtensorflow_framework.so","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Compatibility issues are frustrating. We hope you can submit an issue to ADCME developers; we are happy to resolve the compatibility issue and improve the robustness of ADCME.","category":"page"},{"location":"customop/#GPU-Operators-1","page":"Custom Operators","title":"GPU Operators","text":"","category":"section"},{"location":"customop/#Dependencies-1","page":"Custom Operators","title":"Dependencies","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To create a GPU custom operator, you must have NVCC compiler and CUDA toolkit installed on your system. To install NVCC, see the installation guide. To check you have successfully installed NVCC, type","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"which nvcc","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"It should gives you the location of nvcc compiler.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"For quick installation, you can try","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using ADCME\nenable_gpu()","category":"page"},{"location":"customop/#Manual-Installation-1","page":"Custom Operators","title":"Manual Installation","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In case ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"To install CUDA toolkit (if you do not have one), you can install via conda","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using Conda\nConda.add(\"cudatoolkit\", channel=\"anaconda\")","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The next step is to cp the CUDA include file to tensorflow include directory. This could be done with ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"using ADCME\ngpus = joinpath(splitdir(tf.__file__)[1], \"include/third_party/gpus\")\nif !isdir(gpus)\n  mkdir(gpus)\nend\ngpus = joinpath(gpus, \"cuda\")\nif !isdir(gpus)\n  mkdir(gpus)\nend\nincpath = joinpath(splitdir(strip(read(`which nvcc`, String)))[1], \"../include/\")\nif !isdir(joinpath(gpus, \"include\"))\n    mv(incpath, gpus)\nend","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Finally, add the CUDA library path to LD_LIBRARY_PATH. This can be done by adding the following line to .bashrc","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"export LD_LIBRARY_PATH=<path>:$LD_LIBRARY_PATH","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"where <path> is ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"joinpath(Conda.ROOTENV, \"pkgs/cudatoolkit-10.1.168-0/lib/\")","category":"page"},{"location":"customop/#File-Organization-1","page":"Custom Operators","title":"File Organization","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There should be three files in your source directories","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"MyOp.cpp: driver file\nMyOp.cu: GPU implementation\nMyOp.h: CPU implementation","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"The first two files have been generated for you by customop(). The following are two important notes on the implementation.","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In MyOp.cu, the implementation usually has the structure","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"namespace tensorflow{\n  typedef Eigen::GpuDevice GPUDevice;\n\n    __global__ void forward_(const int nthreads, double *out, const double *y, const double *H0, int n){\n      for(int i : CudaGridRangeX(nthreads)) {\n          // do something here\n      }\n    }\n\n    void forwardGPU(double *out, const double *y, const double *H0, int n, const GPUDevice& d){\n      // forward_<<<(n+255)/256, 256>>>(out, y, H0, n);\n      GpuLaunchConfig config = GetGpuLaunchConfig(n, d);\n      TF_CHECK_OK(GpuLaunchKernel(\n          forward_, config.block_count, config.thread_per_block, 0,\n          d.stream(), config.virtual_thread_count, out, y, H0, n));\n      }\n}","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"In MyOp.cpp, the device information (const GPUDevice& d above) is obtained with ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"context->eigen_device<GPUDevice>()","category":"page"},{"location":"customop/#Best-Practice-and-Caveats-1","page":"Custom Operators","title":"Best Practice and Caveats","text":"","category":"section"},{"location":"customop/#Allocating-Memories-1","page":"Custom Operators","title":"Allocating Memories","text":"","category":"section"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Whenever memory is needed, one should allocate memory by TensorFlow context. ","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"Tensor* tmp_var = nullptr;\nTensorShae tmp_shape({10,10});\nOP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, tmp_shape, &tmp_var));","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"There are three methods to allocate Tensors when an Op kernel executes (details)","category":"page"},{"location":"customop/#","page":"Custom Operators","title":"Custom Operators","text":"allocate_persistent: if the memory is used between Op invocations.\nallocate_temp: if the memory is used only within Compute.\nallocate_output: if the memory will be used as output","category":"page"},{"location":"#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"ADCME is suitable for conducting inverse modeling in scientific computing. The purpose of the package is to: (1) provide differentiable programming framework for scientific computing based on TensorFlow automatic differentiation (AD) backend; (2) adapt syntax to facilitate implementing scientific computing, particularly for numerical PDE discretization schemes; (3) supply missing functionalities in the backend (TensorFlow) that are important for engineering, such as sparse linear algebra, constrained optimization, etc. Applications include","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"full wavelength inversion\nreduced order modeling in solid mechanics\nlearning hidden geophysical dynamics\nphysics based machine learning\nparameter estimation in stochastic processes","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The package inherents the scalability and efficiency from the well-optimized backend TensorFlow. Meanwhile, it provides access to incooperate existing C/C++ codes via the custom operators. For example, some functionalities for sparse matrices are implemented in this way and serve as extendable \"plugins\" for ADCME. ","category":"page"},{"location":"#Getting-Started-1","page":"Overview","title":"Getting Started","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"To install ADCME, use the following command:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using Pkg\nPkg.add(\"ADCME\")","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"to load the package, use","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using ADCME","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"We consider a simple inverse modeling problem: consider the following partial differential equation -bu(x)+u(x)=f(x)quad xin01 u(0)=u(1)=0 where  f(x) = 8 + 4x - 4x^2 Assume that we have observed u(05)=1, we want to estimate b. The true value in this case should be b=1. We can discretize the system using finite difference method, and the resultant linear system will be (bA+I)mathbfu = mathbff where A = beginbmatrix         frac2h^2  -frac1h^2  dots  0\n         -frac1h^2  frac2h^2  dots  0\n         dots \n         0  0  dots  frac2h^2     endbmatrix quad mathbfu = beginbmatrix         u_2\n        u_3\n        vdots\n        u_n     endbmatrix quad mathbff = beginbmatrix         f(x_2)\n        f(x_3)\n        vdots\n        f(x_n)     endbmatrix","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The idea for implementing the inverse modeling method in ADCME is that we make the unknown b a Variable and then solve the forward problem pretending b is known. The following code snippet shows the implementation","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using LinearAlgebra\nusing ADCME\n\nn = 101 # number of grid nodes in [0,1]\nh = 1/(n-1)\nx = LinRange(0,1,n)[2:end-1]\n\nb = Variable(10.0) # we use Variable keyword to mark the unknowns\nA = diagm(0=>2/h^2*ones(n-2), -1=>-1/h^2*ones(n-3), 1=>-1/h^2*ones(n-3)) \nB = b*A + I  # I stands for the identity matrix\nf = @. 4*(2 + x - x^2) \nu = B\\f # solve the equation using built-in linear solver\nue = u[div(n+1,2)] # extract values at x=0.5\n\nloss = (ue-1.0)^2 \n\n# Optimization\nsess = Session(); init(sess) \nBFGS!(sess, loss)\n\nprintln(\"Estimated b = \", run(sess, b))","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The expected output is","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Estimated b = 0.9995582304494237","category":"page"},{"location":"julia_customop/#Julia-Custom-Operators-1","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"In scientific and engineering applications, the operators provided by TensorFlow are not sufficient for high performance computing. In addition, constraining oneself to TensorFlow environment sacrifices the powerful scientific computing ecosystems provided by other languages such as Julia and Python. For example, one might want to code a finite volume method for a sophisticated fluid dynamics problem; it is hard to have the flexible syntax to achieve this goal, obtain performance boost from existing fast solvers such as AMG, and benefit from many other third-party packages within TensorFlow. This motivates us to find a way to \"plugin\" custom operators to TensorFlow.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We have already introduced how to incooperate C++ custom operators.  For many researchers, they usually prototype the solvers in a high level language such as MATLAB, Julia or Python. To enjoy the parallelism and automatic differentiation feature of TensorFlow, they need to port them into C/C++. However, this is also cumbersome sometimes, espeically the original solvers depend on many packages in the high-level language. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We solve this problem by incorporating Julia functions directly into TensorFlow. That is, for any Julia functions, we can immediately convert it to a TensorFlow operator. At runtime, when this operator is executed, the corresponding Julia function is executed. That implies we have the Julia speed. Most importantly, the function is perfectly compitable with the native Julia environment; third-party packages, global variables, nested functions, etc. all work smoothly. Since Julia has the ability to call other languages in a quite elegant and simple manner, such as C/C++, Python, R, Java, this means it is possible to incoporate packages/codes from any supported languages into TensorFlow ecosystem. We need to point out that in TensorFlow, tf.numpy_function can be used to convert a Python function to a TensorFlow operator. However, in the runtime, the speed for this operator falls back to Python (or numpy operation for related parts). This is a drawback. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The key for implementing the mechanism is embedding Julia in C++. Still we need to create a C++ dynamic library for TensorFlow. However, the library is only an interface for invoking Julia code. At runtime, jl_get_function is called to search for the related function in the main module. C++ arrays, which include all the relavant data, are passed to this function through jl_call. It requires routine convertion from C++ arrays to Julia array interfaces jl_array_t*. However, those bookkeeping tasks are programatic and possibly will be automated in the future. Afterwards,Julia returns the result to C++ and thereafter the data are passed to the next operator. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"There are two caveats in the implementation. The first is that due to GIL of Python, we must take care of the thread lock while interfacing with Julia. This was done by putting a guard around th eJulia interface","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"PyGILState_STATE py_threadstate;\npy_threadstate = PyGILState_Ensure();\n// code here \nPyGILState_Release(py_threadstate);","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The second is the memory mangement of Julia arrays. This was done by defining gabage collection markers explicitly","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"jl_value_t **args;\nJL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\nargs[0] = ...\nargs[1] = ...\n# do something\nJL_GC_POP();","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"This technique is remarkable and puts together one of the best langages in scientific computing and that in machine learning. The work that can be built on ADCME is enormous and significantly reduce the development time. ","category":"page"},{"location":"julia_customop/#Example-1","page":"Julia Custom Operators","title":"Example","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Here we present a simple example. Suppose we want to compute the Jacobian of a two layer neural network fracpartial ypartial x","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"y = W_2tanh(W_1x+b_1)+b_2","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"where x b_1 b_2 yin mathbbR^10, W_1 W_2in mathbbR^100. In TensorFlow, this can be done by computing the gradients fracpartial y_ipartial x for each i. In Julia, we can use ForwardDiff to do it automatically. ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"function twolayer(J, x, w1, w2, b1, b2)\n    f = x -> begin\n        w1 = reshape(w1, 10, 10)\n        w2 = reshape(w2, 10, 10)\n        z = w2*tanh.(w1*x+b1)+b2\n    end\n    J[:] = ForwardDiff.jacobian(f, x)[:]\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"To make a custom operator, we first generate a wrapper","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"using ADCME\nmkdir(\"TwoLayer\")\ncd(\"TwoLayer\")\ncustomop()","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"We modify custom_op.txt","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"TwoLayer\ndouble x(?)\ndouble w1(?)\ndouble b1(?)\ndouble w2(?)\ndouble b2(?)\ndouble y(?) -> output","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"and run ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"customop(;julia=true)","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Three files are generatedCMakeLists.txt, TwoLayer.cpp and gradtest.jl. Now create a new file TwoLayer.h","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"#include \"julia.h\"\n#include \"Python.h\"\n\nvoid forward(double *y, const double *x, const double *w1, const double *w2, const double *b1, const double *b2, int n){\n    PyGILState_STATE py_threadstate;\n    py_threadstate = PyGILState_Ensure();\n    jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\n    jl_value_t **args;\n    JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\n    args[0] = (jl_value_t*)jl_ptr_to_array_1d(array_type, y, n*n, 0);\n    args[1] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(x), n, 0);\n    args[2] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w1), n*n, 0);\n    args[3] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w2), n*n, 0);\n    args[4] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b1), n, 0);\n    args[5] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b2), n, 0);\n    auto fun = jl_get_function(jl_main_module, \"twolayer\");\n  \tif (fun==NULL) jl_errorf(\"Function not found in Main module.\");\n    else jl_call(fun, args, 6);\n    JL_GC_POP();\n    if (jl_exception_occurred())\n        printf(\"%s \\n\", jl_typeof_str(jl_exception_occurred()));\n    PyGILState_Release(py_threadstate);\n}","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Most of the codes have been explanined except jl_ptr_to_array_1d. This function generates a Julia array wrapper from C++ arrays. The last argument 0 indicates that Julia is not responsible for gabage collection. TwoLayer.cpp should also be modified according to https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp.","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"Finally, we can test in gradtest.jl ","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"two_layer = load_op(\"build/libTwoLayer\", \"two_layer\")\n\n\nw1 = rand(100)\nw2 = rand(100)\nb1 = rand(10)\nb2 = rand(10)\nx = rand(10)\nJ = rand(100)\ntwolayer(J, x, w1, w2, b1, b2)\n\ny = two_layer(constant(x), constant(w1), constant(b1), constant(w2), constant(b2))\nsess = Session(); init(sess)\nJ0 = run(sess, y)\n@show norm(J-J0)","category":"page"},{"location":"julia_customop/#Usage-in-a-Module-1","page":"Julia Custom Operators","title":"Usage in a Module","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"If the custom operator is intended to be used in a precompiled module, we can load the dynamic library at initialization","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"global my_op \nfunction __init__()\n\tglobal my_op = load_op(\"$(@__DIR__)/path/to/libMyOp\", \"my_op\")\nend","category":"page"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"The corresponding Julia function called by my_op must be exported in the module (such that it is in the Main module when invoked). One such example is given in MyModule","category":"page"},{"location":"julia_customop/#Reference-Sheet-1","page":"Julia Custom Operators","title":"Reference Sheet","text":"","category":"section"},{"location":"julia_customop/#","page":"Julia Custom Operators","title":"Julia Custom Operators","text":"For implementation reference, see Reference Sheet","category":"page"},{"location":"four_types/#Forward-Operator-Types-1","page":"Forward Operator Types","title":"Forward Operator Types","text":"","category":"section"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"(Image: Operators)","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called input, x, and turns out a parameter called ouput, y=f(x). For reverse mode automatic differentiation, besides evaluating f(x), we need also to compute fracpartial Jpartial x given fracpartial Jpartial y where J is a functional of y. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"Note  the operator y=f(x) may be implicit in the sense that f is not given directly. In general, we can write the relationship between x and y as F(xy)=0. The operator is well-defined if for given x, there exists one and only one y such that F(xy)=0. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"For automatic differentiation, besides the well-definedness of F, we also require that we can compute fracpartial Jpartial x given fracpartial Jpartial y. It is easy to see that fracpartial Jpartial x = -fracpartial Jpartial yF_y^-1F_x Therefore, we call an operator F is well-posed if F_y^-1 exists. ","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"All operators can be classified into four types based on the linearity and explicitness.","category":"page"},{"location":"four_types/#","page":"Forward Operator Types","title":"Forward Operator Types","text":"Linear and explicit This type of operators has the form  y = Ax where A is a matrix. In this case,  F(xy) = Ax-y and therefore  fracpartial Jpartial x = fracpartial Jpartial yA\nIn Tensorflow, such an operator can be implemented as (assuming A is )\nimport tensorflow as tf\n@tf.custom_gradient\ndef F(x):\n    u = tf.linalg.matvec(A, x)\n    def grad(dy):\n        return tf.linalg.matvec(tf.transpose(A), dy)\n    return u, grad\nNonlinear and explicit In this case, we have  y = F(x) where F is explicitly given. We have F(xy) = F(x)-yRightarrow fracpartial Jpartial x = fracpartial Jpartial y F_x(x)\nOne challenge here is we need to implement the matrix vector production fracpartial Jpartial y F_x(x) for grad. \nLinear and implicit In this case  Ay = x We have F(xy) = x-Ay and  fracpartial Jpartial x = fracpartial Jpartial yA^-1\nNonlinear and implicit In this case F(xy)=0 and the corresponding gradient is  fracpartial Jpartial x = -fracpartial Jpartial yF_y^-1F_x\nThis case is the most challenging of the four but widely seen in scientific computing code. In many numerical simulation code, F_y is usually sparse and therefore it is rewarding to exploit the sparse structure for computation acceleration in practice.","category":"page"}]
}
