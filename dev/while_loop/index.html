<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>While Loops · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../inverse_modeling/">Inverse Modeling</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../inverse_impl/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../array/">Tensor Operations</a></li><li><a class="tocitem" href="../sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../customop/">Custom Operators</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li class="is-active"><a class="tocitem" href>While Loops</a><ul class="internal"><li><a class="tocitem" href="#Motivation-1"><span>Motivation</span></a></li><li><a class="tocitem" href="#A-Basic-Example-1"><span>A Basic Example</span></a></li><li><a class="tocitem" href="#Finite-Element-Analysis-1"><span>Finite Element Analysis</span></a></li><li><a class="tocitem" href="#Explanation-1"><span>Explanation</span></a></li><li><a class="tocitem" href="#Gradients-through-while_loop-1"><span>Gradients through <code>while_loop</code></span></a></li></ul></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../pytorchnn/">Neural Network in C++</a></li><li><a class="tocitem" href="../extra/">Miscellaneous Tools</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>While Loops</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>While Loops</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/while_loop.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="While-Loops-1"><a class="docs-heading-anchor" href="#While-Loops-1">While Loops</a><a class="docs-heading-anchor-permalink" href="#While-Loops-1" title="Permalink"></a></h1><h2 id="Motivation-1"><a class="docs-heading-anchor" href="#Motivation-1">Motivation</a><a class="docs-heading-anchor-permalink" href="#Motivation-1" title="Permalink"></a></h2><p>In engineering, we usually need to do for loops, e.g., time stepping, finite element matrix assembling, etc. In pseudocode, we have</p><pre><code class="language-julia">for i = 1:1000000
  global x
	x = do_some_simulation(x)
end</code></pre><p>To do automatic differentiation in ADCME, direct implemnetation in the above way incurs creation of 1000000 subgraphs, which requires large memories and long dependency parsing time. </p><p><code>TensorFlow</code> provides us a clever way to do loops, where only one graph is created for the whole loops. The basic idea is to create a <code>while_loop</code> graph based on five primitives, and the corresponding graph for backpropagation is constructed thereafter. </p><h2 id="A-Basic-Example-1"><a class="docs-heading-anchor" href="#A-Basic-Example-1">A Basic Example</a><a class="docs-heading-anchor-permalink" href="#A-Basic-Example-1" title="Permalink"></a></h2><p>As a simple example, we consider assemble the external load vector for linear finite elements in 1D. Assume that the load distribution is <span>$f(x)=1-x^2$</span>, <span>$x\in[0,1]$</span>. The goal is to compute a vector <span>$\mathbf{v}$</span> with <span>$v_i=\int_{0}^1 f(x)\phi_i(x)dx$</span>, where <span>$\phi_i(x)$</span> is the <span>$i$</span>-th linear element. </p><p>The pseudocode for this problem is shown in the following</p><pre><code class="language-pseudocode">F = zeros(ne+1) // ne is the total number of elements
for e = 1:ne
	add load contribution to F[e] and F[e+1]
end</code></pre><p><img src="../assets/externalforce.png" alt/></p><p>However, if <code>ne</code> is very large, writing explicit loops is unwise since it will create <code>ne</code> subgraphs. <code>while_loop</code> can be very helpful in this case (the script can also be found in https://github.com/kailaix/ADCME.jl/tree/master/examples/while<em>loop/while</em>loop_simple.jl)</p><pre><code class="language-julia">using ADCME

ne = 100
h = 1/ne
f = x-&gt;1-x^2
function cond0(i, F_arr)
    i&lt;=ne+1
end
function body(i, F_arr)
    fmid = f(cast(i-2, Float64)*h+h/2)
    F = vector([i-1;i], [fmid*h/2;fmid*h/2], ne+1)
    F_arr = write(F_arr, i, F)
    i+1, F_arr
end

F_arr = TensorArray(ne+1)
F_arr = write(F_arr, 1, constant(zeros(ne+1))) # inform `F_arr` of the data type by writing at index 1
i = constant(2, dtype=Int32)
_, out = while_loop(cond0, body, [i,F_arr]; parallel_iterations=10)
F = sum(stack(out), dims=1)
sess = Session(); init(sess)
F0 = run(sess, F)</code></pre><h2 id="Finite-Element-Analysis-1"><a class="docs-heading-anchor" href="#Finite-Element-Analysis-1">Finite Element Analysis</a><a class="docs-heading-anchor-permalink" href="#Finite-Element-Analysis-1" title="Permalink"></a></h2><p>In this section, we demonstrate how to assemble a finite element matrix based on <code>while_loop</code> for a 2D Poisson problem. We consider the following problem</p><div>\[\begin{aligned}
\nabla \cdot ( D\nabla u(\mathbf{x}) ) &amp;= f(\mathbf{x})&amp; \mathbf{x}\in \Omega\\
u(\mathbf{x}) &amp;= 0 &amp; \mathbf{x}\in \partial \Omega
\end{aligned}\]</div><p>Here <span>$\Omega$</span> is the unit disk. We consider a simple case, where</p><div>\[\begin{aligned}
D&amp;=\mathbf{I}\\
f(\mathbf{x})&amp;=-4
\end{aligned}\]</div><p>Then the exact solution will be </p><div>\[u(\mathbf{x}) = 1-x^2-y^2\]</div><p>The weak formulation is</p><div>\[\langle \nabla v(\mathbf{x}), D\nabla u(\mathbf{x}) \rangle = \langle f(\mathbf{x}),v(\mathbf{x}) \rangle\]</div><p>We  split <span>$\Omega$</span> into triangles <span>$\mathcal{T}$</span> and use piecewise linear basis functions. Typically, we would iterate over all elements and compute the local stiffness matrix for each element. However, this could result in a large loop if we use a fine mesh. Instead, we can use <code>while_loop</code> to complete the task. In <code>ADCME</code>, the syntax for <code>while_loop</code> is </p><pre><code class="language-julia">while_loop(condition, body, loop_vars)</code></pre><p>here <code>condition</code> and <code>body</code> take <code>loop_vars</code> as inputs. The former outputs a bool tensor indicating whether to terminate the loop while the latter outputs the updated <code>loop_vars</code>. <code>TensorArry</code> is used to store variables that change during the loops. The codes for assembling FEM is</p><pre><code class="language-julia">function assemble_FEM(Ds, Fs, nodes, elem)
    NT = size(elem,1)
    cond0 = (i,tai,taj,tav, tak, taf) -&gt; i&lt;=NT
    elem = constant(elem)
    nodes = constant(nodes)
    function body(i, tai, taj, tav, tak, taf)
        el = elem[i]
        x1, y1 = nodes[el[1]][1], nodes[el[1]][2]
        x2, y2 = nodes[el[2]][1], nodes[el[2]][2]
        x3, y3 = nodes[el[3]][1], nodes[el[3]][2]
        T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)
        D = Ds[i]; F = Fs[i]*T/3
        v = T*stack([D*((-x2 + x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y2 - y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((x1 - x3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(-x2 + x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)*(y2 - y3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)*(x1 - x3)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (-y1 + y3)*(y1 - y2)/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2),D*((-x1 + x2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2 + (y1 - y2)^2/(x1*y2 - x1*y3 - x2*y1 + x2*y3 + x3*y1 - x3*y2)^2)])
        tav = write(tav, i, v)
        ii = vec([elem[i] elem[i] elem[i]]&#39;)
        jj = [elem[i]; elem[i]; elem[i]]
        tai = write(tai, i, ii)
        taj = write(taj, i, jj)
        tak = write(tak, i, elem[i])
        taf = write(taf, i, stack([F,F,F]))
        return i+1, tai, taj, tav, tak, taf
    end
    tai = TensorArray(NT, dtype=Int32)
    taj = TensorArray(NT, dtype=Int32)
    tak = TensorArray(NT, dtype=Int32)
    tav = TensorArray(NT)
    taf = TensorArray(NT)
    i = constant(1, dtype=Int32)
    i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)
    _, tai, taj, tav, tak, taf = while_loop(cond0, body, [i, tai, taj, tav, tak, taf]; parallel_iterations=10)
    vec(stack(tai)[1:NT]&#39;), vec(stack(taj)[1:NT]&#39;), vec(stack(tav)[1:NT]&#39;),
                        vec(stack(tak)[1:NT]&#39;), vec(stack(taf)[1:NT]&#39;)
end</code></pre><h2 id="Explanation-1"><a class="docs-heading-anchor" href="#Explanation-1">Explanation</a><a class="docs-heading-anchor-permalink" href="#Explanation-1" title="Permalink"></a></h2><p>We now explain the codes. </p><p>We assume that <code>nodes</code> is a <span>$n_v\times 2$</span> tensor holding all <span>$n_v$</span> coordinates of the nodes, <code>elem</code> is a <span>$n_e\times 3$</span>  tensor holding all <span>$n_e$</span> triangle vertex index triples. We create five <code>TensorArray</code> to hold the row indices, column indices and values for the stiffness matrix, and row indices and values for the right hand side (Here <code>NT</code> denotes <span>$n_e$</span>):</p><pre><code class="language-none">tai = TensorArray(NT, dtype=Int32)
taj = TensorArray(NT, dtype=Int32)
tak = TensorArray(NT, dtype=Int32)
tav = TensorArray(NT)
taf = TensorArray(NT)</code></pre><p>Within each loop (<code>body</code>), we extract the coordinates of each vertex coordinate</p><pre><code class="language-julia">el = elem[i]
x1, y1 = nodes[el[1]][1], nodes[el[1]][2]
x2, y2 = nodes[el[2]][1], nodes[el[2]][2]
x3, y3 = nodes[el[3]][1], nodes[el[3]][2]</code></pre><p>and compute the area of <code>i</code>th triangle</p><pre><code class="language-none">T = abs(0.5*x1*y2 - 0.5*x1*y3 - 0.5*x2*y1 + 0.5*x2*y3 + 0.5*x3*y1 - 0.5*x3*y2)</code></pre><p>The local stiffness matrix is computed and vectorized (<code>v</code>). It is computed symbolically.  To store the computed value into <code>TensorArray</code>, we call the <code>write</code> API (there is also <code>read</code> API, which reads a value from <code>TensorArray</code>)</p><pre><code class="language-julia">tav = write(tav, i, v)</code></pre><p>Note we have called </p><pre><code class="language-julia">i, tai, taj, tav, tak, taf = body(i, tai, taj, tav, tak, taf)</code></pre><p>before we call <code>while_loop</code>. This is because we need to initialize the <code>TensorArray</code>s (i.e., telling them the size and type of elements in the arrays). We must guarantee that the sizes and types of the elements in the arrays are consistent in <code>while_loop</code>. </p><p>Finally, we stack the <code>TensorArray</code> into a tensor and vectorized it according to the row major. This serves as the output of <code>assemble_FEM</code>. The complete script for solving this problem is <a href="https://github.com/kailaix/ADCME.jl/tree/master/examples/while_loop/while_loop.jl">here</a> and the following plot shows the numerical result and corresponding reference solution. </p><p><img src="../assets/while_loop.png" alt="Result for the Poisson Problem"/></p><h2 id="Gradients-through-while_loop-1"><a class="docs-heading-anchor" href="#Gradients-through-while_loop-1">Gradients through <code>while_loop</code></a><a class="docs-heading-anchor-permalink" href="#Gradients-through-while_loop-1" title="Permalink"></a></h2><p>To inspect the gradients through the loops, we can run </p><pre><code class="language-julia">println(run(sess, gradients(sum(u), Ds))) # a sparse tensor</code></pre><p>This outputs a sparse tensor instead of a full tensor. To obtain the full tensor, we could call <code>tf.convert_to_tensor</code></p><pre><code class="language-julia">println(run(sess, tf.convert_to_tensor(gradients(sum(u), Ds)))) # full tensor</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../global/">« Shared Memory Across Kernels</a><a class="docs-footer-nextpage" href="../julia_customop/">Julia Custom Operators »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 17 February 2020 18:46">Monday 17 February 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
